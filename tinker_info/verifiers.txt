Command line interface and SDKs for managing Prime Intellect GPU resources, sandboxes, and environments.

Quick Start
# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Install prime
uv tool install prime

# Authenticate
prime login

# Browse verified environments
prime env list

# List available GPU resources
prime availability list
Features
Environments - Access hundreds of verified environments on our community hub
Evaluations - Push and manage evaluation results (supports JSON and verifiers format)
GPU Resource Management - Query and filter available GPU resources
Pod Management - Create, monitor, and terminate compute pods
Sandboxes - Easily run AI-generated code in the cloud
SSH Access - Direct SSH access to running pods
Team Support - Manage resources across team environments
Installation
Using uv (recommended)
First, install uv if you haven't already:

curl -LsSf https://astral.sh/uv/install.sh | sh
Then install prime:

uv tool install prime
Using pip
pip install prime
Sandboxes SDK Only
If you only need the sandboxes SDK (lightweight, ~50KB):

uv pip install prime-sandboxes
See prime-sandboxes documentation for SDK usage.

Usage
Configuration
API Key Setup
# Interactive mode (recommended - hides input)
prime config set-api-key

# Non-interactive mode (for automation)
prime config set-api-key YOUR_API_KEY

# Environment variable (most secure for scripts)
export PRIME_API_KEY="your-api-key-here"
Other Configuration
# Configure SSH key for pod access
prime config set-ssh-key-path

# View current configuration
prime config view
Security Note: When using non-interactive mode, the API key may be visible in your shell history. For enhanced security, use interactive mode or environment variables.

Environments Hub
Access hundreds of verified environments on our community hub with deep integrations with sandboxes, training, and evaluation stack.

# Browse available environments
prime env list

# View environment details
prime env info <environment-name>

# Install an environment locally
prime env install <environment-name>

# Create and push your own environment
prime env init my-environment
prime env push my-environment
GPU Resources
# List all available GPUs
prime availability list

# Filter by GPU type
prime availability list --gpu-type H100_80GB

# Show available GPU types
prime availability gpu-types
Pod Management
# List your pods
prime pods list

# Create a pod
prime pods create
prime pods create --id <ID>     # With specific GPU config
prime pods create --name my-pod # With custom name

# Monitor and manage pods
prime pods status <pod-id>
prime pods terminate <pod-id>
prime pods ssh <pod-id>
Evaluations
Push and manage evaluation results to the Environments hub. Supports verifiers format and JSON format.

# Auto-discover and push evaluations from current directory
prime eval push

# Push specific directory
prime eval push examples/verifiers_example

# List all evaluations
prime eval list

# Get evaluation details
prime eval get <eval-id>

# View evaluation samples
prime eval samples <eval-id>
See examples/README.md for detailed format documentation.

Team Management
# List teams
prime teams list

# Set team context
prime config set-team-id
Development
# Clone the repository
git clone https://github.com/PrimeIntellect-ai/prime-cli
cd prime-cli

# Set up workspace (installs all packages in editable mode)
uv sync

# Install CLI globally in editable mode
uv tool install -e packages/prime

# Now you can use the CLI directly
prime --help

# Run tests
uv run pytest packages/prime/tests
uv run pytest packages/prime-sandboxes/tests
All packages (prime-core, prime-sandboxes, prime) are installed in editable mode. Changes to code are immediately reflected.

Releasing
This monorepo contains two independently versioned packages: prime (CLI + full SDK) and prime-sandboxes (lightweight SDK).

Versions are single-sourced from each package's __init__.py file:

prime: packages/prime/src/prime_cli/__init__.py
prime-sandboxes: packages/prime-sandboxes/src/prime_sandboxes/__init__.py
To release a new version:
Update the __version__ string in the appropriate __init__.py file
Commit and push the change
Tagging and publishing to PyPI is handled automatically by CI.

Version sync considerations:
When releasing prime, consider whether prime-sandboxes should also be bumped, as prime depends on prime-sandboxes. The packages can be released independently or together depending on what changed.






PRIME-Environments: Training-Ready RL Environments + Evals
Installation
Quick Installation (Recommended)

curl -sSL https://raw.githubusercontent.com/PrimeIntellect-ai/prime-environments/main/scripts/install.sh | bash
Manual Installation
Creating environments
Create a new environment template

prime env init <your-env-name> 
This will create an environment in envirnments/<your-env-name>. Enter the project directory with

cd environments/<your-env-name>
Then, edit your environment by implementing the load_environment function. To test, install the environment as a local package (editable) and then run the vf-eval entrypoint.

uv pip install -e .
uv run vf-eval <your-env-name>
Once you are done, push the environment to the registry.

prime env push 
If you bump the version in an environment's pyproject.toml, our CI will automatically build and publish that environment to the hub under the primeintellect organization. No manual action is required beyond the version bump.




Verifiers: Environments for LLM Reinforcement Learning
Documentation • Environments Hub • PRIME-RL

Style Test Envs

News & Updates
[11/07/25] Verifiers v0.1.7 is released! This includes an improved quickstart configuration for training with [prime-rl], a new included "nano" trainer (vf.RLTrainer, replacing vf.GRPOTrainer), and a number of bug fixes and improvements to the documentation.
[10/27/25] A new iteration of the Prime Intellect Environments Program is live!
Overview
Verifiers is a library of modular components for creating RL environments and training LLM agents. Environments built with Verifiers can be used directly as LLM evaluations, synthetic data pipelines, or agent harnesses for any OpenAI-compatible model endpoint, in addition to RL training. Verifiers is supported by prime-rl for large-scale performance-optimized async RL training, includes a minimal transformers-based trainer (vf.RLTrainer) for simple algorithmic experiments, and can easily be integrated into any RL training stack which exposes an OpenAI-compatible inference client.

Full documentation is available here.

Verifiers is the native library used by Prime Intellect's Environments Hub; see here for information about publishing your Environments to the Hub, and here for a collection of Environments built with Verifiers.

Quick Start
Verifiers supports CPU-based environment development and evaluation with API models, as well as large-scale GPU-based RL training with prime-rl and several other trainers. Environments built with Verifiers are standalone Python packages that can be installed and used in your own projects, or shared with the community through the Environments Hub.

To get started, install uv and the prime CLI, and add verifiers to your project:

curl -LsSf https://astral.sh/uv/install.sh | sh
uv init && uv venv --python 3.12    # to create a new project if needed
uv tool install prime
uv add verifiers
Select an environment from the Environments Hub to install:

prime env install will/wiki-search
Or install an environment from this repo:

uv run vf-install wordle --from-repo
Run a quick evaluation with OpenAI models:

uv run vf-eval wordle -m gpt-5-nano
For advanced evaluation configurations with the prime CLI, see here

RL Training
prime-rl
We recommend using the prime-rl trainer, and provide a basic setup guide below. See the prime-rl documentation for more information.

To get started, do:

uv run vf-setup
This will clone and install the prime-rl trainer and its dependencies, and set up a default configuration for training with the included wiki-search Environment.

Then, you can start training with:

uv run prime-rl @ configs/prime-rl/wiki-search.toml
This will launch a tmux session with separate panes for the trainer, orchestrator, and inference server.

vf.RLTrainer
The included RLTrainer is a minimal, hackable training loop based on transformers.Trainer that supports both full-parameter finetuning and LoRA training. RLTrainer can be viewed as a "baby" prime-rl that adopts a similar default training recipe (async CISPO with one-step off-policy overlap), intended for single-node test runs with dense models. The primary files (trainer.py and orchestrator.py, located in verifiers/rl/trainer/) are under 1000 lines of code, and are designed to be a convenient starting point for writing your own training loop.

The feature set is intentionally kept minimal and focused. Users seeking maximum performance, MoE support, multi-node training, multidimensional parallelism, and other advanced features should use the prime-rl trainer.

To use vf.RLTrainer in your own project, install with RL extras:

uv add 'verifiers[rl]'
Then, create a training configuration file, e.g. configs/vf-rl/wiki-search.toml, and do:

uv run vf-rl @ configs/vf-rl/wiki-search.toml
Example configuration files can be created in your project by running uv run vf-setup.

Other Trainers
verifiers is intended to be largely trainer-agnostic. It is supported by [SkyRL] and [Tinker], and is straightforward to support for any trainer which can expose an OpenAI-compatible inference client for rollouts. See the integrations directory for more information.

Development
To install verifiers from source for core library development, or to use the latest main branch, install with:

curl -sSL https://raw.githubusercontent.com/PrimeIntellect-ai/verifiers/main/scripts/install.sh | bash
If you want to develop with RL extras enabled in this repo, do:

uv sync --extra rl
Please use the Environments Hub to share your Environments with the community, rather than PRs to this repo. If you find yourself needing to clone and modify the core library in order to implement key functionality for your project, please open an issue or PR so that we can help you.

Environments
Environments in Verifiers are installable Python modules which can specify dependencies in a pyproject.toml, and which expose a load_environment function for instantiation by downstream applications (e.g. trainers). See environments/ for examples.

To initialize a blank Environment module template, do:

uv run vf-init environment-name # -p /path/to/environments (defaults to "./environments")
To install an Environment module into your project, do:

uv run vf-install environment-name # -p /path/to/environments (defaults to "./environments") 
To install an Environment module from the Environments Hub, do:

prime env install user/environment-name
To install an Environment module from this repo's environments folder, do:

uv run vf-install math-python --from-repo # -b branch_or_commit (defaults to "main")
Once an Environment module is installed, you can create an instance of the Environment using load_environment, passing any necessary args:

import verifiers as vf
vf_env = vf.load_environment("environment-name", **env_args)
To run a quick evaluation of your Environment with an API-based model, do:

uv run vf-eval environment-name -s # run and save eval results locally
# vf-eval -h for config options; defaults to gpt-4.1-mini, 5 prompts, 3 rollouts for each
If you're using Prime Intellect infrastructure, the prime CLI provides first-class commands for working with Verifiers environments through the Environments Hub. Install it with uv tool install prime, authenticate via prime login, then use prime env push to publish your package and prime env install owner/name (optionally pinning a version) to consume it from pods or local machines.

The core elements of Environments are:

Datasets: a Hugging Face Dataset with a prompt column for inputs, and optionally answer (str) or info (dict) columns for evaluation (both can be omitted for environments that evaluate based solely on completion quality)
Rollout logic: interactions between models and the environment (e.g. env_response + is_completed for any MultiTurnEnv)
Rubrics: an encapsulation for one or more reward functions
Parsers: optional; an encapsulation for reusable parsing logic
We support both /v1/chat/completions-style and /v1/completions-style inference via OpenAI clients, though we generally recommend /v1/chat/completions-style inference for the vast majority of applications. Both prime-rl as well as the included vf.RLTrainer support the full set of SamplingParams exposed by vLLM (via their OpenAI-compatible server interface), and leveraging this will often be the appropriate way to implement rollout strategies requiring finer-grained control, such as interrupting and resuming generations for interleaved tool use, or enforcing reasoning budgets.

SingleTurnEnv
For tasks requiring only a single response from a model for each prompt, you can use SingleTurnEnv directly by specifying a Dataset and a Rubric. Rubrics are sets of reward functions, which can be either sync or async.

from datasets import load_dataset
import verifiers as vf

dataset = load_dataset("my-account/my-dataset", split="train")

def reward_A(prompt, completion, info) -> float:
	# reward fn, e.g. correctness
	...

def reward_B(parser, completion) -> float:
	# auxiliary reward fn, e.g. format
	...

async def metric(completion) -> float:
	# non-reward metric, e.g. proper noun count
	...

rubric = vf.Rubric(funcs=[reward_A, reward_B, metric], weights=[1.0, 0.5, 0.0])

vf_env = vf.SingleTurnEnv(
	dataset=dataset,
	rubric=rubric
)

# Async evaluation (recommended)
from openai import AsyncOpenAI
results = await vf_env.evaluate(client=AsyncOpenAI(), model="gpt-4.1-mini", num_examples=100, rollouts_per_example=1)

# Sync evaluation
from openai import OpenAI
results = vf_env.evaluate_sync(client=OpenAI(), model="gpt-4.1-mini", num_examples=100, rollouts_per_example=1)

vf_env.make_dataset(results) # HF dataset format
Datasets should be formatted with columns for:

'prompt' (List[ChatMessage]) OR 'question' (str) fields
ChatMessage = e.g. {'role': 'user', 'content': '...'}
if question is set instead of prompt, you can also pass system_prompt (str) and/or few_shot (List[ChatMessage])
answer (str) AND/OR info (dict) (both optional, can be omitted entirely)
task (str): optional, used by EnvGroup and RubricGroup for orchestrating composition of Environments and Rubrics
The following named attributes available for use by reward functions in your Rubric:

prompt: sequence of input messages
completion: sequence of messages generated during rollout by model and Environment
answer: primary answer column, optional (defaults to empty string if omitted)
state: can be modified during rollout to accumulate any metadata (state['responses'] includes full OpenAI response objects by default)
info: auxiliary info needed for reward computation (e.g. test cases), optional (defaults to empty dict if omitted)
task: tag for task type (used by EnvGroup and RubricGroup)
parser: the parser object declared. Note: vf.Parser().get_format_reward_func() is a no-op (always 1.0); use vf.ThinkParser or a custom parser if you want a real format adherence reward.
Note: Some environments can fully evaluate using only prompt, completion, and state without requiring ground truth answer or info data. Examples include format compliance checking, completion quality assessment, or length-based rewards.

For tasks involving LLM judges, you may wish to use vf.JudgeRubric() for managing requests to auxiliary models.

ToolEnv
For many applications involving tool use, you can use ToolEnv to leverage models' native tool/function-calling capabilities in an agentic loop. Tools must be stateless and idempotent—each call should be fully determined by the provided arguments—because the environment will automatically terminate once the assistant responds without tool calls. Tools can be specified as generic Python functions (with type hints and docstrings), which will then be passed in JSON schema form to each inference request.

import verifiers as vf
vf_env = vf.ToolEnv(
	dataset= ... # HF Dataset with 'prompt'/'question' and optionally 'answer'/'info' columns
	rubric= ... # Rubric object; vf.ToolRubric() can be optionally used for counting tool invocations in each rollout
	tools=[search_tool, read_article_tool, python_tool], # python functions with type hints + docstrings
	max_turns=10
)
In cases where your tools require heavy computational resources, we recommend hosting your tools as standalone servers (e.g. MCP servers) and creating lightweight wrapper functions to pass to ToolEnv. Parallel tool call support is enabled by default. If you need to inject per-rollout or cross-call state (IDs, credentials, cached resources), promote the environment to StatefulToolEnv and populate that state through setup_state/update_tool_args instead of hiding globals.

StatefulToolEnv
StatefulToolEnv extends ToolEnv for workflows where tool calls must incorporate dynamic state (for example, sandbox handles or per-user secrets). Implement setup_state to seed the state dict and override update_tool_args to merge state into each tool invocation. Any arguments you strip from the OpenAI schema via args_to_skip should be tracked in skipped_args so the model never sees sensitive parameters. Avoid storing global state; keep everything in the provided state dict.

SandboxEnv & PythonEnv
SandboxEnv builds on StatefulToolEnv to coordinate long-running sandboxes. Queue heavyweight provisioning inside setup_state (without awaiting) and gate tool execution on readiness inside update_tool_args or the tools themselves. PythonEnv is a concrete sandboxed executor that demonstrates the pattern: it spins up a Prime sandbox, injects the sandbox ID into each tool call, and tears down resources when the rollout finishes. Treat both environments as references when building similar stateful tool workflows.

For training, or self-hosted endpoints, you'll want to enable auto tool choice in vLLM with the appropriate parser. If your model does not support native tool calling, you may find the XMLParser abstraction useful for rolling your own tool call parsing on top of MultiTurnEnv; see environments/xml_tool_env for an example.

MultiTurnEnv
Both SingleTurnEnv and ToolEnv are instances of MultiTurnEnv, which exposes an interface for writing custom Environment interaction protocols. Override is_completed and env_response, and make sure any custom completion logic defers to the base class so turn limits and other shared guards keep working.

from typing import Tuple
import verifiers as vf
from verifiers.types import Messages, State
class YourMultiTurnEnv(vf.MultiTurnEnv):
    def __init__(self,
                 dataset: Dataset,
                 rubric: Rubric,
				 max_turns: int,
                 **kwargs):
	
  async def is_completed(self, messages: Messages, state: State, **kwargs) -> bool:
    # Always call the base check so max_turns and shared guards are respected
    if await super().is_completed(messages, state, **kwargs):
        return True
    # return whether or not a rollout is completed
    return state.get("task_complete", False)

  async def env_response(self, messages: Messages, state: State, **kwargs) -> Tuple[Messages, State]:
    # return new environment message(s) + updated state
If your application requires more fine-grained control than is allowed by MultiTurnEnv, you may want to inherit from the base Environment functionality directly and override the rollout method.

Troubleshooting
Ensure your wandb and huggingface-cli logins are set up (or set report_to=None in training_args). You should also have something set as your OPENAI_API_KEY in your environment (can be a dummy key for vLLM).
If using high max concurrency, increase the number of allowed open sockets (e.g. ulimit -n 4096)
On some setups, inter-GPU communication can hang or crash during vLLM weight syncing. This can usually be alleviated by setting (or unsetting) NCCL_P2P_DISABLE=1 in your environment (or potentially NCCL_CUMEM_ENABLE=1). Try this as your first step if you experience NCCL-related issues.
If problems persist, please open an issue.
Resource Requirements
prime-rl can be run on a single GPU by allocating only a fraction of the available memory to the inference server (see here for an example configuration), and can also be scaled to hundreds of GPUs for large-scale training. A wide range of competitively-priced cluster configurations are available on Prime Intellect.




Docs
Getting Started
Welcome to Verifiers! This library provides a flexible framework for creating RL environments and evaluations with custom multi-turn interaction protocols.
​
What is Verifiers?
Verifiers enables you to:
Define custom interaction protocols between models and environments
Build agents, multi-turn conversations, tool-augmented reasoning, and interactive games
Create reusable evaluation environments with multi-criteria reward functions
Train models with the included RL trainer (via vf-rl) or integrate with other RL frameworks
Key features:
First-class OpenAI-compatibility for ChatCompletions and Completions
Extensible multi-turn interactions via MultiTurnEnv
Native tool calling support with ToolEnv
Modular reward functions through Rubric classes
End-to-end async compatibility with sync support where you want it
Full-spectrum scaling from CPU evaluations in Jupyter to multi-node GPU RL training
Environments as Python modules for easy installation, sharing, and reuse
​
Installation
​
Basic Installation
For evaluation and API model usage:

Copy


uv add verifiers
​
Training Support
For RL training with the included trainer:

Copy


uv add 'verifiers[rl]'
To use the latest main with RL extras:

Copy


uv add 'verifiers[rl] @ git+https://github.com/PrimeIntellect-ai/verifiers.git@main'
​
Latest Development Version
To use the latest main branch:

Copy


uv add verifiers@git+https://github.com/PrimeIntellect-ai/verifiers.git
​
Development Setup
For contributing to verifiers:

Copy


git clone https://github.com/PrimeIntellect-ai/verifiers.git
cd verifiers
uv sync --all-extras && uv pip install flash-attn --no-build-isolation
uv run pre-commit install
​
Integration with prime-rl
For large-scale FSDP training, see prime-rl.
​
Integration with Prime Intellect Environments Hub
Coming soon.
​
Documentation
​
Getting Started
Overview — Core concepts and architecture. Start here if you’re new to Verifiers to understand how environments orchestrate interactions.
Environments — Creating custom interaction protocols with MultiTurnEnv, ToolEnv, and basic rubrics.
​
Advanced Usage
Components — Advanced rubrics, tools, parsers, with practical examples. Covers judge rubrics, tool design, and complex workflows.
Training — GRPO training and hyperparameter tuning. Read this when you’re ready to train models with your environments.
​
Reference
Development — Contributing to verifiers
Type Reference — Understanding data structures


Docs
Overview
Verifiers provides a flexible framework for defining custom interaction protocols between LLMs and environments, enabling sophisticated multi-turn reasoning, tool use, and interactive evaluation.
The three key pieces of environments in Verifiers are:
Your dataset (str or List[ChatMessage])
Your Rubric (one or more reward functions)
Your interaction protocol, extended from MultiTurnEnv
​
Core Concept: Interaction Protocols
Verifiers allows defining arbitrary interaction patterns between models and environments:

Copy


Environment (orchestration layer)
    ├── Defines interaction protocol (what to observe, how to respond, when to terminate)
    ├── Manages conversation state
    ├── Integrates tools and external resources
    └── Evaluates performance via Rubrics
​
Example Protocols
Q&A Tasks: Single model response → evaluation
Tool Use: Model request → tool execution → model continues
Games: Model move → game state update → environment feedback → repeat
Tutoring: Model attempt → hint/correction → retry until correct
Debate: Model A argument → Model B rebuttal → judge evaluation
​
Environment Types
​
MultiTurnEnv: Maximum Flexibility
The base class for custom interaction protocols:

Copy


import verifiers as vf
from verifiers.types import Messages, State
from typing import Tuple

class MyProtocol(vf.MultiTurnEnv):
    async def env_response(self, messages: Messages, state: State) -> Tuple[Messages, State]:
        """Define how environment responds to model"""
        response = [{"role": "user", "content": "Environment feedback"}]
        state["turn"] = state.get("turn", 0) + 1
        return response, state
    
    async def is_completed(self, messages: Messages, state: State) -> bool:
        """Define when interaction ends"""
        # Always defer to the base implementation so turn limits are respected
        if await super().is_completed(messages, state):
            return True
        return state.get("task_complete", False)
​
ToolEnv: Native Tool Calling
Leverages models’ built-in tool calling for agentic workflows:

Copy


env = vf.ToolEnv(
    tools=[search, calculate, execute_code],  # Stateless Python functions
    max_turns=10,
    dataset=dataset,
    rubric=rubric
)
Tools may be sync or async. Keep them pure: the environment ends when the assistant responds without tool calls. If you must inject rollout-specific context, upgrade to StatefulToolEnv and override update_tool_args instead of relying on global state.
​
SingleTurnEnv: Simple Evaluation
For straightforward Q&A tasks without interaction:
env = vf.SingleTurnEnv( dataset=dataset, system_prompt=“Answer the question.”, rubric=rubric, )
​
Key Components
​
Rubrics: Multi-Criteria Evaluation
Rubrics define how to evaluate model responses by combining multiple criteria:

Copy


# Simple reward function (can be sync or async)
async def correctness(prompt, completion, answer, state):
    return 1.0 if answer.lower() in completion[-1]['content'].lower() else 0.0

# Combine multiple criteria
rubric = vf.Rubric(
    funcs=[correctness, efficiency, clarity],
    weights=[1.0, 0.3, 0.2]  # Relative importance
)
Each reward function receives the full context (prompt, response, ground truth answer, and environment state) and returns a score. The rubric combines these scores based on weights to produce a final reward.
Common rubric patterns:
Single criterion: One reward function (e.g., exact match)
Multi-criteria: Weighted combination of multiple aspects
Judge-based: Using LLMs to evaluate quality
Stateful: Tracking patterns across interactions
​
Environment Modules
Package your interaction protocol as a reusable module:

Copy


my_environment/
├── outputs/                # Evaluation logs
├── my_environment.py       # Defines load_environment() -> vf.Environment
├── pyproject.toml          # Dependencies
└── README.md               # Documentation
This enables:
Easy sharing and versioning
Dependency isolation
Standardized interfaces
​
State Management
Environments maintain state throughout interactions:

Copy


state = {
    # automatically managed
    "prompt": prompt, # inputs from dataset
    "completion": [], # trajectory so far
    "answer": answer, # golden answer (str)
    "task": task, # optional environment ID column
    "info": info, # evaluation metadata (dict) -- can use answer/info/both
    "responses": [], # Raw API responses from OpenAI client
    "example_id": example_id, # Source dataset row identifier
    "turn": 0,
    "timing": {"generation_ms": 0.0, "scoring_ms": 0.0, "total_ms": 0.0},
    # custom user-managed state
    "lives_remaining": 2,
    "inventory": {"potion": 1, "power-up": 2}
    ...
}
A wide variety of complex interaction protocols, reward schemes, and training algorithms can be coordinated via tracking appropriate data in state.
​
Design Philosophy
​
1. Protocol-First Design
Start by defining your interaction pattern:
When should the environment respond?
What information should it provide?
How should the conversation end?
​
2. Composable Evaluation
Build complex evaluation from simple parts:
Individual reward functions for specific criteria
Rubrics to combine and weight them
Environments to orchestrate the process
​
3. OpenAI-Compatible Integration
Works with any OpenAI-compatible API:

Copy


# OpenAI, vLLM, or any compatible endpoint
from openai import AsyncOpenAI
import asyncio

async_client = AsyncOpenAI(base_url="http://localhost:8000/v1")
results = asyncio.run(env.evaluate(client=async_client, model="llama-3.1-8b"))
# Prefer env.evaluate_sync(OpenAI(...), ...) if you need a blocking helper
​
Data Flow
Dataset provides prompts and ground truth
Environment orchestrates the interaction protocol
Model generates responses via OpenAI-compatible client
Rubric evaluates quality through reward functions
Results include full interaction traces and scores
​
Evaluation lifecycle
Inputs expected by environments:
prompt: str or list[ChatMessage] (chat-style). If you use question in your dataset, environments will turn it into a chat message, adding system_prompt/few_shot if provided.
answer or info: optional. answer is a string; info is a dict for richer metadata. Both can be omitted for environments that evaluate based solely on completion quality (e.g., format adherence, length constraints, style assessment).
task: optional string used by EnvGroup/RubricGroup to route behavior.
Running evaluation:

Copy


import asyncio
from openai import AsyncOpenAI

async_client = AsyncOpenAI()
results = asyncio.run(
    env.evaluate(
        client=async_client,
        model=model,
        num_examples=100,
        rollouts_per_example=2,
        max_concurrent=32,
    )
)
rollouts_per_example > 1 repeats dataset entries internally.
max_concurrent throttles concurrent rollouts.
save_every (when > 0) checkpoints intermediate progress during interleaved rollouts (set interleave_scoring=True).
Scoring:
Each reward function returns a float. Weights applied inside Rubric combine them into results.reward.
All individual scores are logged under results.metrics keyed by function name (even if weight is 0.0).
Outputs (GenerateOutputs):
prompt, completion, answer, state, info, task, id, reward, metrics: dict[str, list[float]], plus a metadata block summarizing the run.
Message types:
message_type="chat" (default) expects chat messages; "completion" expects raw text continuation. Choose based on your task (e.g., continuation quality uses completion).
​
Optional Utilities
​
Parsers
For extracting structured information when needed:
XMLParser: Extract XML-tagged fields
ThinkParser: Separate reasoning from answers
Custom parsers for domain-specific formats
Parsers are optional conveniences - many environments work perfectly with raw text.
​
Integration Points
​
For Evaluation
The most convenient way to run quick evaluations is via the vf-eval CLI tool:

Copy


vf-install my-environment-module # from ./environments/my_environment_module 
vf-eval my-environment-module -m gpt-5 -n 10 -r 5 -s 
We also provide a TUI for browsing locally-cached (with -s) eval results:

Copy


vf-tui 
You can also evaluate models in your environments programmatically:

Copy


import asyncio
from openai import AsyncOpenAI

async_client = AsyncOpenAI()
results = asyncio.run(env.evaluate(client=async_client, model=model, num_examples=100))
env.evaluate is async—wrap it with asyncio.run(...) (as above) or call env.evaluate_sync when you must stay in synchronous code.
​
For Training
Run RL training via vf-rl using a single TOML to configure model, environment, inference, and trainer. See the training guide for a minimal example.
​
For Custom Workflows
All components can be used independently:

Copy


# Use rubrics standalone
scores = await rubric.score_rollout(prompt, completion, answer, state)

# Create custom protocols
class MyProtocol(vf.MultiTurnEnv):
    # Your interaction logic
​
Next Steps
To create custom interactions, see Environments
For advanced component usage and examples, see Components
To train models with your environments, see Training

Add to assistant
Getting Started


Docs
Environments
This guide covers how to create, develop, and use environments in Verifiers.
​
Creating a New Environment
The recommended approach is to create an environment module, i.e. a self-contained package that can be installed and reused.
​
Initialize from Template

Copy


vf-init my-math-env
This creates:

Copy


environments/my_math_env/
├── my_math_env.py      # Main implementation
├── pyproject.toml      # Dependencies and metadata
└── README.md           # Documentation
​
Basic Environment Structure
Every environment module must export a load_environment function:

Copy


# my_math_env.py
import verifiers as vf

def load_environment(**kwargs):
    """Load and configure the environment."""
    # 1. Load dataset
    dataset = vf.load_example_dataset("gsm8k", split="train")
    
    # 2. Configure parser
    parser = vf.ThinkParser()
    
    # 3. Define reward functions -- can automatically reference:
    # - parser, prompt, completion, answer, state , task, info 
    def correct_answer(parser, completion, answer):
        response = parser.parse_answer(completion) or ''
        return 1.0 if response.strip() == answer.strip() else 0.0
    
    # 4. Create rubric
    rubric = vf.Rubric(
        funcs=[correct_answer, parser.get_format_reward_func()],
        weights=[1.0, 0.2]
    )
    
    # 5. Return configured environment
    return vf.SingleTurnEnv(
        dataset=dataset,
        system_prompt="Think step-by-step, then give your answer.",
        parser=parser,
        rubric=rubric,
        **kwargs  # Pass through additional arguments
    )
​
Adding Dependencies
Specify environment-specific dependencies in pyproject.toml:

Copy


[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "my_math_env"
description = "Single-turn math environment"
tags = ["math", "verifiable-reward"]
version = "0.1.0"
requires-python = ">=3.11"
dependencies = [
    "verifiers",
    "sympy",  # For symbolic math
]

[tool.hatch.build]
include = ["my_math_env.py", "pyproject.toml"]
​
Default Evaluation Configuration
You can specify default evaluation parameters in your pyproject.toml to customize the default behavior when users run vf-eval without explicit arguments:

Copy


[tool.verifiers.eval]
num_examples = 20
rollouts_per_example = 5
These defaults are automatically read from the installed package’s pyproject.toml and used when:
Users don’t provide -n / --num-examples or -r / --rollouts-per-example flags
The package is installed and pyproject.toml is included in the package distribution
Important: Ensure pyproject.toml is included in your package by adding it to the [tool.hatch.build] section:

Copy


[tool.hatch.build]
include = ["my_math_env.py", "pyproject.toml"]
CLI arguments always take precedence over these defaults—if a user explicitly passes -n 10, that value will be used regardless of what’s in pyproject.toml.
Third-party libraries can also access these defaults programmatically:

Copy


import importlib.resources
try:
    import tomllib  # Python 3.11+
except ImportError:
    import tomli as tomllib

package_ref = importlib.resources.files("my_math_env")
pyproject_file = package_ref / "pyproject.toml"
with pyproject_file.open("rb") as f:
    pyproject_data = tomllib.load(f)
    
eval_config = pyproject_data.get("tool", {}).get("verifiers", {}).get("eval", {})
num_examples = eval_config.get("num_examples", 5)  # fallback to 5 if not specified
​
Development Workflow
​
1. Install Your Environment
During development, install your environment locally:

Copy


vf-install my-math-env # wraps 'uv pip install -e ...'
This installs the module and its dependencies in your Python environment.
​
2. Test Your Environment
Use the CLI to quickly test:

Copy


vf-eval my-math-env -m gpt-4.1-mini -n 5 # runs a small batch of rollouts; use -h to see options
Or test programmatically:

Copy


import verifiers as vf
from openai import OpenAI

# Load your environment
env = vf.load_environment("my-math-env")

# Test with a model
client = OpenAI()
results = env.evaluate_sync(
    client=client,
    model="gpt-4.1-mini",
    num_examples=5,
    rollouts_per_example=2,
    max_concurrent=32,
    save_every=10,
)
print(results)
Prefer AsyncOpenAI + await env.evaluate(...) for fully async workflows; the sync helper is ideal when integrating into existing blocking scripts. Intermediate saving via save_every requires the default interleaved scoring pipeline.
​
3. Iterate on Design
Common iterations:
Adjust system prompts for better performance
Refine parser logic for edge cases
Add new reward functions to the rubric
Configure dataset filtering or sampling
​
Using Prime CLI and the Environments Hub
Prime Intellect provides a hosted Environments Hub for discovering, installing, and sharing verifiers packages. The prime CLI wraps the same templates and packaging helpers exposed here, so you can publish environments once and re-use them from local machines, CI pipelines, or Prime pods.
​
Install and authenticate

Copy


uv tool install prime
prime login  # stores an API token used for hub operations
If you collaborate through a team account, run prime config use <team> (or set --team flags in later commands) so pushes end up in the shared namespace.
​
Bootstrap templates from the CLI
prime env init <name> uses the same generator as vf-init, but it pre-populates the directory inside ./environments/ and prints the follow-up commands for publishing. Use this when you’re starting a project that you plan to ship through the Hub:

Copy


prime env init vf-math-demo
cd environments/vf_math_demo
# edit vf_math_demo.py, pyproject.toml, README.md, etc.
For existing environments you created earlier with vf-init, no migration is required—prime env push operates on any directory that contains a valid pyproject.toml and load_environment implementation.
​
Publish versions to the Hub
Once your package is ready, build and upload it with:

Copy


prime env push --visibility PUBLIC  # or PRIVATE for internal distributions
The command builds a wheel (using uv build when available), computes a deterministic content hash, and uploads the artifact. Add --auto-bump to increment the patch version before publishing, or pass --team <slug> to publish under a team namespace. Successful pushes print a dashboard link plus a one-line install command.
You can manage published artifacts directly from the CLI:
prime env version list owner/name shows version history and hashes.
prime env version delete owner/name <content_hash> removes a specific build.
prime env delete owner/name deletes the environment entirely.
​
Discover, install, and inspect environments
The CLI also helps consumers find and install verifiers:

Copy


prime env list --owner my-team           # browse available environments
prime env info my-team/vf-math-demo      # show install commands and metadata
prime env install my-team/vf-math-demo   # install latest release with uv
prime env install owner/env@0.1.2 --with pip  # pin version & use pip instead
prime env pull owner/env@latest --target ./tmp-env  # download source tarball
prime env install prefers installing from the Hub’s simple index (so upgrades work with uv add/pip install too), and falls back to direct wheel URLs for older releases. After installation the package becomes available to verifiers.load_environment just like any other module:

Copy


from verifiers import load_environment

env = load_environment("vf-math-demo")
When you run workloads on Prime pods provisioned via prime pods create, include these install commands in your startup scripts so the same environment definitions are available remotely.
​
Working with Rubrics
Rubrics are central to defining what makes a good response in your environment. Here’s how to use them effectively:
​
Basic Reward Functions
A reward function takes the full context and returns a score (typically 0.0 to 1.0):

Copy


def exact_match(prompt, completion, answer, state):
    """Reward exact matches."""
    response = completion[-1]['content']
    return 1.0 if response.strip() == answer.strip() else 0.0

def partial_credit(prompt, completion, answer, state):
    """Give partial credit for containing key terms."""
    key_terms = answer.lower().split()
    response = completion[-1]['content']
    found = sum(1 for term in key_terms if term in response.lower())
    return found / len(key_terms) if key_terms else 0.0
​
Creating Rubrics
Combine multiple reward functions with weights:

Copy


# Single criterion
rubric = vf.Rubric(funcs=[exact_match])

# Multi-criteria with weights
rubric = vf.Rubric(
    funcs=[exact_match, partial_credit, length_penalty],
    weights=[1.0, 0.5, 0.1]  # Relative importance
)
​
Using Parser Format Rewards
Parsers often provide format reward functions:

Copy


parser = vf.ThinkParser(extract_fn=extract_boxed_answer)

def correct_answer(parser, completion, answer):
    parsed = parser.parse_answer(completion) # applies extract_fn to final message
    return 1.0 if parsed == answer else 0.0

rubric = vf.Rubric(
    funcs=[
        correct_answer,
        parser.get_format_reward_func()  # Rewards proper <think> format
    ],
    weights=[1.0, 0.2]
)
​
Stateful Reward Functions
Access environment state for complex evaluation:

Copy


def efficiency_reward(prompt, response, answer, state):
    """Reward based on number of steps taken."""
    max_steps = 10
    steps_taken = state.get("turn", 0)
    return max(0, (max_steps - steps_taken) / max_steps)
​
Environment Types
Choose the appropriate base class for your task:
​
SingleTurnEnv
For one-shot tasks with clear input/output:

Copy


def load_environment(**kwargs):
    return vf.SingleTurnEnv(
        dataset=dataset,
        system_prompt="Answer the question.", # only used if dataset has 'question' (str) and not 'prompt'
        parser=parser,
        rubric=rubric,
        **kwargs
    )
​
MultiTurnEnv
For interactive tasks requiring multiple steps:

Copy


from verifiers.types import Messages, State
from typing import Tuple

class MyGameEnv(vf.MultiTurnEnv):

    async def env_response(self, messages: Messages, state: State) -> Tuple[Messages, State]:
        """Define how the environment responds."""
        last_msg = messages[-1]
        if last_msg["role"] != "assistant":
            return [], state

        player_action = last_msg["content"]
        if self.is_game_over(state):
            state["done"] = True
            return [{"role": "user", "content": "Game over!"}], state

        state = self.update_state(state, player_action)
        feedback = self.get_game_feedback(state)
        return [{"role": "user", "content": feedback}], state

    async def is_completed(self, messages: Messages, state: State) -> bool:
        if await super().is_completed(messages, state):
            return True
        return state.get("solved", False) or state.get("failed", False)
​
ToolEnv
For tasks requiring external tools:

Copy


def calculate(expression: str) -> float:
    """Calculate a mathematical expression."""
    return eval(expression)  # Simplified example

def load_environment(**kwargs):
    return vf.ToolEnv(
        dataset=dataset,
        tools=[calculate],  # Automatically converted to tool schemas
        parser=parser,
        rubric=rubric,
        **kwargs
    )
Tool functions should be deterministic and free of hidden side effects. A rollout ends when the model produces an assistant turn with no tool calls, so store per-session context in state rather than globals. Use StatefulToolEnv if you need to inject extra arguments or secrets into each tool invocation.
​
StatefulToolEnv
For stateful workflows, extend vf.StatefulToolEnv:

Copy


class SandboxAwareEnv(vf.StatefulToolEnv):
    async def setup_state(self, state: State, **kwargs) -> State:
        state = await super().setup_state(state, **kwargs)
        state["sandbox_id"] = await provision_sandbox_async()
        return state

    def update_tool_args(self, tool_name: str, tool_args: dict, messages: Messages, state: State, **kwargs) -> dict:
        return {**tool_args, "sandbox_id": state["sandbox_id"]}
Keep heavy provisioning asynchronous and defer expensive await calls until a tool actually needs the resource. Remove sensitive args from the tool schema via add_tool(..., args_to_skip=[...]) so the model never sees them.
​
SandboxEnv & PythonEnv
vf.SandboxEnv packages the pattern above for Prime sandboxes: it kicks off container startup in setup_state and injects handles into tool calls once ready. vf.PythonEnv is the canonical example—review it when building similar sandboxes or remote runtimes.
​
Advanced Patterns
​
Configurable Environments
Accept parameters to customize behavior:

Copy


def load_environment(
    dataset_name="gsm8k",
    num_examples=None,
    difficulty="all",
    use_calculator=False,
    **kwargs
):
    # Load dataset with filtering
    dataset = vf.load_example_dataset(dataset_name)
    if difficulty != "all":
        dataset = dataset.filter(lambda x: x["difficulty"] == difficulty)
    if num_examples:
        dataset = dataset.select(range(num_examples))
    
    # Conditionally add tools
    tools = [calculate] if use_calculator else []
    
    # Return appropriate environment type
    if tools:
        return vf.ToolEnv(dataset=dataset, tools=tools, **kwargs)
    else:
        return vf.SingleTurnEnv(dataset=dataset, **kwargs)
​
Custom Datasets
Load datasets from various sources:

Copy


def load_environment(dataset_path=None, **kwargs):
    if dataset_path:
        # Load from file
        dataset = Dataset.from_json(dataset_path)
    else:
        # Load from Hugging Face
        dataset = load_dataset("owner/dataset-name", split="train")
    
    # Ensure required columns
    assert "prompt" in dataset.column_names
    assert "answer" in dataset.column_names or "info" in dataset.column_names
    
    return vf.SingleTurnEnv(dataset=dataset, **kwargs)
​
Composition with EnvGroup
Combine multiple environments for training on diverse tasks:

Copy


def load_environment(**kwargs):
    # Environment 1: GSM8K
    gsm8k_dataset = vf.load_example_dataset("gsm8k")
    gsm8k_env = vf.SingleTurnEnv(
        dataset=gsm8k_dataset,
        parser=parser,
        rubric=gsm8k_rubric
    )
    
    # Environment 2: MATH
    math_dataset = vf.load_example_dataset("math")
    math_env = vf.SingleTurnEnv(
        dataset=math_dataset,
        parser=parser,
        rubric=math_rubric
    )
    
    # Create grouped environment
    return vf.EnvGroup(
        envs=[gsm8k_env, math_env],
        env_names=["gsm8k", "math"] # stored as "task" column
    )
How EnvGroup Works:
Dataset Concatenation: Combines datasets from all environments with task labels
Automatic Routing: Routes rollouts to the correct environment based on the task column
Unified Scoring: Aggregates scores across all environments
This is particularly useful for:
Training on multiple task types simultaneously
Evaluating general capabilities across domains
Creating curriculum learning setups
​
Installing from Repository
Install environments from the verifiers repository:

Copy


# Install specific environment
vf-install math-python --from-repo

# Install from branch
vf-install wordle --from-repo -b dev

# List available environments
vf-install --list
​
Best Practices
Start Simple: Begin with SingleTurnEnv and basic reward functions
Test Early: Use vf-eval to test your environment during development
Document Well: Include clear README with examples and expected behavior
Handle Errors: Ensure parsers and reward functions handle edge cases
Version Dependencies: Pin specific versions in pyproject.toml
​
Next Steps
See Components for advanced rubrics, tools, parsers, and practical examples
Explore Training to use your environment for model improvement

Add to assistant
Overview


Docs
Components
This guide covers the advanced components available in Verifiers: Rubrics, Tools, and Parsers. Each section includes practical examples of how to use these components in real-world scenarios.
​
Advanced Rubrics
Beyond basic reward functions, Verifiers provides specialized rubric types for complex evaluation scenarios.
​
JudgeRubric: LLM-Based Evaluation
Use language models to evaluate responses when rule-based scoring is insufficient:

Copy


# Basic usage with default prompt
judge_rubric = vf.JudgeRubric()

# Custom evaluation criteria
judge_rubric = vf.JudgeRubric(
    judge_prompt="""Evaluate the response based on:
    1. Accuracy of the solution
    2. Clarity of explanation
    3. Appropriate use of mathematical notation
    
    Rate from 0.0 to 1.0."""
)
Note on Concurrency and Caching
When multiple reward functions rely on the same judge call, JudgeRubric avoids making redundant API requests by caching the judge’s response within the state dictionary for each rollout.
To make this caching effective, JudgeRubric defaults to parallelize_scoring=False. This forces its reward functions to run sequentially, ensuring that the first function makes the API call and populates the cache, while subsequent functions get an instant cache hit.
Example: Multi-Step Math with Judge Evaluation

Copy


def load_environment(**kwargs):
    # Base rubric for correctness
    def check_answer(prompt, response, answer, state):
        final_answer = extract_number(response)
        return 1.0 if abs(final_answer - float(answer)) < 0.01 else 0.0
    
    base_rubric = vf.Rubric(funcs=[check_answer])
    
    # Add judge for solution quality
    judge = vf.JudgeRubric(
        judge_prompt="Evaluate the mathematical reasoning: Is each step justified? Are there logical errors?"
    )
    
    # Combine with RubricGroup
    return vf.SingleTurnEnv(
        dataset=dataset,
        rubric=vf.RubricGroup([base_rubric, judge]),
        **kwargs
    )
​
RubricGroup: Combining Multiple Rubrics
Aggregate scores from different rubrics:

Copy


# Combine different evaluation approaches
group = vf.RubricGroup([
    correctness_rubric,  # Weight: 1.0 (default)
    style_rubric,        # Weight: 1.0
    efficiency_rubric    # Weight: 1.0
])

# With custom weights: set weights inside each Rubric, then group them
correctness = vf.Rubric(funcs=[check_answer], weights=[2.0])
style = vf.Rubric(funcs=[style_score], weights=[1.0])

group = vf.RubricGroup([correctness, style])
Example: Multi-Criteria Code Evaluation

Copy


class CodeEvalEnv(vf.MultiTurnEnv):
    def __init__(self, **kwargs):
        # Rubric 1: Correctness
        correctness = vf.Rubric(funcs=[self.test_correctness])
        
        # Rubric 2: Performance
        performance = vf.Rubric(funcs=[self.measure_performance])
        
        # Rubric 3: Style (via judge)
        style_judge = vf.JudgeRubric(
            judge_prompt="Rate code style: readability, naming, structure (0-1)"
        )
        
        # Combine all rubrics
        super().__init__(
            rubric=vf.RubricGroup([correctness, performance, style_judge]),
            **kwargs
        )
​
ToolRubric: Tracking Tool Usage
Count total and per-tool calls during a rollout. Pass your tool functions to enable per-tool counters. By default, counts are added as metrics with zero weight; adjust reward_weights if you want the counts to affect reward.

Copy


# Define tools (type hints + docstrings omitted for brevity)
def calculate(expr: str) -> float: ...
def search_web(query: str, max_results: int = 5) -> list[dict]: ...

# Initialize with tools to track
tool_rubric = vf.ToolRubric(tools=[calculate, search_web])

# Metrics exposed (names):
# - total_tool_calls
# - calculate_calls
# - search_web_calls

# Optional: turn counts into rewards by setting weights
# Index 0 corresponds to total_tool_calls; subsequent indices follow the tools order
tool_rubric.reward_weights[0] = -0.1   # penalize excessive tool calls
tool_rubric.reward_weights[2] = 0.2    # reward using search_web specifically
​
Rubric.class_objects: Passing Objects to Reward Functions
The class_objects pattern allows you to pass class instances or other objects directly to your reward functions. This is especially useful when your reward functions need access to parsers, clients, or other stateful objects.
How it works:
When a rubric calls a reward function, it automatically merges self.class_objects with the standard arguments (prompt, completion, answer, state, etc.). Your reward functions can then accept these objects as parameters.
Basic Example:

Copy


class CustomRubric(vf.Rubric):
    def __init__(self, api_client, custom_parser, **kwargs):
        super().__init__(**kwargs)
        self.api_client = api_client
        self.custom_parser = custom_parser
        
        # Make objects available to reward functions
        self.class_objects = {
            "parser": self.parser,           # Standard parser (automatically added)
            "api_client": self.api_client,   # Custom API client
            "custom_parser": self.custom_parser,  # Additional parser
        }
        
        # Add reward functions that use these objects
        self.reward_funcs = [self.api_based_reward, self.parsing_reward]

    def api_based_reward(self, completion, answer, api_client, **kwargs):
        """Reward function that uses the API client."""
        # Extract answer from completion
        extracted = self.parser.parse_answer(completion)
        
        # Use API client to validate answer
        validation_result = api_client.validate(extracted, answer)
        return 1.0 if validation_result.is_correct else 0.0
    
    def parsing_reward(self, completion, custom_parser, **kwargs):
        """Reward function that uses a custom parser."""
        # Use the custom parser passed via class_objects
        parsed_data = custom_parser.parse(completion)
        return 1.0 if parsed_data.is_well_formed else 0.0
Real-world Example: JudgeRubric
The built-in JudgeRubric demonstrates this pattern:

Copy


class JudgeRubric(vf.Rubric):
    def __init__(self, judge_client=None, judge_model="gpt-4o-mini", **kwargs):
        super().__init__(**kwargs)
        self.judge_client = judge_client or AsyncOpenAI()
        self.judge_model = judge_model
        
        # Expose objects to reward functions
        self.class_objects = {
            "parser": self.parser,
            "judge": self.judge,  # The judge method itself
            "judge_client": self.judge_client,
            "judge_model": self.judge_model,
        }

    async def judge(self, prompt, completion, answer, state, **kwargs):
        """Judge method that can be called by reward functions."""
        # Implementation uses self.judge_client to evaluate responses
        ...
Usage in Reward Functions:

Copy


# Your reward function can access any object from class_objects
def quality_reward(completion, answer, judge, **kwargs):
    """Use the judge method to evaluate response quality."""
    judgment = await judge(prompt, completion, answer, state)
    return 1.0 if "excellent" in judgment.lower() else 0.5

def format_reward(completion, parser, **kwargs):
    """Use the parser to check formatting."""
    parsed = parser.parse_answer(completion)
    return 1.0 if parsed else 0.0
Best Practices:
Include all necessary objects: Add any parsers, clients, or helpers your reward functions need
Use descriptive names: Choose clear names for your class_objects keys
Document dependencies: Make it clear which reward functions use which objects
Handle missing objects gracefully: Use **kwargs and check for object availability
Advanced Pattern: Shared State

Copy


class StatefulRubric(vf.Rubric):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.shared_state = {"call_count": 0}
        
        self.class_objects = {
            "parser": self.parser,
            "shared_state": self.shared_state,
        }
        
        self.reward_funcs = [self.counting_reward]
    
    def counting_reward(self, completion, shared_state, **kwargs):
        """Track calls across reward function invocations."""
        shared_state["call_count"] += 1
        return 1.0 if shared_state["call_count"] <= 5 else 0.5
​
Tools
Verifiers provides native support for tool calling, leveraging models’ built-in function calling capabilities.
​
Defining Tools
Tools are simple Python functions with type hints, and can be either sync or async:

Copy


def calculate(expression: str) -> float:
    """Evaluate a mathematical expression safely."""
    # Use a safe math parser in production
    import ast
    return eval(expression, {"__builtins__": {}}, {})

async def search_web(query: str, max_results: int = 5) -> list[dict]:
    """Search the web for information.
    
    Args:
        query: Search query string
        max_results: Maximum number of results to return
        
    Returns:
        List of search results with title, snippet, and url
    """
    # Implementation here
    return results
​
Using ToolEnv
ToolEnv automatically converts Python functions to tool schemas and handles tool calling:

Copy


def load_environment(**kwargs):
    return vf.ToolEnv(
        dataset=dataset,
        tools=[calculate, search_web],  # Just pass the functions
        max_turns=10,
        rubric=rubric,
        **kwargs
    )
Note: ToolEnv uses the model’s native tool calling format via the tokenizer’s chat template. It automatically injects tool schemas into request payloads and treats role: tool messages as tool outputs. It does NOT impose any XML structure or require hardcoded patterns.
​
Tool Design Best Practices
Clear Signatures: Use descriptive names and type hints
Comprehensive Docstrings: Models use these to understand tool purpose
Error Handling: Return helpful error messages, don’t raise exceptions
Timeouts: Add timeouts for long-running operations
Input Validation: Validate and sanitize inputs
Example: Wiki Search Environment

Copy


def wiki_search(query: str) -> str:
    """Search Wikipedia for information."""
    try:
        # Add timeout
        with timeout(5.0):
            results = wikipedia.search(query, results=3)
            if results:
                page = wikipedia.page(results[0])
                return f"Title: {page.title}\n\n{page.summary[:500]}..."
            return "No results found."
    except Exception as e:
        return f"Search error: {str(e)}"

def wiki_get_page(title: str) -> str:
    """Get full Wikipedia page content."""
    try:
        with timeout(5.0):
            page = wikipedia.page(title)
            return page.content[:2000]  # Limit length
    except Exception as e:
        return f"Page error: {str(e)}"

def load_environment(**kwargs):
    dataset = load_qa_dataset()  # Questions requiring research
    
    # Rubric rewards correct answers and efficient tool use
    rubric = vf.Rubric(
        funcs=[check_answer, efficiency_bonus],
        weights=[1.0, 0.2]
    )
    
    return vf.ToolEnv(
        dataset=dataset,
        tools=[wiki_search, wiki_get_page],
        max_turns=8,
        rubric=rubric,
        **kwargs
    )
​
Complex Tool Examples
For more sophisticated tool setups, see the wiki_search environment in the repository, which demonstrates:
Multiple interdependent tools
State management across tool calls
Sophisticated error handling
Tool usage optimization
​
Parsers
Parsers extract structured information from model outputs. While many tasks work with raw text, parsers help when you need specific formats.
​
Built-in Parsers
​
XMLParser
Extract XML-tagged content:

Copy


# Define which tags are expected in the output
# Strings define fixed tags; tuples define canonical name + allowed aliases
parser = vf.XMLParser(
    fields=["think", ("answer", "code")],
    answer_field="answer",
)

# In practice
response = "<think>Let me calculate...</think>\n<answer>42</answer>"
answer = parser.parse_answer(response)  # => "42"
​
ThinkParser
Separate reasoning from final answers:

Copy


# Strip any content before </think>, then apply extract_fn

def extract_number(text: str) -> str:
    import re
    m = re.search(r"[-+]?\d*\.?\d+", text)
    return m.group() if m else ""

parser = vf.ThinkParser(extract_fn=extract_number)
​
Custom Parser Patterns
Create domain-specific parsers by extending the base class:
Example: Code Block Parser

Copy


class CodeParser(vf.Parser):
    """Extract and validate code blocks from responses."""
    
    def parse_answer(self, response: str) -> str:
        # Extract code between triple backticks
        import re
        code_blocks = re.findall(r'```(?:python)?\n(.*?)```', response, re.DOTALL)
        
        if not code_blocks:
            return ""
        
        # Return the last code block (usually the final solution)
        code = code_blocks[-1].strip()
        
        # Basic validation
        try:
            compile(code, '<string>', 'exec')
            return code
        except SyntaxError:
            return ""  # Invalid Python code
Example: Math Step Parser

Copy


class MathStepParser(vf.Parser):
    """Parse step-by-step math solutions."""
    
    def parse_answer(self, response: str) -> str:
        lines = response.strip().split('\n')
        
        # Look for final answer patterns
        for line in reversed(lines):
            if any(marker in line.lower() for marker in ['therefore', 'answer:', '=']):
                # Extract number from this line
                import re
                match = re.search(r'[-+]?\d*\.?\d+', line)
                if match:
                    return match.group()
        
        return ""
    
    def get_format_reward_func(self):
        def reward_steps(prompt, response, answer, state):
            # Reward showing work
            steps = response.count('\n')
            return min(1.0, steps / 5)  # Expect ~5 steps
        return reward_steps
​
Parser Integration
Parsers integrate seamlessly with environments and rubrics:

Copy


def load_environment(**kwargs):
    parser = CodeParser()
    
    def code_runs(prompt, response, answer, state):
        code = parser.parse_answer(response)
        if not code:
            return 0.0
        try:
            exec(code)
            return 1.0
        except:
            return 0.0
    
    rubric = vf.Rubric(
        funcs=[code_runs, parser.get_format_reward_func()],
        weights=[1.0, 0.1]
    )
    
    return vf.SingleTurnEnv(
        dataset=dataset,
        parser=parser,
        rubric=rubric,
        **kwargs
    )
​
Practical Examples
​
Interactive Game Environment
Build a Wordle-like game with multi-turn interaction:

Copy


from verifiers.types import Messages, State
from typing import Tuple

class WordleEnv(vf.MultiTurnEnv):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.max_guesses = 6
    
    def env_response(self, messages: Messages, state: State) -> Tuple[Messages, State]:
        if state.get("turn", 0) == 0:
            # First turn: initialize
            state["turn"] = 1
            state["target"] = state["answer"]
            state["guesses"] = []
            return [{"role": "user", "content": "Guess a 5-letter word. You have 6 attempts."}], state
        
        # Get the last assistant message
        last_msg = messages[-1]
        if last_msg["role"] != "assistant":
            return [], state  # No response if not assistant message
            
        guess = last_msg["content"].strip().upper()
        target = state["target"]
        
        # Validate guess
        if len(guess) != 5 or not guess.isalpha():
            return [{"role": "user", "content": "Please guess a 5-letter word."}], state
        
        # Generate feedback
        feedback = self.get_feedback(guess, target)
        state["guesses"].append(guess)
        state["turn"] += 1
        
        if guess == target:
            state["solved"] = True
            return [{"role": "user", "content": f"Correct! The word was {target}."}], state
        elif state["turn"] > self.max_guesses:
            state["failed"] = True
            return [{"role": "user", "content": f"Out of guesses. The word was {target}."}], state
        else:
            remaining = self.max_guesses - state["turn"] + 1
            return [{"role": "user", "content": f"{feedback}\n{remaining} guesses remaining."}], state
    
    def is_completed(self, messages: Messages, state: State) -> bool:
        return state.get("solved", False) or state.get("failed", False)
​
Training Data Generation
Generate training data using environment rollouts:

Copy


async def generate_training_data(env, client, model, num_samples=1000):
    """Generate diverse solutions for training."""
    results = []
    
    for i in range(num_samples):
        # Get a random prompt
        prompt = env.dataset[i]["prompt"]
        answer = env.dataset[i]["answer"]
        
        # Generate multiple solutions
        for temp in [0.3, 0.7, 1.0]:
            completion, state = await env.rollout(
                client=client,
                model=model,
                prompt=prompt,
                answer=answer,
                sampling_args={"temperature": temp, "max_tokens": 1000}
            )
            
            # Score the solution
            rewards = await env.rubric.score_rollout(
                prompt, completion, answer, state
            )
            
            # Save high-quality solutions
            if rewards["total"] > 0.8:
                results.append({
                    "prompt": prompt,
                    "completion": completion,
                    "score": rewards["total"]
                })
    
    return Dataset.from_list(results)
​
Environment Composition
Build complex environments from simpler ones:

Copy


def load_math_suite(**kwargs):
    """Comprehensive math environment covering multiple domains."""
    
    # Shared components
    parser = vf.ThinkParser(extract_fn=extract_boxed_answer)
    
    # Basic arithmetic
    arithmetic_env = vf.SingleTurnEnv(
        dataset=load_arithmetic_dataset(),
        parser=parser,
        rubric=vf.Rubric(funcs=[exact_match]),
        system_prompt="Solve the arithmetic problem."
    )
    
    # Algebra with tools
    algebra_env = vf.ToolEnv(
        dataset=load_algebra_dataset(),
        tools=[solve_equation, factor_polynomial],
        parser=parser,
        rubric=vf.Rubric(funcs=[check_algebra, tool_efficiency])
    )
    
    # Geometry with judge
    geometry_env = vf.SingleTurnEnv(
        dataset=load_geometry_dataset(),
        parser=parser,
        rubric=vf.RubricGroup([
            vf.Rubric(funcs=[check_geometry]),
            vf.JudgeRubric(judge_prompt="Rate the geometric reasoning and diagram interpretation.")
        ])
    )
    
    # Combine all
    return vf.EnvGroup(
        envs=[arithmetic_env, algebra_env, geometry_env],
        env_names=["arithmetic", "algebra", "geometry"],
        **kwargs
    )
​
Best Practices
​
For Rubrics
Start simple with basic reward functions
Use JudgeRubric when rule-based evaluation is insufficient
Combine rubrics with RubricGroup for multi-faceted evaluation
Test reward functions thoroughly with edge cases
​
For Tools
Keep tool functions simple and focused
Use clear names and comprehensive docstrings
Handle errors gracefully - return messages, don’t raise
Add timeouts for external operations
Let the model’s chat template handle tool calling format
​
For Parsers
Use built-in parsers when they fit your needs
Create custom parsers for domain-specific formats
Always handle parsing failures gracefully
Consider providing format rewards to guide model output
​
Next Steps
Build your own environments using these components in Environments
Train models with your environments in Training
Understand the type system in Type Reference

Add to assistant
Environments


Docs
Training
This guide covers RL training with the included trainer and vLLM inference, orchestrated via a single TOML config.
​
Training options
You can train with the included RLTrainer (via vf-rl) or with external projects like prime-rl.
Use the included trainer when you want a simple, hackable training loop and LoRA-first defaults.
Use prime-rl when you want FSDP-first orchestration and large-scale features.
​
Summary of similarities and differences
Similarities
OpenAI-compatible inference (vLLM) and async rollouts
One-step off-policy overlap by default (generate at step n-1 while training at step n)
Differences
RLTrainer: Accelerate/DeepSpeed-based; optional LoRA/PEFT; easy to script and extend in Python
PRIME-RL: FSDP-first; rl entrypoint; strong checkpointing; extensive CLI/TOML configuration
​
Train with vf-rl (included trainer)
The included trainer runs alongside a vLLM server, managed automatically by vf-rl inside a tmux session. Configure everything in a single TOML.
​
Quick Start
Install RL extras and set up default configs:

Copy


uv add 'verifiers[rl]'
uv run vf-setup
Launch training from a TOML (tmux with vLLM + trainer panes):

Copy


uv run vf-rl @ configs/rl/config.toml
​
TOML Configuration
Minimal TOML example:

Copy


model = "Qwen/Qwen3-4B-Instruct-2507"

[env]
id = "kalomaze/alphabet-sort" # auto-installed from hub if given as user/env-id, or from local project if given as env-id

[inference]
gpus = 1

[inference.args]
enforce_eager = true

[trainer]
gpus = 1

[trainer.args]
run_name = "alphabet-sort"
use_lora = true
learning_rate = 1e-5
micro_batch_size = 4
rollouts_per_example = 16
batch_size = 512
max_steps = 100
max_tokens = 512
max_seq_len = 2048
​
Key Hyperparameters
​
Batch Configuration
Key fields in [trainer.args]:
rollouts_per_example: completions per prompt (group size)
micro_batch_size: rollouts per GPU per step
batch_size: rollouts per global batch (must be divisible by micro_batch_size * world_size)
How to think about batch settings:
num_generations: Larger groups (16-32) increase reward diversity but use more memory
per_device_train_batch_size: Limited by GPU memory after model weights
gradient_accumulation_steps: Use to achieve larger effective batch sizes
​
Generation Parameters
Specify in [trainer.args]:
max_tokens (per-turn), temperature, top_p, top_k, min_p, repetition_penalty
max_prompt_len, max_seq_len
Generation strategy:
High temperature (0.8-1.0) increases diversity within groups
Consider your model’s context window when setting lengths
Longer completions allow more complex reasoning but increase memory usage
​
Training Schedule
Core fields in [trainer.args]:
learning_rate, lr_scheduler_type, warmup_steps, max_steps
max_grad_norm, bf16, gradient_checkpointing
​
Async Generation
RLTrainer is asynchronous (one step off-policy) by default. Generation is controlled via [trainer.args] and the environment:
generation_timeout, max_concurrent
​
Evaluation During Training
Set eval_strategy/eval_steps in [trainer.args] and provide an eval split via your environment configuration if supported.
​
Parameter-Efficient Training
LoRA is enabled by default; configure via [trainer.args] fields like use_lora, lora_rank, lora_alpha, lora_dropout, and optionally lora_target_modules.
​
RL Rules of Thumb
RL is notoriously sensitive to implementation details. Here’s practical guidance:
​
Before Training
Evaluate baseline performance: If your model gets 0% reward after 10+ attempts, the task is too hard
Check task difficulty: If baseline is already 80%+, consider harder examples
Ensure reward diversity: You want varied scores within each generation group
​
Stability vs Performance Trade-offs
For more aggressive training (higher risk of collapse):
Set beta = 0 (no KL penalty)
Increase learning rate (2e-6 to 5e-6)
Increase num_iterations (2-4)
For more stable training (slower progress):
Increase num_generations (32-64)
Increase batch size via gradient_accumulation_steps
Decrease max_grad_norm (0.001-0.005)
Use larger models (14B+)
Keep num_iterations = 1 (stay on-policy)
​
Best Practices
Likely beneficial:
Learning rate warmup (10-20 steps minimum)
Periodic reference model updates for 500+ step runs
One-step off-policy training (num_batches_ahead = 1)
Context-dependent:
High beta values (0.1+) - more conservative
Overlong filtering - depends on task
Tool response masking - useful for multi-turn
Key insight: The best way to improve training is ensuring appropriate task difficulty for your model - not too easy, not too hard.
​
Troubleshooting
​
Common Issues
Non-Increasing Chat Templates: The Qwen3 and DeepSeek-R1 model series both remove <think> sections from messages when processing inputs, which violates the increasing context requirement for multi-turn training. We provide versions of many of these models with modified chat templates here.
OOM during generation:
Reduce num_generations or per_device_train_batch_size
Use LoRA instead of full finetuning
Check vLLM server has sufficient memory
Training instability:
Reduce learning rate
Decrease max_grad_norm
Increase beta for stronger KL regularization
Poor reward diversity:
Increase temperature
Check if task difficulty matches model capability
Ensure your rubric differentiates quality levels
​
Infrastructure
Ensure huggingface and wandb logins are configured
Set OPENAI_API_KEY (can be dummy for vLLM)
Increase ulimit for high concurrency: ulimit -n 4096
For NCCL issues: try NCCL_P2P_DISABLE=1
​
Advanced Configuration
​
Custom Sampling

Copy


# Fine-grained generation control
args.repetition_penalty = 1.1   # Reduce repetition
args.top_k = 50                # Limit vocabulary
args.min_p = 0.05              # Min probability threshold
​
Resource Optimization

Copy


# Memory-constrained settings
args.gradient_checkpointing = True
args.ds3_gather_for_generation = False  # For very large models
args.generation_batch_size = 16  # Control generation batch size
​
Monitoring

Copy


# Logging configuration
args.logging_steps = 1
args.log_completions = True
args.report_to = "wandb"  # or "none" to disable
args.num_completions_to_print = 5  # Sample size to log
​
Train with PRIME-RL
If you prefer an FSDP-first setup with higher throughput, you can train the same verifiers Environments using prime-rl.
Install prime-rl (see its README for CUDA requirements):

Copy


curl -sSL https://raw.githubusercontent.com/PrimeIntellect-ai/prime-rl/main/scripts/install.sh | bash
Create or install a Verifiers Environment module (inside your prime-rl checkout if developing there):

Copy


# create a new Environment template
uv run vf-init vf-custom-environment

# OR install an existing Environment from this repo
uv run vf-install vf-math-python --from-repo
Configure the orchestrator to use your Environment. In your orchestrator TOML (e.g. configs/my_exp/orch.toml):

Copy


[environment]
id = "vf-math-python"  # or your custom environment ID

[environment.args]
# Example args forwarded to the Environment
split = "train"
rollouts_per_example = 8
max_concurrent = 512
Launch a single-node run (adjust GPU split to your hardware):

Copy


uv run rl \
  --trainer @ configs/my_exp/train.toml \
  --orchestrator @ configs/my_exp/orch.toml \
  --inference @ configs/my_exp/infer.toml \
  --trainer-gpus 2 --inference-gpus 6
Tips:
Use bash scripts/tmux.sh in prime-rl to open a panes layout for trainer/orchestrator/inference logs.
Log to W&B by adding --wandb.project <proj> --wandb.name <run> on uv run rl (shared to trainer + orchestrator).
For checkpointing/resume, see the prime-rl README (supports step-tagged checkpoints across trainer/orchestrator).
​
Next Steps
Explore Environments to create custom tasks
Review Components for advanced patterns
See the examples directory on GitHub for complete training scripts

Add to assistant
Components

Docs
Development
This guide covers development setup, testing, and contributing to the verifiers package.
​
Setup
​
Prerequisites
Python 3.11 or 3.12
uv package manager
​
Installation

Copy


# Clone and install for development
git clone https://github.com/PrimeIntellect-ai/verifiers.git
cd verifiers
uv sync --all-extras
uv run pre-commit install
​
Project Structure

Copy


verifiers/
├── verifiers/          # Main package
│   ├── envs/           # Environment classes
│   ├── parsers/        # Parser classes  
│   ├── rubrics/        # Rubric classes
│   └── utils/          # Utilities
├── environments/       # Installable environment modules
├── examples/           # Usage examples
├── tests/              # Test suite
└── docs/               # Documentation
​
Running Tests

Copy


# Run all tests
uv run pytest tests/

# Run with coverage
uv run pytest tests/ --cov=verifiers --cov-report=html

# Run specific test file
uv run pytest tests/test_parsers.py

# Stop on first failure with verbose output
uv run pytest tests/ -xvs

# Run tests matching a pattern
uv run pytest tests/ -k "xml_parser"
The test suite includes 130+ tests covering parsers, rubrics, and environments. The test suite does not currently cover example environments or the trainer. If you require robust performance guarantees for training, you will likely want to use prime-rl.
​
Writing Tests
​
Test Structure

Copy


class TestFeature:
    """Test the feature functionality."""
    
    def test_basic_functionality(self):
        """Test normal operation."""
        # Arrange
        feature = Feature()
        
        # Act
        result = feature.process("input")
        
        # Assert
        assert result == "expected"
    
    def test_error_handling(self):
        """Test error cases."""
        with pytest.raises(ValueError):
            Feature().process(invalid_input)
​
Using Mocks
The test suite provides mock OpenAI clients:

Copy


from tests.mock_openai_client import MockOpenAIClient

def test_with_mock(mock_client):
    env = vf.SingleTurnEnv(client=mock_client)
    # Test without real API calls
​
Guidelines
Test both success and failure cases
Use descriptive test names that explain what’s being tested
Leverage existing fixtures from conftest.py
Group related tests in test classes
Keep tests fast - use mocks instead of real API calls
Tip: When subclassing MultiTurnEnv, always call await super().is_completed(...) (or await self.max_turns_reached(state)) so shared guards—especially max turn limits—remain effective.
​
Contributing
​
Workflow
Fork the repository
Create a feature branch: git checkout -b feature-name
Make changes following existing patterns
Add tests for new functionality
Run tests: uv run pytest tests/
Update docs if adding/changing public APIs
Submit PR with clear description
​
Code Style
Follow existing conventions in the codebase
Use type hints for function parameters and returns
Write docstrings for public functions/classes
Keep functions focused and modular
​
PR Checklist
 Tests pass locally
 Added tests for new functionality
 Updated documentation if needed
 No breaking changes (or clearly documented)
​
Common Issues
​
Import Errors

Copy


# Ensure package is installed in development mode
uv pip install -e .
​
Async Test Issues

Copy


# May need nest-asyncio for some environments
uv add nest-asyncio
​
Test Failures

Copy


# Debug specific test
uv run pytest tests/test_file.py::test_name -vvs --pdb
​
Environment Development
​
Creating a New Environment Module

Copy


# Initialize template
vf-init my-environment

# Install locally for testing
vf-install my-environment

# Test your environment
vf-eval my-environment -m gpt-4.1-mini -n 5
​
Environment Module Structure

Copy


# my_environment.py
import verifiers as vf

def load_environment(**kwargs):
    """Load the environment."""
    dataset = vf.load_example_dataset("dataset_name")
    parser = vf.XMLParser(fields=["reasoning", "answer"])
    
    def reward_func(parser, completion, answer, **kwargs):
        return 1.0 if parser.parse_answer(completion) == answer else 0.0
    
    rubric = vf.Rubric(
        funcs=[reward_func, parser.get_format_reward_func()],
        weights=[1.0, 0.2],
        parser=parser
    )
    
    return vf.SingleTurnEnv(
        dataset=dataset,
        parser=parser,
        rubric=rubric,
        **kwargs
    )
​
Quick Reference
​
Essential Commands

Copy


# Development setup
uv sync --all-extras

# Run tests
uv run pytest tests/                    # All tests
uv run pytest tests/ -xvs              # Debug mode
uv run pytest tests/ --cov=verifiers   # With coverage

# Environment tools
vf-init new-env                        # Create environment
vf-install new-env                     # Install environment
vf-eval new-env                        # Test environment
vf-tui                                 # Browse eval results in your terminal

# Documentation
cd docs && make html                   # Build docs
​
Project Guidelines
Environments: Installable modules with load_environment() function
Parsers: Extract structured data from model outputs
Rubrics: Define multi-criteria evaluation functions
Tests: Comprehensive coverage with mocks for external dependencies
For more details, see the full documentation at readthedocs.

Add to assistant
Training


Docs
Api reference
This guide explains the key types and data structures in Verifiers.
​
Core Types
​
Pydantic Models
Verifiers uses Pydantic models for structured data:

Copy


from pydantic import BaseModel

class GenerateInputs(BaseModel):
    """Pydantic model for generation inputs."""

    prompt: list[Messages]
    answer: list[str] | None = None
    info: list[dict] | None = None
    task: list[str] | None = None
    completion: list[Messages] | None = None
    id: list[int] | None = None

class GenerateOutputs(BaseModel):
    """Pydantic model for generation outputs."""

    prompt: list[Messages]
    completion: list[Messages]
    answer: list[str]
    state: list[State]
    info: list[Info]
    task: list[str]
    id: list[int]
    reward: list[float]
    metrics: dict[str, list[float]] = Field(default_factory=dict)
    metadata: GenerateMetadata

class RolloutScore(BaseModel):
    """Pydantic model for rollout scores."""

    reward: float
    metrics: dict[str, float] = Field(default_factory=dict)


class RolloutScores(BaseModel):
    """Pydantic model for rubric outputs."""

    reward: list[float]
    metrics: dict[str, list[float]] = Field(default_factory=dict)


class ProcessedOutputs(BaseModel):
    """Pydantic model for processed outputs."""

    prompt_ids: list[list[int]]
    prompt_mask: list[list[int]]
    completion_ids: list[list[int]]
    completion_mask: list[list[int]]
    completion_logprobs: list[list[float]]
    rewards: list[float]
GenerateOutputs.metadata captures run-level context (environment + arguments, model + sampling configuration, summary statistics, and the resolved save path) so downstream tooling can reproduce or resume evaluations without guessing defaults.
​
State Dictionary
The State object tracks rollout information throughout an interaction:

Copy


State = dict[str, Any]

# Common state fields during rollout:
{
    "prompt": list[ChatMessage],      # Original prompt messages
    "completion": list[ChatMessage],  # Model's response messages
    "answer": str,                    # Ground truth answer
    "task": str,                      # Task identifier (for EnvGroup)
    "info": dict[str, Any],          # Additional metadata from dataset
    "responses": list[Any],          # Raw LLM response objects
    "example_id": int,                # Row identifier from the dataset
    "timing": dict[str, float],       # Timing information for generation and scoring
    
    # Custom fields added by specific environments:
    "turn": int,                     # Current turn number (MultiTurnEnv)
    "tools_called": list[str],       # Tool invocations (ToolEnv)
    "game_state": Any,               # Game-specific state
}
The responses field contains raw API response objects with:
choices[0].logprobs.content: Token-level log probabilities
choices[0].logprobs.token_ids: Token IDs
choices[0].finish_reason: Why generation stopped
usage: Token usage statistics
​
Message Formats

Copy


# Import from verifiers.types
from verifiers.types import ChatMessage, Messages

# Chat format (recommended)
# ChatMessage is a dict with these fields:
ChatMessage = {
    "role": str,                    # "system", "user", or "assistant"
    "content": str,                 # Message text
    "tool_calls": list[...],        # Optional tool calls
    "tool_call_id": str,            # Optional tool call ID
}

Messages = str | list[ChatMessage]  # Can be string (completion) or chat

# Example chat format:
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is 2+2?"},
    {"role": "assistant", "content": "2+2 equals 4."}
]

# Completion format (legacy):
completion = "Q: What is 2+2?\nA: 4"
​
Reward Function Signature
All reward functions must follow this signature:

Copy


from collections.abc import Awaitable, Callable

RewardFunc = Callable[..., float | Awaitable[float]]

def my_reward_func(
    completion: Messages,            # Model's response (chat or string)
    answer: str = "",                # Ground truth answer
    prompt: Messages | None = None,  # Original prompt
    state: State | None = None,      # Environment state
    parser: Parser | None = None,    # Parser instance (if rubric has one)
    **kwargs                         # Additional arguments
) -> float:
    """Return a float reward between 0.0 and 1.0."""
    return 1.0
​
Environment Response
For MultiTurnEnv.env_response:

Copy


def env_response(
    self,
    messages: list[ChatMessage],
    state: State,
    **kwargs
) -> tuple[Messages, State]:
    """
    Returns:
        - Response messages (list[ChatMessage] or str for completion mode)
        - Updated state
    """
    # Return a list of ChatMessage dicts (typical case)
    response = [{"role": "user", "content": "Environment feedback"}]
    
    # Update state as needed
    state["turn"] = state.get("turn", 0) + 1
    state["last_action"] = "provided feedback"
    
    return response, state
​
Sampling Arguments
vLLM-specific generation parameters:

Copy


SamplingArgs = dict[str, Any]

sampling_args = {
    # Basic sampling
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 50,
    "max_tokens": 2048,
    
    # Advanced vLLM options
    "extra_body": {
        "logprobs": True,              # Return token logprobs
        "top_logprobs": 5,             # Top-k logprobs per token
        "skip_special_tokens": False,  # Include special tokens
        "guided_decoding": {           # Structured generation
            "regex": r"\d{3}-\d{3}-\d{4}"  # Phone number format
        }
    }
}
​
Dataset Info
The info field in datasets can contain arbitrary metadata:

Copy


Info = dict[str, Any]

# Dataset row with info dict:
{
    "prompt": "Solve this problem",
    "info": {
        "answer": "42",              # Required: ground truth
        "difficulty": "medium",      # Optional metadata
        "source": "textbook",
        "chapter": 3,
        "requires_tool": True
    }
}

# Access in reward functions:
def reward_func(completion, answer, info=None, **kwargs):
    difficulty = info.get("difficulty", "unknown") if info else "unknown"
    # Adjust scoring based on difficulty...
​
Type Utilities
​
Environment Rollout Types

Copy


# Rollout returns
async def rollout(...) -> tuple[Messages, State]:
    """Returns (completion, final_state)"""

# Evaluation results
async def evaluate(...) -> GenerateOutputs:
    """Async interface that returns GenerateOutputs."""

# Synchronous convenience wrappers
def evaluate_sync(...) -> GenerateOutputs:
    """Blocking helper that wraps the async evaluate coroutine."""

# Generation results
async def generate(...) -> GenerateOutputs:
    """Async interface that returns GenerateOutputs containing rollout data"""

def generate_sync(...) -> GenerateOutputs:
    """Blocking helper for integrate-with-sync-code scenarios"""
​
Parser Types

Copy


# Parser return types can be anything
def parse(text: str) -> Any:
    """Can return str, dict, dataclass, etc."""

# parse_answer must return optional string
def parse_answer(completion: Messages) -> str | None:
    """Must return string answer or None"""
​
Common Patterns
​
Accessing Completion Content

Copy


def get_text_content(completion: Messages) -> str:
    """Extract text from either format."""
    if isinstance(completion, str):
        return completion
    else:
        # Chat format - get last assistant message
        return completion[-1]["content"]
​
State Initialization

Copy


def reset_for_rollout(self, prompt: Messages, answer: str, info: Info | None) -> State:
    """Initialize state for new rollout."""
    state = {
        "prompt": prompt,
        "answer": answer,
        "info": info or {},
        "task": info.get("task", "default") if info else "default",
        "responses": [],
        # Add custom fields
        "turn": 0,
        "history": []
    }
    return state




PROMPT / QUESTION EXAMPLES:

```

To run a quick evaluation of your Environment with an API-based model, do:
```bash
uv run vf-eval environment-name -s # run and save eval results locally
# vf-eval -h for config options; defaults to gpt-4.1-mini, 5 prompts, 3 rollouts for each
```

If you're using Prime Intellect infrastructure, the [`prime` CLI](https://github.com/PrimeIntellect-ai/prime-cli) provides first-class commands for working with Verifiers environments through the [Environments Hub](https://docs.primeintellect.ai/tutorials-environments/environments). Install it with `uv tool install prime`, authenticate via `prime login`, then use `prime env push` to publish your package and `prime env install owner/name` (optionally pinning a version) to consume it from pods or local machines.

The core elements of Environments are:
- Datasets: a Hugging Face `Dataset` with a `prompt` column for inputs, and optionally `answer (str)` or `info (dict)` columns for evaluation (both can be omitted for environments that evaluate based solely on completion quality)
- Rollout logic: interactions between models and the environment (e.g. `env_response` + `is_completed` for any `MultiTurnEnv`)
- Rubrics: an encapsulation for one or more reward functions
- Parsers: optional; an encapsulation for reusable parsing logic

We support both `/v1/chat/completions`-style and `/v1/completions`-style inference via OpenAI clients, though we generally recommend `/v1/chat/completions`-style inference for the vast majority of applications. Both  `prime-rl` as well as the included `vf.RLTrainer` support the full set of [SamplingParams](https://docs.vllm.ai/en/stable/api/vllm/sampling_params.html#vllm.sampling_params.SamplingParams) exposed by vLLM (via their OpenAI-compatible [server](https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html) interface), and leveraging this will often be the appropriate way to implement rollout strategies requiring finer-grained control, such as interrupting and resuming generations for interleaved tool use, or enforcing reasoning budgets.


### SingleTurnEnv

For tasks requiring only a single response from a model for each prompt, you can use `SingleTurnEnv` directly by specifying a Dataset and a Rubric. Rubrics are sets of reward functions, which can be either sync or async.

```python
from datasets import load_dataset
import verifiers as vf

dataset = load_dataset("my-account/my-dataset", split="train")

def reward_A(prompt, completion, info) -> float:
	# reward fn, e.g. correctness
	...

def reward_B(parser, completion) -> float:
	# auxiliary reward fn, e.g. format

vf_env.make_dataset(results) # HF dataset format
```

Datasets should be formatted with columns for:
- `'prompt' (List[ChatMessage])` OR `'question' (str)` fields
	- `ChatMessage` = e.g. `{'role': 'user', 'content': '...'}`
	- if `question` is set instead of `prompt`, you can also pass `system_prompt (str)` and/or `few_shot (List[ChatMessage])`
- `answer (str)` AND/OR `info (dict)` (both optional, can be omitted entirely)
- `task (str)`: optional, used by `EnvGroup` and `RubricGroup` for orchestrating composition of Environments and Rubrics

The following named attributes available for use by reward functions in your Rubric:
- `prompt`: sequence of input messages
- `completion`: sequence of messages generated during rollout by model and Environment
- `answer`: primary answer column, optional (defaults to empty string if omitted)
- `state`: can be modified during rollout to accumulate any metadata (`state['responses']` includes full OpenAI response objects by default)
- `info`: auxiliary info needed for reward computation (e.g. test cases), optional (defaults to empty dict if omitted)
- `task`: tag for task type (used by `EnvGroup` and `RubricGroup`)
- `parser`: the parser object declared. Note: `vf.Parser().get_format_reward_func()` is a no-op (always 1.0); use `vf.ThinkParser` or a custom parser if you want a real format adherence reward.

**Note**: Some environments can fully evaluate using only `prompt`, `completion`, and `state` without requiring ground truth `answer` or `info` data. Examples include format compliance checking, completion quality assessment, or length-based rewards.

For tasks involving LLM judges, you may wish to use `vf.JudgeRubric()` for managing requests to auxiliary models.


### ToolEnv


```python
import verifiers as vf
vf_env = vf.ToolEnv(
	dataset= ... # HF Dataset with 'prompt'/'question' and optionally 'answer'/'info' columns
	rubric= ... # Rubric object; vf.ToolRubric() can be optionally used for counting tool invocations in each rollout
	tools=[search_tool, read_article_tool, python_tool], # python functions with type hints + docstrings
	max_turns=10
)
```
- **Hardcode**: Dataset names, URLs, defaults  
- **Arguments**: Only for essential customization via `load_environment()`
- **State keys**: Only rely on what your env explicitly sets

### State Management
Default state includes: `prompt`, `completion`, `responses`, `turn`, `timing`, `task`, `info`. 
Only depend on keys your environment explicitly manages.


### Checklist
- Guidelines here are followed
from pydantic import BaseModel

class GenerateInputs(BaseModel):
    """Pydantic model for generation inputs."""

    prompt: list[Messages]
    answer: list[str] | None = None
    info: list[dict] | None = None
    task: list[str] | None = None
    completion: list[Messages] | None = None
    id: list[int] | None = None

class GenerateOutputs(BaseModel):
    """Pydantic model for generation outputs."""

    prompt: list[Messages]
    completion: list[Messages]
    answer: list[str]
    state: list[State]
    info: list[Info]
    task: list[str]


class ProcessedOutputs(BaseModel):
    """Pydantic model for processed outputs."""

    prompt_ids: list[list[int]]
    prompt_mask: list[list[int]]
    completion_ids: list[list[int]]
    completion_mask: list[list[int]]
    completion_logprobs: list[list[float]]
    rewards: list[float]
```
```python
State = dict[str, Any]

# Common state fields during rollout:
{
    "prompt": list[ChatMessage],      # Original prompt messages
    "completion": list[ChatMessage],  # Model's response messages
    "answer": str,                    # Ground truth answer
    "task": str,                      # Task identifier (for EnvGroup)
    "info": dict[str, Any],          # Additional metadata from dataset
    "responses": list[Any],          # Raw LLM response objects
RewardFunc = Callable[..., float | Awaitable[float]]

def my_reward_func(
    completion: Messages,            # Model's response (chat or string)
    answer: str = "",                # Ground truth answer
    prompt: Messages | None = None,  # Original prompt
    state: State | None = None,      # Environment state
    parser: Parser | None = None,    # Parser instance (if rubric has one)
    **kwargs                         # Additional arguments
) -> float:
    """Return a float reward between 0.0 and 1.0."""
```python
Info = dict[str, Any]

# Dataset row with info dict:
{
    "prompt": "Solve this problem",
    "info": {
        "answer": "42",              # Required: ground truth
        "difficulty": "medium",      # Optional metadata
        "source": "textbook",
        "chapter": 3,
```

### State Initialization

```python
def reset_for_rollout(self, prompt: Messages, answer: str, info: Info | None) -> State:
    """Initialize state for new rollout."""
    state = {
        "prompt": prompt,
        "answer": answer,
        "info": info or {},
        "task": info.get("task", "default") if info else "default",
        "responses": [],
        # Add custom fields
### JudgeRubric: LLM-Based Evaluation

Use language models to evaluate responses when rule-based scoring is insufficient:

```python
# Basic usage with default prompt
judge_rubric = vf.JudgeRubric()

# Custom evaluation criteria
judge_rubric = vf.JudgeRubric(
    judge_prompt="""Evaluate the response based on:
    1. Accuracy of the solution
    2. Clarity of explanation
    3. Appropriate use of mathematical notation
    
    Rate from 0.0 to 1.0."""
**Example: Multi-Step Math with Judge Evaluation**

```python
def load_environment(**kwargs):
    # Base rubric for correctness
    def check_answer(prompt, response, answer, state):
        final_answer = extract_number(response)
        return 1.0 if abs(final_answer - float(answer)) < 0.01 else 0.0
    
    base_rubric = vf.Rubric(funcs=[check_answer])
    
    # Add judge for solution quality
    judge = vf.JudgeRubric(
        judge_prompt="Evaluate the mathematical reasoning: Is each step justified? Are there logical errors?"
    )
    
    # Combine with RubricGroup
    return vf.SingleTurnEnv(
        dataset=dataset,
        # Rubric 2: Performance
        performance = vf.Rubric(funcs=[self.measure_performance])
        
        # Rubric 3: Style (via judge)
        style_judge = vf.JudgeRubric(
            judge_prompt="Rate code style: readability, naming, structure (0-1)"
        )
        
        # Combine all rubrics
        super().__init__(
            rubric=vf.RubricGroup([correctness, performance, style_judge]),

The `class_objects` pattern allows you to pass class instances or other objects directly to your reward functions. This is especially useful when your reward functions need access to parsers, clients, or other stateful objects.

**How it works:**

When a rubric calls a reward function, it automatically merges `self.class_objects` with the standard arguments (`prompt`, `completion`, `answer`, `state`, etc.). Your reward functions can then accept these objects as parameters.

**Basic Example:**

```python
class CustomRubric(vf.Rubric):
            "judge": self.judge,  # The judge method itself
            "judge_client": self.judge_client,
            "judge_model": self.judge_model,
        }

    async def judge(self, prompt, completion, answer, state, **kwargs):
        """Judge method that can be called by reward functions."""
        # Implementation uses self.judge_client to evaluate responses
        ...
```


```python
# Your reward function can access any object from class_objects
def quality_reward(completion, answer, judge, **kwargs):
    """Use the judge method to evaluate response quality."""
    judgment = await judge(prompt, completion, answer, state)
    return 1.0 if "excellent" in judgment.lower() else 0.5

def format_reward(completion, parser, **kwargs):
    """Use the parser to check formatting."""
    parsed = parser.parse_answer(completion)
            return page.content[:2000]  # Limit length
    except Exception as e:
        return f"Page error: {str(e)}"

def load_environment(**kwargs):
    dataset = load_qa_dataset()  # Questions requiring research
    
    # Rubric rewards correct answers and efficient tool use
    rubric = vf.Rubric(
        funcs=[check_answer, efficiency_bonus],
        weights=[1.0, 0.2]
                    return match.group()
        
        return ""
    
    def get_format_reward_func(self):
        def reward_steps(prompt, response, answer, state):
            # Reward showing work
            steps = response.count('\n')
            return min(1.0, steps / 5)  # Expect ~5 steps
        return reward_steps
```

```python
def load_environment(**kwargs):
    parser = CodeParser()
    
    def code_runs(prompt, response, answer, state):
        code = parser.parse_answer(response)
        if not code:
            return 0.0
        try:
            exec(code)
async def generate_training_data(env, client, model, num_samples=1000):
    """Generate diverse solutions for training."""
    results = []
    
    for i in range(num_samples):
        # Get a random prompt
        prompt = env.dataset[i]["prompt"]
        answer = env.dataset[i]["answer"]
        
        # Generate multiple solutions
        for temp in [0.3, 0.7, 1.0]:
            completion, state = await env.rollout(
                client=client,
                model=model,
                prompt=prompt,
                answer=answer,
                sampling_args={"temperature": temp, "max_tokens": 1000}
            )
            
            # Score the solution
            rewards = await env.rubric.score_rollout(
                prompt, completion, answer, state
            )
            
            # Save high-quality solutions
            if rewards["total"] > 0.8:
                results.append({
                    "prompt": prompt,
                    "completion": completion,
                    "score": rewards["total"]
                })
    
    return Dataset.from_list(results)
    # Basic arithmetic
    arithmetic_env = vf.SingleTurnEnv(
        dataset=load_arithmetic_dataset(),
        parser=parser,
        rubric=vf.Rubric(funcs=[exact_match]),
        system_prompt="Solve the arithmetic problem."
    )
    
    # Algebra with tools
    algebra_env = vf.ToolEnv(
        dataset=load_algebra_dataset(),
    geometry_env = vf.SingleTurnEnv(
        dataset=load_geometry_dataset(),
        parser=parser,
        rubric=vf.RubricGroup([
            vf.Rubric(funcs=[check_geometry]),
            vf.JudgeRubric(judge_prompt="Rate the geometric reasoning and diagram interpretation.")
        ])
    )
    
    # Combine all
    return vf.EnvGroup(
    
    # 2. Configure parser
    parser = vf.ThinkParser()
    
    # 3. Define reward functions -- can automatically reference:
    # - parser, prompt, completion, answer, state , task, info 
    def correct_answer(parser, completion, answer):
        response = parser.parse_answer(completion) or ''
        return 1.0 if response.strip() == answer.strip() else 0.0
    
    # 4. Create rubric
    )
    
    # 5. Return configured environment
    return vf.SingleTurnEnv(
        dataset=dataset,
        system_prompt="Think step-by-step, then give your answer.",
        parser=parser,
        rubric=rubric,
        **kwargs  # Pass through additional arguments
    )
```
Prefer `AsyncOpenAI` + `await env.evaluate(...)` for fully async workflows; the sync helper is ideal when integrating into existing blocking scripts. Intermediate saving via `save_every` requires the default interleaved scoring pipeline.

### 3. Iterate on Design

Common iterations:
- Adjust system prompts for better performance
- Refine parser logic for edge cases
- Add new reward functions to the rubric
- Configure dataset filtering or sampling

## Using Prime CLI and the Environments Hub
### Basic Reward Functions

A reward function takes the full context and returns a score (typically 0.0 to 1.0):

```python
def exact_match(prompt, completion, answer, state):
    """Reward exact matches."""
    response = completion[-1]['content']
    return 1.0 if response.strip() == answer.strip() else 0.0

def partial_credit(prompt, completion, answer, state):
    """Give partial credit for containing key terms."""
    key_terms = answer.lower().split()
    response = completion[-1]['content']
    found = sum(1 for term in key_terms if term in response.lower())
    return found / len(key_terms) if key_terms else 0.0
### Stateful Reward Functions

Access environment state for complex evaluation:

```python
def efficiency_reward(prompt, response, answer, state):
    """Reward based on number of steps taken."""
    max_steps = 10
    steps_taken = state.get("turn", 0)
    return max(0, (max_steps - steps_taken) / max_steps)
```

```python
def load_environment(**kwargs):
    return vf.SingleTurnEnv(
        dataset=dataset,
        system_prompt="Answer the question.", # only used if dataset has 'question' (str) and not 'prompt'
        parser=parser,
        rubric=rubric,
        **kwargs
    )
```
    else:
        # Load from Hugging Face
        dataset = load_dataset("owner/dataset-name", split="train")
    
    # Ensure required columns
    assert "prompt" in dataset.column_names
    assert "answer" in dataset.column_names or "info" in dataset.column_names
    
    return vf.SingleTurnEnv(dataset=dataset, **kwargs)
```

## Key Hyperparameters

### Batch Configuration

Key fields in `[trainer.args]`:
- `rollouts_per_example`: completions per prompt (group size)
- `micro_batch_size`: rollouts per GPU per step
- `batch_size`: rollouts per global batch (must be divisible by `micro_batch_size * world_size`)

**How to think about batch settings:**
- `rollouts_per_example`: Larger groups (16-32) increase reward diversity but increase training time and memory usage
- **Install an environment from this GitHub repo**: `vf-install math-python --from-repo`
- **Evaluate**: `vf-eval math-python` (defaults to gpt-4.1-mini, small sample)

## Common usage patterns and examples

### SingleTurnEnv (prompt → single response)
- **gsm8k**: Classic QA with exact-match reward; toggles `ThinkParser` vs `Parser` and format reward.
- **math**: Hendrycks MATH dataset with `MathRubric` reward (using HuggingFace's `math-verify` scorer).
- **reverse_text**: XML formatting with non-binary LCS reward + format reward.
- **gpqa**: Multiple-choice; demonstrates optional judge-based secondary scoring via `RubricGroup`.
- **simpleqa**: Judge-graded A/B/C classification using `JudgeRubric` rewards.

### Judge-based evaluation (LLM-as-judge)
- **simpleqa**: Judge rubric maps graded letters to reward.
- **continuation_quality**: Judge rubric extracts `<grade>` and maps A–F to a continuous score.
- **toxicity_explanation**: Judge rubric returns 0–10 normalized score for both classification correctness and explanation quality.
- **self_reward**: pattern for `SingleTurnEnv` with only a `JudgeRubric` over a dataset that supplies `question`/`answer`; intended for online RL where model acts as its own judge.

### Parsers and formatting
- **ThinkParser**: Used in `gsm8k`, `wiki_search` to separate reasoning from final answers.
- **XMLParser**: Used in `reverse_text`, `wordle`, `summarize_text`, `reasoning_gym_env`, `xml_tool_env`, `xlam_function_calling` to enforce structured outputs and enable format rewards.
- **Custom parsers**: `smolagents_math_tools` defines a bespoke parser to interoperate with external tool schemas.
    # make sure there's at least ~25 words before and after the split point
    split_space = min(n_spaces - 25, max(25, split_space))
    idx = -1
    for _ in range(split_space):
        idx = text.find(" ", idx + 1)
    return {"prompt": text[:idx], "answer": text[idx:]}


def load_environment(
    dataset_name: str = "agentlans/wikipedia-paragraphs",
    dataset_split: str | None = "train",
    dataset = dataset.filter(lambda x: x[dataset_key].count(" ") > 100)
    dataset = dataset.map(lambda x: make_cut(x[dataset_key]))
    dataset = dataset.shuffle(seed=777)

    judge_client = OpenAI(api_key=os.getenv(judge_api_key_var), base_url=judge_base_url)
    judge_prompt = """Evaluate this base model continuation from a prefix, compared to the true continuation from Wikipedia.

<prefix>
{question}
</prefix>

<true_continuation>
{answer}
</true_continuation>

Think aloud in a <scratchpad> for a few lines, then respond with the letter grade in <grade> ... </grade> tags."""
    rubric = vf.JudgeRubric(
        judge_client=judge_client,
        judge_model=judge_model,
        judge_prompt=judge_prompt,
    )

    grade_parser = vf.XMLParser(fields=["grade"], answer_field="grade")

    async def grade_reward(judge, prompt, completion, answer, state, **kwargs) -> float:
        judge_response = await judge(prompt, completion, answer, state, **kwargs)
        judge_grade = (
            (grade_parser.parse_answer(judge_response) or "F")
            .strip()
            .replace("+", "")
            .replace("-", "")
================================================
# doublecheck

### Overview
- **Environment ID**: `doublecheck`
- **Short description**: Two-turn math QA that asks the model to answer, then prompts “Are you sure?”; scored with a math rubric.
- **Tags**: math, multi-turn, xml, think-answer, verification

### Datasets
- **Primary dataset(s)**: `math` (example dataset loaded via `load_example_dataset`)
- **Source links**: Uses the example loader in `verifiers.utils.data_utils`
import verifiers as vf
from verifiers.types import Messages, State
from verifiers.rubrics.math_rubric import MathRubric
from verifiers.utils.data_utils import load_example_dataset

SIMPLE_PROMPT = """
Respond in the following format, using careful step-by-step reasoning.

<reasoning>
...
</reasoning>
):
    dataset = load_example_dataset(dataset_name, dataset_split, n=num_train_examples)
    rubric = MathRubric()
    vf_env = DoubleCheckEnv(
        dataset=dataset,
        system_prompt=SIMPLE_PROMPT,
        few_shot=[],
        parser=rubric.parser,
        rubric=rubric,
    )
    return vf_env
) -> vf.Environment:
    if use_diamond:
        eval_dataset = load_example_dataset("gpqa_diamond", "train")
    else:
        eval_dataset = load_example_dataset("gpqa_main", "train")
    system_prompt = """Give the letter of the correct answer inside \\boxed{...}."""
    parser = vf.Parser(extract_fn=extract_boxed_answer)

    def correct_answer_reward_func(completion, answer, **kwargs) -> float:
        response = parser.parse_answer(completion) or ""
        return 1.0 if response.startswith(str(answer)) else 0.0

    rubric = vf.Rubric(parser=parser, funcs=[correct_answer_reward_func], weights=[1.0])
    vf_env = vf.SingleTurnEnv(
        eval_dataset=eval_dataset,
        system_prompt=system_prompt,
        parser=parser,
        rubric=rubric,
    )
    # judge_rubric = vf.JudgeRubric()

    # async def judge_reward(judge, prompt, completion, answer, state):
    #     judge_response = await judge(prompt, completion, answer, state)
    #     return 1.0 if "yes" in judge_response.lower() else 0.0

    # judge_rubric.add_reward_func(judge_reward, 1.0)
    # vf_env.rubric = vf.RubricGroup([judge_rubric, vf_env.rubric])
    return vf_env
================================================
FILE: environments/gsm8k/gsm8k.py
================================================
import verifiers as vf
from verifiers.utils.data_utils import (
    BOXED_SYSTEM_PROMPT,
    extract_boxed_answer,
    load_example_dataset,
)


def load_environment(
    system_prompt: str = BOXED_SYSTEM_PROMPT,
    num_train_examples=-1,
    num_eval_examples=-1,
):
    dataset = load_example_dataset("gsm8k", split="train")
    if num_train_examples != -1:
    )

    vf_env = vf.SingleTurnEnv(
        dataset=dataset,
        eval_dataset=eval_dataset,
        system_prompt=system_prompt,
        parser=parser,
        rubric=rubric,
    )
    return vf_env

================================================
FILE: environments/math_group/math_group.py
================================================
import verifiers as vf
from verifiers.utils.data_utils import (
    BOXED_SYSTEM_PROMPT,
    extract_boxed_answer,
    load_example_dataset,
)


        weights=[1.0, 0.0],
    )
    dataset1 = load_example_dataset("gsm8k", split="train").select(range(1000))
    env1 = vf.SingleTurnEnv(
        dataset=dataset1,
        system_prompt=BOXED_SYSTEM_PROMPT,
        parser=parser,
        rubric=rubric1,
    )

    # env 2: math
        weights=[1.0, 0.2],
    )
    dataset2 = load_example_dataset("math", split="train").select(range(1000))
    env2 = vf.SingleTurnEnv(
        dataset=dataset2,
        system_prompt=BOXED_SYSTEM_PROMPT,
        parser=parser,
        rubric=rubric2,
    )

    vf_env = vf.EnvGroup([env1, env2], env_names=["gsm8k", "math"])
    sandbox_timeout_minutes: int = 60,
    sandbox_timeout_per_command_seconds: int = 30,
    **kwargs,
):
    dataset = load_example_dataset(dataset_name, dataset_split, n=num_train_examples)
    system_prompt = (
        "Use python for all calculations. Give your answer inside \\boxed{}."
    )

    parser = vf.Parser(extract_fn=extract_boxed_answer)
    math_rubric = vf.MathRubric(parser=parser)
    vf_env = vf.PythonEnv(
        dataset=dataset,
        system_prompt=system_prompt,
        parser=parser,
        rubric=math_rubric,
        max_turns=max_turns,
        # python env args
        max_startup_wait_seconds=max_startup_wait_seconds,
    mcp_servers: list = EXA_FETCH_TOOLS, dataset=None, **kwargs
) -> vf.Environment:
    """Load an MCPEnv environment with fetch server for testing."""
    dataset = dataset or Dataset.from_dict(
        {
            "question": [
                "Find out what Prime Intellect's newest announcement was from their website, give me the headline in 2 words. Their url is primeintellect.ai",
            ],
            "answer": ["ENVIRONMENTS HUB"],
        }
    )

    rubric = vf.JudgeRubric(judge_model="gpt-4.1-mini")

    async def judge_reward(judge, prompt, completion, answer, state):
        judge_response = await judge(prompt, completion, answer, state)
        return 1.0 if "yes" in judge_response.lower() else 0.0

    rubric.add_reward_func(judge_reward, weight=1.0)
    vf_env = MCPEnv(
        mcp_servers=mcp_servers,
    2: "C",
    3: "D",
}


def format_prompt(example: dict):
    pil_image = example["image_1"]
    buffer = BytesIO()
    pil_image.save(buffer, format="PNG")
    b64_img = base64.b64encode(buffer.getvalue()).decode("utf-8")
    txt = example["question"] + "\n\n"
    options = ast.literal_eval(example["options"])
    assert len(options) == 4
    for i, o in enumerate(options):
        txt += f"{MC_MAP[i]}. {o}\n"
    txt += "\nThink step-by-step and give the letter of your final answer inside \\boxed{}."

    prompt = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": str(txt)},
                {
                    "image_url": {"url": f"data:image/png;base64,{b64_img}"},
                },
            ],
        }
    ]
    return prompt


def load_environment(
    subset: str | None = "Art", split: str = "dev", **kwargs
) -> vf.Environment:

    task_datasets = []
    for task in tasks:
        # load MMMU dataset
        task_dataset = load_dataset("MMMU/MMMU", task, split=split).map(
            lambda x: {"prompt": format_prompt(x), "answer": x["answer"]}
        )
        assert isinstance(task_dataset, Dataset)
        cols = task_dataset.column_names
        cols_to_remove = [col for col in cols if col not in ["prompt", "answer"]]
        task_dataset = task_dataset.remove_columns(cols_to_remove)
        task_datasets.append(task_dataset)

    dataset = concatenate_datasets(task_datasets)

- **Environment ID**: `reverse-text`
- **Short description**: Reverse a given text; evaluated by LCS similarity between the parsed answer and ground-truth reversal.
- **Tags**: text, transformation, single-turn, xml

### Datasets
- **Primary dataset(s)**: `PrimeIntellect/Reverse-Text-RL` mapped to question/answer pairs
- **Source links**: [PrimeIntellect/Reverse-Text-RL](https://huggingface.co/datasets/PrimeIntellect/Reverse-Text-RL)
- **Split sizes**: Uses `train` split

### Task
- **Type**: single-turn
### Environment Arguments
| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `dataset_name` | str | `"PrimeIntellect/Reverse-Text-RL"` | Name of the dataset to use |
| `dataset_split` | str | `"train"` | Split of the dataset to use |
| `system_prompt` | str | None | `"Reverse the text character-by-character. Put your answer in <reversed_text> tags."` | System prompt to use |

### Metrics
| Metric | Meaning |
| ------ | ------- |
| `reward` | LCS similarity between reversed text and parsed answer |


def load_environment(
    dataset_name: str = "PrimeIntellect/Reverse-Text-RL",
    dataset_split: str = "train",
    system_prompt: str
    | None = "Reverse the text character-by-character. Put your answer in <reversed_text> tags.",
) -> vf.Environment:
    train_dataset = load_dataset(dataset_name, split=dataset_split).map(
        lambda x: {
            "question": x["prompt"],
            "answer": x["prompt"][::-1],
            "info": {},
            "task": "reverse-text",
        }
    )
    train_dataset = train_dataset.remove_columns(["prompt"])

    parser = vf.XMLParser(["reversed_text"], answer_field="reversed_text")

    def lcs_reward_func(completion, answer, **kwargs) -> float:
        """
        LCS ratio of the reversed prompt and the parsed completion.
        """

        def lcs_ratio(x: str, y: str) -> float:
            """
            Return the longest common subsequence ratio of x and y.
        weights=[1.0],
    )

    vf_env = vf.SingleTurnEnv(
        dataset=train_dataset,
        system_prompt=system_prompt,
        parser=parser,
        rubric=rubric,
    )
    return vf_env

================================================
# self-reward

### Overview
- **Environment ID**: `self-reward`
- **Short description**: Single-turn evaluation where a judge model scores responses based on a simple scoring prompt.
- **Tags**: judge, single-turn, self-reward, openai-compatible

### Datasets
- **Primary dataset(s)**: Any HF dataset with `question`/`answer` columns (specified by `dataset_name`)
- **Source links**: Hugging Face Datasets
- **Split sizes**: Uses the dataset’s `train` file by default

### Task
- **Type**: single-turn
- **Parser**: default `Parser`
- **Rubric overview**: `JudgeRubric` uses a judge client/model/prompt to produce a 0–1 score

### Quickstart
Run an evaluation with default settings (example):

```bash
- Reports are written under `./environments/self_reward/reports/` and auto-embedded below.

### Environment Arguments
| Arg | Type | Default | Description |
| --- | ---- | ------- | ----------- |
| `dataset_name` | str | — | HF dataset name or path containing `question`/`answer` |
| `model_name` | str | — | Judge model name (OpenAI-compatible) |
| `base_url` | str | `"http://0.0.0.0:8000/v1"` | Judge API base URL |
| `api_key_var` | str | `"JUDGE_API_KEY"` | Env var containing judge API key |

### Metrics
    dataset_name: str,
    model_name: str,
    base_url: str = "http://0.0.0.0:8000/v1",
    api_key_var: str = "JUDGE_API_KEY",
):
    judge_prompt = "Q: {question}\nA: {answer}\nGiven: {response}\nRespond with a score between 0.0 and 1.0."
    rubric = vf.JudgeRubric(
        judge_client=OpenAI(base_url=base_url, api_key=os.getenv(api_key_var, "EMPTY")),
        judge_model=model_name,
        judge_prompt=judge_prompt,
    )
    vf_env = vf.SingleTurnEnv(
        dataset=load_dataset(
            dataset_name, data_files="train"
        ),  # HF dataset with "question" and "answer" columns
        system_prompt="You are a helpful assistant.",
        rubric=rubric,
    )

    return vf_env



index = [("first", 0), ("second", 1), ("third", 2), ("fourth", 3), ("fifth", 4)]


def get_sentence_questions(x):
    sentences = x["sentences"]
    shuffled_index = deepcopy(index)
    random.shuffle(shuffled_index)
    questions_answers = [
        (
            f"What's the {shuffled_index[0][0]} sentence of the paragraph?",
            f'The {shuffled_index[0][0]} sentence of the paragraph is: "{sentences[shuffled_index[0][1]]}"',
        ),
        (
            f"The {shuffled_index[4][0]} sentence of the paragraph is: {sentences[shuffled_index[4][1]]}",
        ),
    ]
    x["task"] = "sentence-repeater"
    x["info"] = {}
    x["info"]["questions"] = [q for q, _ in questions_answers]
    x["info"]["answers"] = [a for _, a in questions_answers]
    paragraph = ". ".join(sentences)
    x["prompt"] = [
        {
            "role": "user",
            "content": f'Answer questions about the following paragraph:\n\n"{paragraph}"\n\n{questions_answers[0][0]}',
        }
    ]
    return x


class SentenceRepeaterEnv(vf.MultiTurnEnv):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    @vf.stop
    async def all_questions_answered(self, state: State) -> bool:
        return len(state["trajectory"]) >= len(state["info"]["questions"])

    async def env_response(
        self, messages: Messages, state: State, **kwargs
    ) -> Messages:
        num_turn = len(state["trajectory"])
        return [
            {
                "role": "user",
                "content": state["info"]["questions"][num_turn],
            }
        ]


def load_environment(**kwargs) -> vf.Environment:
    dataset: Dataset = load_dataset("agentlans/wikipedia-paragraphs", split="train")  # type: ignore
    dataset = dataset.map(lambda x: {"sentences": get_sentences(x["text"])})
    dataset = dataset.filter(lambda x: len(x["sentences"]) == 5)
    dataset = dataset.map(get_sentence_questions)
    parser = vf.Parser()

    def compare_answers_reward_func(parser, completion, info, **kwargs) -> float:
        assert isinstance(completion, list)
        model_answers = [
    """
    Adapted from: https://github.com/openai/simple-evals/blob/main/simpleqa_eval.py
    """
    eval_dataset = load_dataset("basicv8vc/SimpleQA", split="test").map(
        lambda x: {
            "question": x["problem"],
            "answer": x["answer"],
            "task": "simpleqa",
        }
    )

    JUDGE_TEMPLATE = """\
Your job is to look at a question, a gold target, and a predicted answer, and then assign a grade of either ["CORRECT", "INCORRECT", "NOT_ATTEMPTED"].
First, I will give examples of each grade, and then you will grade a new example.


The following are examples of CORRECT predicted answers.
```
Question: What are the names of Barack Obama's children?
Gold target: Malia Obama and Sasha Obama
Predicted answer 1: sasha and malia obama
Predicted answer 2: most people would say Malia and Sasha, but I'm not sure and would have to double check
Predicted answer 3: Barack Obama has two daughters. Their names are Malia Ann and Natasha Marian, but they are commonly referred to as Malia Obama and Sasha Obama. Malia was born on July 4, 1998, and Sasha was born on June 10, 2001.
```
    - Hedging and guessing are permissible, provided that the gold target is fully included and the response contains no incorrect information or contradictions.


The following are examples of INCORRECT predicted answers.
```
Question: What are the names of Barack Obama's children?
Gold target: Malia and Sasha
Predicted answer 1: Malia.
Predicted answer 2: Malia, Sasha, and Susan.
Predicted answer 3: Barack Obama does not have any children.
Predicted answer 4: I think it's either Malia and Sasha. Or it could be Malia and Jackie. Or it could be Joey and Malia.
    - A factual statement in the answer contradicts the gold target. Incorrect statements that have some hedging (e.g., "it is possible that", "although i'm not sure, i think") are also considered incorrect.


The following are examples of NOT_ATTEMPTED predicted answers.
```
Question: What are the names of Barack Obama's children?
Gold target: Malia and Sasha
Predicted answer 1: I don't know.
Predicted answer 2: I need more context about which Obama you are talking about.
Predicted answer 3: Without researching the web, I cannot answer this question. However, I can tell you that Barack Obama has two children.
Predicted answer 4: Barack Obama has two children. I know that one of them is Malia, but I'm not sure about the other one.
```
These predicted answers are all NOT_ATTEMPTED because:
    - The important information in the gold target is not included in the answer.
    - No statements in the answer contradict the gold target.


Also note the following things:
- For grading questions where the gold target is a number, the predicted answer needs to be correct to the last significant figure in the gold answer. For example, consider a question "How many citations does the Transformer Paper have?" with gold target "120k".
    - Predicted answers "120k", "124k", and 115k" are all CORRECT.
    - Predicted answers "100k" and "113k" are INCORRECT.
    - Predicted answers "around 100k" and "more than 50k" are considered NOT_ATTEMPTED because they neither confirm nor contradict the gold target.
- The gold target may contain more information than the question. In such cases, the predicted answer only needs to contain the information that is in the question.
    - For example, consider the question "What episode did Derek and Meredith get legally married in Grey's Anatomy?" with gold target "Season 7, Episode 20: White Wedding". Either "Season 7, Episode 20" or "White Wedding" would be considered a CORRECT answer.
- Do not punish predicted answers if they omit information that would be clearly inferred from the question.
    - For example, consider the question "What city is OpenAI headquartered in?" and the gold target "San Francisco, California". The predicted answer "San Francisco" would be considered CORRECT, even though it does not include "California".
    - Consider the question "What award did A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity win at NAACL '24?", the gold target is "Outstanding Paper Award". The predicted answer "Outstanding Paper" would be considered CORRECT, because "award" is presumed in the question.
    - For the question "What is the height of Jason Wei in meters?", the gold target is "1.73 m". The predicted answer "1.75" would be considered CORRECT, because meters is specified in the question.
    - For the question "What is the name of Barack Obama's wife?", the gold target is "Michelle Obama". The predicted answer "Michelle" would be considered CORRECT, because the last name can be presumed.
- Do not punish for typos in people's name if it's clearly the same name.
    - For example, if the gold target is "Hyung Won Chung", you can consider the following predicted answers as correct: "Hyoong Won Choong", "Hyungwon Chung", or "Hyun Won Chung".


Here is a new example. Simply reply with either CORRECT, INCORRECT, NOT ATTEMPTED. Don't apologize or correct yourself if there was a mistake; we are just trying to grade the answer.
```
Question: {question}
Gold target: {answer}
Predicted answer: {response}
```

Grade the predicted answer of this new question as one of:
A: CORRECT
B: INCORRECT
C: NOT_ATTEMPTED

Just return the letters "A", "B", or "C", with no text around it.
    judge_client = OpenAI(base_url=judge_base_url, api_key=api_key)

    rubric = vf.JudgeRubric(
        judge_client=judge_client,
        judge_model=judge_model,
        judge_prompt=JUDGE_TEMPLATE,
    )

    async def correct_answer_reward_func(
        judge, prompt, completion, answer, state, **kwargs
    ) -> float:
        judge_response = await judge(prompt, completion, answer, state, **kwargs)
        match = re.search(r"(A|B|C)", judge_response)
        result = match.group(0) if match else "C"
        return 1.0 if result == "A" else 0.0

    async def incorrect_answer_reward_func(
        judge, prompt, completion, answer, state, **kwargs
    ) -> float:
        judge_response = await judge(prompt, completion, answer, state, **kwargs)
        match = re.search(r"(A|B|C)", judge_response)
        result = match.group(0) if match else "C"
        return 1.0 if result == "B" else 0.0

    async def not_attempted_answer_reward_func(
        judge, prompt, completion, answer, state, **kwargs
    ) -> float:
        judge_response = await judge(prompt, completion, answer, state, **kwargs)
        match = re.search(r"(A|B|C)", judge_response)
        result = match.group(0) if match else "C"
        return 1.0 if result == "C" else 0.0

    rubric.add_reward_func(correct_answer_reward_func, weight=1.0)
- **Environment ID**: `tool-test`
- **Short description**: Sanity-check tool-calling environment that asks models to invoke a random subset of dummy tools.
- **Tags**: tools, single-turn, function-calling, sanity

### Datasets
- **Primary dataset(s)**: Synthetic HF dataset generated in-memory with prompts specifying required tools
- **Source links**: N/A (programmatically generated)
- **Split sizes**: Controlled by `num_train_examples` and `num_eval_examples`

### Task
- **Type**: tool use (single-turn ToolEnv)
    eval_rows = []
    for i in range(num_train_examples + num_eval_examples):
        tool_names = random.sample(
            tool_name_list, random.randint(1, len(tool_name_list))
        )
        prompt = [
            {
                "role": "user",
                "content": f"Call the following tools with arguments of your choice: {tool_names}",
            }
        ]
        info = {"tool_names": tool_names}
        if i < num_train_examples:
            train_rows.append({"prompt": prompt, "info": info})
        else:
            eval_rows.append({"prompt": prompt, "info": info})

    dataset = Dataset.from_list(train_rows)
    eval_dataset = Dataset.from_list(eval_rows)
    parser = vf.Parser()
    rubric = vf.ToolRubric(tools=tool_list)
            toxicity_details.append("insulting")
        if example.get("identity_attack", 0) >= threshold:
            toxicity_details.append("identity attack")

        return {
            "question": f"Analyze the following text for toxicity and explain your reasoning:\n\n{example['text']}",
            "answer": "toxic" if is_toxic else "non-toxic",  # For JudgeRubric
            "info": {
                "is_toxic": is_toxic,
                "categories": toxicity_details if toxicity_details else ["non-toxic"],
                "text": example["text"],
            },
        }

    dataset = dataset.map(transform_example, remove_columns=dataset.column_names)  # type: ignore

    system_prompt = """You are a content moderation assistant. For each text, you must:

1. Classify whether the text is toxic or non-toxic
2. Explain your reasoning with specific examples from the text
3. Consider context and intent, not just individual words

    parser = vf.Parser()
    judge_client = AsyncOpenAI(
        api_key=os.getenv(judge_api_key_var), base_url=judge_base_url
    )

    judge_prompt = """Evaluate this toxicity classification task.

Text to analyze: {question}

Expected classification: {answer}

Model's response: {response}


    # Create the JudgeRubric
    rubric = vf.JudgeRubric(
        judge_client=judge_client,
        judge_model=judge_model,
        judge_prompt=judge_prompt,
        parser=parser,
    )

    async def comprehensive_evaluation(
        judge, prompt, completion, answer, state, **kwargs
    ) -> float:
        judge_response = await judge(prompt, completion, answer, state, **kwargs)

        numbers = re.findall(r"\b([0-9]|10)\b", judge_response)
        if numbers:
            score = float(numbers[0]) / 10.0  # Normalize to 0-1
            return max(0.0, min(1.0, score))  # Clamp to [0, 1]

    rubric.add_reward_func(comprehensive_evaluation, weight=1.0)

    env = vf.SingleTurnEnv(
        dataset=dataset,
        system_prompt=system_prompt,
        parser=parser,
        rubric=rubric,
        **kwargs,
    )

- **Environment ID**: `wiki-search`
- **Short description**: Multi-turn tool-use QA over a small Wikipedia corpus using ChromaDB and OpenAI embeddings, with judge-based scoring.
- **Tags**: retrieval, tools, multi-turn, embeddings, judge

### Datasets
- **Primary dataset(s)**: `willcb/wiki-trivia-questions` (HF) and a Wikipedia corpus indexed in ChromaDB (from `willcb/rare-wiki-pages`, indexed at `.chroma_db` on first run)
- **Source links**: Hugging Face Datasets, ChromaDB
- **Split sizes**: Uses the `train` split for prompts

### Task
- **Type**: multi-turn tool use
- **Rubric overview**: Combines the default tool rubric with a `JudgeRubric` for answer quality

        search_pages,
        view_sections,
        read_section,
    ]
    parser = vf.Parser()
    dataset = load_dataset("willcb/wiki-trivia-questions-v4", split="train")
    tool_rubric = vf.ToolRubric(tools=tools)

    JUDGE_PROMPT = """Given a ground truth answer \
    and a response, determine if the response is both correct and coherent.

    Question:
    ```
    {question}
    ```

    Ground truth answer:
    ```
    {answer}
    )
    judge_rubric = JudgeRubric(
        judge_client=judge_client,
        judge_model=judge_model,
        parser=parser,
        judge_prompt=JUDGE_PROMPT,
    )

    async def judge_reward_func(judge, prompt, completion, answer, state) -> float:
        judge_response = await judge(prompt, completion, answer, state)
        if "yes" in judge_response.lower():
            return 1.0
        else:
            return 0.0

    system_prompt = "Use the provided Wikipedia search tools to help answer questions."
    judge_rubric.add_reward_func(judge_reward_func, weight=1.0)
    rubric = vf.RubricGroup(rubrics=[tool_rubric, judge_rubric])
    vf_env = vf.ToolEnv(
        dataset=dataset,
        system_prompt=system_prompt,
        parser=parser,
        rubric=rubric,
        tools=tools,
        max_turns=max_turns,
    )
FILE: environments/wordle/wordle.py
================================================
import verifiers as vf
from verifiers.envs.textarena_env import TextArenaEnv

### prompt

GUESS_SYSTEM_PROMPT = """You are a competitive game player. \
Make sure you read the game instructions carefully, and always follow the required format.

In each turn, think step-by-step, then give your guess inside <guess>...</guess> tags."""


### environment loader
def load_environment(
    num_train_examples: int = 2000,
    num_eval_examples: int = 20,
):
    system_prompt = GUESS_SYSTEM_PROMPT
    parser = vf.XMLParser(fields=["guess"], answer_field="guess")

    rubric = vf.Rubric(parser=parser)
    rubric.add_reward_func(check_answer_reward_func)
    rubric.add_reward_func(partial_credit_reward_func)

    vf_env = TextArenaEnv(
        game="Wordle-v0",
        num_train_examples=num_train_examples,
        num_eval_examples=num_eval_examples,
        system_prompt=system_prompt,
        parser=parser,
        rubric=rubric,
        feedback_fn=wordle_feedback_fn,
    )
    return vf_env
- Reorganization of examples and training scripts, removing lots of duplicated logic and creating a cleaner separation between library code and example code.
- Deprecation of the manual dynamically-batched `LLM` inference worker in favor of proper `AsyncLLM` support, allowing full control of native vLLM sampling parameters. 
- Support for native tool call parsing + parallel tool calls in `ToolEnv` (replacing the manual `XMLParser` approach).
- Another trainer! Environments built with `verifiers` are now trainable with `prime-rl` (as of [58ac91f](https://github.com/PrimeIntellect-ai/prime-rl/commit/58ac91fd3e19968e33c12f255de446d959982062) for `v0.1.2`), which supports multi-node FSDP async training, is the primary RL framework used by the Prime Intellect research team, and is under ongoing development and stress-testing in advance of large-scale multi-environment training runs. 
- Pydantic types for core data classes used by Environments.
- Improvements to `GRPOTrainer`, including supporting a single `max_seq_len` option (instead of separate prompt + completion lengths), and configurable turn length limits via `max_tokens`.
- Many more Environment examples.
- Improved logging and evaluation options.
- Overhauled README.md and [docs](https://verifiers.readthedocs.io/en/latest/).


Minor post-release update focusing on polish: CLI script bug fixes and enhancements, environment example cleanup, better reporting, and improved test coverage.

## Highlights
- vf-eval: fixed rollout indexing bugs and improved reliability when sampling multiple rollouts.
- vf-init: streamlined project initialization and naming (removed automatic `vf-` prefix) and refreshed templates.
- Environments: documentation and prompt cleanups; added/updated AIME examples; improved report embedding.
- Tests: expanded coverage across rubric behavior, XML parser, and environment edge cases.

## Changes by Area
### CLI and Scripts
- vf-eval
  - Remove automatic `vf-` prefix during init to honor provided names (PR #190).
  - Update README template/content for new environments (multiple small tweaks).

### Environments and Examples
- AIME 2024 / AIME 2025 updates (PR #199).
- Math Python example: prompt/readme/report cleanups.
- General environment cleanup and README refreshes across multiple examples.
- HotpotQA example: troubleshooting notes and minor fixes.

### Parsers, Rubrics, and Utils
- XMLParser: fix handling of string completions during `parse_answer` (PR #196).
  - Update sampling args for `gpt-5` (hotfix commit).

### Environments and Examples
- Add a stateful tool environment; load tool information via environment responses (PR #224).
- Rename and consolidate environments, introduce tag metadata for discoverability (PR #222; additional env tag updates).
- Math environment updates and prompt tweaks.
- Remove dead processing code in `environment.py`; general cleanup and type hint improvements.

### Parsers, Rubrics, and Utils
- Caching improvements for JudgeRubric to reduce redundant work (PR #216).
- More robust rule-based math verification and heuristics (PR #213).
- Report tweaks and endpoints path loading (PR #206 and follow-ups)
- Integrate and document prime-rl training (PR #204)
- Update report generation and vf-init template (PR #203)
- Add support for base model RL / `message_type="completions"` (PR #201)
- Add `rich` as default dependency for eval script (PR #200)
- Math env updates, prompt tweaks, type hints, and cleanup in `environment.py`

## Full Changelog
- `v0.1.2.post0...HEAD`: https://github.com/willccbb/verifiers/compare/v0.1.2.post0...HEAD



Verifiers v0.1.8 introduces a major refactor of the rollout system to use trajectory-based tracking, where each LLM request/response pair is recorded as an independent step. This enables cleaner horizontal training workflows (e.g. truncated thinking, branching rollouts, sub-agents, self-summarization) and eliminates retokenization brittleness by preserving vLLM's native token IDs and logprobs.

## Highlights

- **Trajectory-based rollouts**: All rollouts now track trajectory steps automatically. Each step represents one complete LLM API call with its prompt, completion, tokens, and logprobs.
- **Input-first API**: `rollout()` now takes `input: RolloutInput` as the primary parameter, matching dataset structure more closely.
- **Decorator-based termination**: New `@stop`, `@cleanup`, and `@teardown` decorators for declarative rollout lifecycle management.
- **State structure improvements**: Clear separation between `init_state()` (environment-agnostic) and `setup_state()` (environment-specific configuration).
- **Horizontal training support**: Each trajectory step can be processed as an independent training example without retokenization.
- **Training integration**: Both `RLTrainer` and `prime-rl` now use trajectory-based rollouts for training. `prime-rl` support is available via the `will/trajectories` branch, which is automatically pinned when using `vf-setup`.
- **Group reward functions**: Reward functions can now operate on groups of rollouts simultaneously by accepting plural parameters (`states`, `prompts`, `completions`, etc.), enabling relative scoring (e.g. pairwise, tournament). 
- **Simplified scoring logic**: Scoring is now performed at the group level via `score_group()` by default, parallelizing across rollouts for any rollout-based (non-group) reward functions.
- **Completion rendering**: Internal `_render_completion()` method automatically renders completion from trajectory for output saving, ensuring consistent formatting.

## Breaking Changes

```python
async def rollout(
    self,
    client: AsyncOpenAI,
    model: str,
    prompt: Messages,
    completion: Messages | None = None,
    answer: str = "",
    state: State = {},
    task: str = "default",
    info: Info | None = None,

**Old `init_state()` signature (deprecated):**
```python
async def init_state(
    self,
    prompt: Messages,
    completion: Messages,
    answer: str,
    task: str,
    info: Info,
    example_id: int,
    # Creates state from input, sets up trajectory list, timing, etc.
    # ...
```

**Key changes:**
- `rollout()` now takes `input: RolloutInput` as the first parameter instead of many individual parameters (`prompt`, `completion`, `answer`, `task`, `info`, `example_id`, etc.)
- `rollout()` now returns `State` instead of `tuple[Messages, State]`
- `init_state()` now takes `input: RolloutInput, client: AsyncOpenAI, model: str, sampling_args: SamplingArgs | None = None` instead of individual parameters
- `setup_state()` is now abstract in `Environment` and **must be implemented** by environments that inherit directly from `Environment`. `MultiTurnEnv` implements it as a no-op (returns state unchanged), so environments inheriting from `MultiTurnEnv` can override it as needed but don't need to implement it
- State is created internally via `init_state()` and `setup_state()` within `rollout()`

- Environments that inherit directly from `Environment` (e.g., custom environments) **must** update their `rollout()` and `init_state()` signatures and implement `setup_state()`

### Scoring Changes

- **Group-level scoring**: Scoring is now always performed at the group level via `rubric.score_group()`. The `interleave_scoring` flag is deprecated and no longer has any effect.
- **Group reward functions**: Reward functions can now accept plural parameters (`states`, `prompts`, `completions`, `answers`, `tasks`, `infos`) to score multiple rollouts together. Rubrics automatically detect group functions by signature inspection.

## Migration Guide

### For Environment Developers


### For Users

- Most existing environments continue to work without changes due to backward compatibility via state forwarding (e.g. any SingleTurnEnv, ToolEnv, TextArenaEnv, etc.) will work without changes. Environments that override `is_completed()` should be updated to use `@stop` decorators instead.
- Trajectory data is now available in all rollouts via `state["trajectory"]`
- Each trajectory step contains `prompt`, `completion`, `response`, `tokens`, `reward`, and `advantage` fields

## Technical Details

- Trajectory steps preserve vLLM's native token IDs and logprobs, eliminating retokenization
- Token extraction requires vLLM configuration with `return_tokens_as_token_ids=True` and `return_token_ids=True` in `sampling_args` (trainers must pass these flags to enable training)
- Only completion logprobs are stored (prompt logprobs are not included)
- `_render_completion()` is called automatically when rollouts complete to render `state["completion"]` from the trajectory for backward compatibility and output saving
- Group reward functions are detected automatically by checking for plural parameter names or list return types

**Full Changelog**: https://github.com/PrimeIntellect-ai/verifiers/compare/v0.1.7...v0.1.8


Verifiers v0.1.8 introduces a major refactor of the rollout system to use trajectory-based tracking, where each LLM request/response pair is recorded as an independent step. This enables cleaner horizontal training workflows (e.g. truncated thinking, branching rollouts, sub-agents, self-summarization) and eliminates retokenization brittleness by preserving vLLM's native token IDs and logprobs.

## Highlights

- **Trajectory-based rollouts**: All rollouts now track trajectory steps automatically. Each step represents one complete LLM API call with its prompt, completion, tokens, and logprobs.
- **Input-first API**: `rollout()` now takes `input: RolloutInput` as the primary parameter, matching dataset structure more closely.
- **Decorator-based termination**: New `@stop`, `@cleanup`, and `@teardown` decorators for declarative rollout lifecycle management.
- **State structure improvements**: Clear separation between `init_state()` (environment-agnostic) and `setup_state()` (environment-specific configuration).
- **Horizontal training support**: Each trajectory step can be processed as an independent training example without retokenization.
- **Training integration**: Both `RLTrainer` and `prime-rl` now use trajectory-based rollouts for training. `prime-rl` support is available via the `will/trajectories` branch, which is automatically pinned when using `vf-setup`.
- **Group reward functions**: Reward functions can now operate on groups of rollouts simultaneously by accepting plural parameters (`states`, `prompts`, `completions`, etc.), enabling relative scoring (e.g. pairwise, tournament). 
- **Simplified scoring logic**: Scoring is now performed at the group level via `score_group()` by default, parallelizing across rollouts for any rollout-based (non-group) reward functions.
- **Completion rendering**: Internal `_render_completion()` method automatically renders completion from trajectory for output saving, ensuring consistent formatting.

## Breaking Changes

```python
async def rollout(
    self,
    client: AsyncOpenAI,
    model: str,
    prompt: Messages,
    completion: Messages | None = None,
    answer: str = "",
    state: State = {},
    task: str = "default",
    info: Info | None = None,

**Old `init_state()` signature (deprecated):**
```python
async def init_state(
    self,
    prompt: Messages,
    completion: Messages,
    answer: str,
    task: str,
    info: Info,
    example_id: int,
    # Creates state from input, sets up trajectory list, timing, etc.
    # ...
```

**Key changes:**
- `rollout()` now takes `input: RolloutInput` as the first parameter instead of many individual parameters (`prompt`, `completion`, `answer`, `task`, `info`, `example_id`, etc.)
- `rollout()` now returns `State` instead of `tuple[Messages, State]`
- `init_state()` now takes `input: RolloutInput, client: AsyncOpenAI, model: str, sampling_args: SamplingArgs | None = None` instead of individual parameters
- `setup_state()` is now abstract in `Environment` and **must be implemented** by environments that inherit directly from `Environment`. `MultiTurnEnv` implements it as a no-op (returns state unchanged), so environments inheriting from `MultiTurnEnv` can override it as needed but don't need to implement it
- State is created internally via `init_state()` and `setup_state()` within `rollout()`

- Environments that inherit directly from `Environment` (e.g., custom environments) **must** update their `rollout()` and `init_state()` signatures and implement `setup_state()`

### Scoring Changes

- **Group-level scoring**: Scoring is now always performed at the group level via `rubric.score_group()`. The `interleave_scoring` flag is deprecated and no longer has any effect.
- **Group reward functions**: Reward functions can now accept plural parameters (`states`, `prompts`, `completions`, `answers`, `tasks`, `infos`) to score multiple rollouts together. Rubrics automatically detect group functions by signature inspection.

## Migration Guide

### For Environment Developers


### For Users

- Most existing environments continue to work without changes due to backward compatibility via state forwarding (e.g. any SingleTurnEnv, ToolEnv, TextArenaEnv, etc.) will work without changes. Environments that override `is_completed()` should be updated to use `@stop` decorators instead.
- Trajectory data is now available in all rollouts via `state["trajectory"]`
- Each trajectory step contains `prompt`, `completion`, `response`, `tokens`, `reward`, and `advantage` fields

## Technical Details

- Trajectory steps preserve vLLM's native token IDs and logprobs, eliminating retokenization
- Token extraction requires vLLM configuration with `return_tokens_as_token_ids=True` and `return_token_ids=True` in `sampling_args` (trainers must pass these flags to enable training)
- Only completion logprobs are stored (prompt logprobs are not included)
- `_render_completion()` is called automatically when rollouts complete to render `state["completion"]` from the trajectory for backward compatibility and output saving
- Group reward functions are detected automatically by checking for plural parameter names or list return types

**Full Changelog**: https://github.com/PrimeIntellect-ai/verifiers/compare/v0.1.7...v0.1.8


Verifiers v0.1.8 introduces a major refactor of the rollout system to use trajectory-based tracking, where each LLM request/response pair is recorded as an independent step. This enables cleaner horizontal training workflows (e.g. truncated thinking, branching rollouts, sub-agents, self-summarization) and eliminates retokenization brittleness by preserving vLLM's native token IDs and logprobs.

## Highlights

- **Trajectory-based rollouts**: All rollouts now track trajectory steps automatically. Each step represents one complete LLM API call with its prompt, completion, tokens, and logprobs.
- **Input-first API**: `rollout()` now takes `input: RolloutInput` as the primary parameter, matching dataset structure more closely.
- **Decorator-based termination**: New `@stop`, `@cleanup`, and `@teardown` decorators for declarative rollout lifecycle management.
- **State structure improvements**: Clear separation between `init_state()` (environment-agnostic) and `setup_state()` (environment-specific configuration).
- **Horizontal training support**: Each trajectory step can be processed as an independent training example without retokenization.
- **Training integration**: Both `RLTrainer` and `prime-rl` now use trajectory-based rollouts for training. `prime-rl` support is available via the `will/trajectories` branch, which is automatically pinned when using `vf-setup`.
- **Group reward functions**: Reward functions can now operate on groups of rollouts simultaneously by accepting plural parameters (`states`, `prompts`, `completions`, etc.), enabling relative scoring (e.g. pairwise, tournament). 
- **Simplified scoring logic**: Scoring is now performed at the group level via `score_group()` by default, parallelizing across rollouts for any rollout-based (non-group) reward functions.
- **Completion rendering**: Internal `_render_completion()` method automatically renders completion from trajectory for output saving, ensuring consistent formatting.

## Breaking Changes

```python
async def rollout(
    self,
    client: AsyncOpenAI,
    model: str,
    prompt: Messages,
    completion: Messages | None = None,
    answer: str = "",
    state: State = {},
    task: str = "default",
    info: Info | None = None,

**Old `init_state()` signature (deprecated):**
```python
async def init_state(
    self,
    prompt: Messages,
    completion: Messages,
    answer: str,
    task: str,
    info: Info,
    example_id: int,
    # Creates state from input, sets up trajectory list, timing, etc.
    # ...
```

**Key changes:**
- `rollout()` now takes `input: RolloutInput` as the first parameter instead of many individual parameters (`prompt`, `completion`, `answer`, `task`, `info`, `example_id`, etc.)
- `rollout()` now returns `State` instead of `tuple[Messages, State]`
- `init_state()` now takes `input: RolloutInput, client: AsyncOpenAI, model: str, sampling_args: SamplingArgs | None = None` instead of individual parameters
- `setup_state()` is now abstract in `Environment` and **must be implemented** by environments that inherit directly from `Environment`. `MultiTurnEnv` implements it as a no-op (returns state unchanged), so environments inheriting from `MultiTurnEnv` can override it as needed but don't need to implement it
- State is created internally via `init_state()` and `setup_state()` within `rollout()`

- Environments that inherit directly from `Environment` (e.g., custom environments) **must** update their `rollout()` and `init_state()` signatures and implement `setup_state()`

### Scoring Changes

- **Group-level scoring**: Scoring is now always performed at the group level via `rubric.score_group()`. The `interleave_scoring` flag is deprecated and no longer has any effect.
- **Group reward functions**: Reward functions can now accept plural parameters (`states`, `prompts`, `completions`, `answers`, `tasks`, `infos`) to score multiple rollouts together. Rubrics automatically detect group functions by signature inspection.

## Migration Guide

### For Environment Developers


### For Users

- Most existing environments continue to work without changes due to backward compatibility via state forwarding (e.g. any SingleTurnEnv, ToolEnv, TextArenaEnv, etc.) will work without changes. Environments that override `is_completed()` should be updated to use `@stop` decorators instead.
- Trajectory data is now available in all rollouts via `state["trajectory"]`
- Each trajectory step contains `prompt`, `completion`, `response`, `tokens`, `reward`, and `advantage` fields

## Technical Details

- Trajectory steps preserve vLLM's native token IDs and logprobs, eliminating retokenization
- Token extraction requires vLLM configuration with `return_tokens_as_token_ids=True` and `return_token_ids=True` in `sampling_args` (trainers must pass these flags to enable training)
- Only completion logprobs are stored (prompt logprobs are not included)
- `_render_completion()` is called automatically when rollouts complete to render `state["completion"]` from the trajectory for backward compatibility and output saving
- Group reward functions are detected automatically by checking for plural parameter names or list return types

**Full Changelog**: https://github.com/PrimeIntellect-ai/verifiers/compare/v0.1.7...v0.1.8

FILE: notes/RELEASE_v0.1.8.post2.md
================================================
# Verifiers v0.1.8 Release Notes

**Post-release update:**
- Fix `None` in multimodal prompt schemas, sandbox teardown fixes, sandbox retry logic, tenacity-based retry, dict-type logprobs parsing, `VF_LOG_LEVEL` env var, bump prime-sandboxes to 0.2.5 (post2).
- Fix for group reward function handling (post1).
- Fix for eval log ordering (post0).

*Date:* 11/19/2025

Verifiers v0.1.8 introduces a major refactor of the rollout system to use trajectory-based tracking, where each LLM request/response pair is recorded as an independent step. This enables cleaner horizontal training workflows (e.g. truncated thinking, branching rollouts, sub-agents, self-summarization) and eliminates retokenization brittleness by preserving vLLM's native token IDs and logprobs.

## Highlights

- **Trajectory-based rollouts**: All rollouts now track trajectory steps automatically. Each step represents one complete LLM API call with its prompt, completion, tokens, and logprobs.
- **Input-first API**: `rollout()` now takes `input: RolloutInput` as the primary parameter, matching dataset structure more closely.
- **Decorator-based termination**: New `@stop`, `@cleanup`, and `@teardown` decorators for declarative rollout lifecycle management.
- **State structure improvements**: Clear separation between `init_state()` (environment-agnostic) and `setup_state()` (environment-specific configuration).
- **Horizontal training support**: Each trajectory step can be processed as an independent training example without retokenization.
- **Training integration**: Both `RLTrainer` and `prime-rl` now use trajectory-based rollouts for training. `prime-rl` support is available via the `will/trajectories` branch, which is automatically pinned when using `vf-setup`.
- **Group reward functions**: Reward functions can now operate on groups of rollouts simultaneously by accepting plural parameters (`states`, `prompts`, `completions`, etc.), enabling relative scoring (e.g. pairwise, tournament). 
- **Simplified scoring logic**: Scoring is now performed at the group level via `score_group()` by default, parallelizing across rollouts for any rollout-based (non-group) reward functions.
- **Completion rendering**: Internal `_render_completion()` method automatically renders completion from trajectory for output saving, ensuring consistent formatting.

## Breaking Changes

```python
async def rollout(
    self,
    client: AsyncOpenAI,
    model: str,
    prompt: Messages,
    completion: Messages | None = None,
    answer: str = "",
    state: State = {},
    task: str = "default",
    info: Info | None = None,

**Old `init_state()` signature (deprecated):**
```python
async def init_state(
    self,
    prompt: Messages,
    completion: Messages,
    answer: str,
    task: str,
    info: Info,
    example_id: int,
    # Creates state from input, sets up trajectory list, timing, etc.
    # ...
```

**Key changes:**
- `rollout()` now takes `input: RolloutInput` as the first parameter instead of many individual parameters (`prompt`, `completion`, `answer`, `task`, `info`, `example_id`, etc.)
- `rollout()` now returns `State` instead of `tuple[Messages, State]`
- `init_state()` now takes `input: RolloutInput, client: AsyncOpenAI, model: str, sampling_args: SamplingArgs | None = None` instead of individual parameters
- `setup_state()` is now abstract in `Environment` and **must be implemented** by environments that inherit directly from `Environment`. `MultiTurnEnv` implements it as a no-op (returns state unchanged), so environments inheriting from `MultiTurnEnv` can override it as needed but don't need to implement it
- State is created internally via `init_state()` and `setup_state()` within `rollout()`

- Environments that inherit directly from `Environment` (e.g., custom environments) **must** update their `rollout()` and `init_state()` signatures and implement `setup_state()`

### Scoring Changes

- **Group-level scoring**: Scoring is now always performed at the group level via `rubric.score_group()`. The `interleave_scoring` flag is deprecated and no longer has any effect.
- **Group reward functions**: Reward functions can now accept plural parameters (`states`, `prompts`, `completions`, `answers`, `tasks`, `infos`) to score multiple rollouts together. Rubrics automatically detect group functions by signature inspection.

## Migration Guide

### For Environment Developers


### For Users

- Most existing environments continue to work without changes due to backward compatibility via state forwarding (e.g. any SingleTurnEnv, ToolEnv, TextArenaEnv, etc.) will work without changes. Environments that override `is_completed()` should be updated to use `@stop` decorators instead.
- Trajectory data is now available in all rollouts via `state["trajectory"]`
- Each trajectory step contains `prompt`, `completion`, `response`, `tokens`, `reward`, and `advantage` fields

## Technical Details

- Trajectory steps preserve vLLM's native token IDs and logprobs, eliminating retokenization
- Token extraction requires vLLM configuration with `return_tokens_as_token_ids=True` and `return_token_ids=True` in `sampling_args` (trainers must pass these flags to enable training)
- Only completion logprobs are stored (prompt logprobs are not included)
- `_render_completion()` is called automatically when rollouts complete to render `state["completion"]` from the trajectory for backward compatibility and output saving
- Group reward functions are detected automatically by checking for plural parameter names or list return types

**Full Changelog**: https://github.com/PrimeIntellect-ai/verifiers/compare/v0.1.7...v0.1.8


**Solution**: Trajectories are always enabled. Each rollout tracks every LLM request/response as an independent step in `state["trajectory"]`.

```python
TrajectoryStepTokens = TypedDict("TrajectoryStepTokens", {
    "prompt_ids": list[int],
    "prompt_mask": list[int],
    "completion_ids": list[int],
    "completion_mask": list[int],
    "completion_logprobs": list[float],
})

TrajectoryStep = TypedDict("TrajectoryStep", {
    "prompt": Messages,           # Exact prompt sent to LLM for THIS request
    "completion": Messages,       # Exact completion returned from THIS request
    "response": ModelResponse,    # Raw response object with token_ids/logprobs from vLLM
    "tokens": TrajectoryStepTokens | None,  # Extracted token IDs, masks, and logprobs (None if unavailable)
    "reward": float | None,       # Reward for this step (None during rollout, filled by Rubric)
    "advantage": float | None,    # Advantage for this step (for RL training)
    "extras": dict[str, Any],     # Additional step metadata
})

BaseRolloutInput = TypedDict("BaseRolloutInput", {
    "prompt": Messages,
    "example_id": int,
    "task": str,
})

RolloutInput(BaseRolloutInput, total=False):  # answer and info are optional
```python
class State(dict):
    """
    Dict subclass with forwarding for INPUT_FIELDS.
    
    Accessing state["prompt"] forwards to state["input"].prompt if "input" exists,
    otherwise accesses directly from dict (backward compat).
    """
    INPUT_FIELDS = ["prompt", "answer", "task", "info", "example_id"]
    
    # Required: input fields (always in "input" RolloutInput)
    input: RolloutInput
    
    # Created during rollout
    metrics: dict[str, float] | None   # Additional metrics
    timing: RolloutTiming | None       # Timing info
    
    # Custom fields (env-specific, added by subclasses)
    # Examples:
    # "prompt_too_long": bool,         # For max length handling
    # "sandbox_id": str,                # For SandboxEnv
```

**Key Points**:
- State forwards access to `INPUT_FIELDS` (prompt, answer, task, info, example_id) from `state["input"]` for backward compatibility
- `trajectory` is always a list (never None) - trajectories are always enabled
- `completion` is rendered from trajectory when rollout completes
- Custom fields can be added by environment subclasses

**Key Benefits**:
- Each trajectory step = one vLLM request with native token_ids/logprobs (no retokenization)
- Prompts can be completely independent (no prefix-sharing requirement)
- Enables horizontal training: each step becomes an independent training example
- Supports truncated thinking naturally (just another step with new prompt)
- Clean foundation for step-based APIs

### 2. Input-First API ✅

**Solution**: `rollout()` takes `input: RolloutInput` as the primary parameter. State is created internally via `init_state()` and `setup_state()`.
**Solution**: Internal `_render_completion()` method renders completion from trajectory for display/scoring.

```python
async def _render_completion(self, state: State):
    """Render completion from last trajectory step."""
    last_prompt = state["trajectory"][-1]["prompt"]
    last_completion = state["trajectory"][-1]["completion"]
    full_conversation = concat_messages([last_prompt, last_completion])
    state["completion"] = full_conversation[len(state["prompt"]) :]
```

**Impact**:
- Called automatically when rollout completes (via `is_completed()`)
- Sets `state["completion"]` from trajectory for backward compatibility
    async def max_turns_reached(self, state: State) -> bool:
        """Check if the maximum number of turns has been reached."""
        return len(state["trajectory"]) >= self.max_turns and self.max_turns > 0
    
    @stop
    async def prompt_too_long(self, state: State) -> bool:
        return state.get("prompt_too_long", False)

async def is_completed(self, state: State, **kwargs) -> bool:
    """Check all stop conditions. Sets state.is_completed=True if any condition is met."""
    for condition in self._stop_conditions:  # Auto-discovered via @stop decorator
        if await self._render_stop(state, condition):
async def is_completed(self, messages: Messages, state: State, **kwargs) -> bool:
    return len(state["responses"]) == 1

# NEW (fixed):
async def is_completed(self, state: State, **kwargs) -> bool:
    # Always check base stop conditions first (max_turns, prompt_too_long, etc.)
    if await super().is_completed(state, **kwargs):
        return True
    # Then check custom conditions
    return len(state["trajectory"]) == 1  # Use trajectory instead of responses
```
**Solution**: Trajectory-based rollouts enable clean horizontal processing where each trajectory step becomes an independent training example:

```python
def process_trajectories_for_training(
    states: list[State],
    mask_prompts: bool = True,
) -> list[TrainingExample]:
    """
    Convert trajectory-based states to training examples.
    
    Each trajectory step becomes ONE training example with:
    - token_ids: extracted directly from step["tokens"] (already tokenized by vLLM)
    - logprobs: extracted directly from step["tokens"]
    - mask: simple prompt vs completion masking
    - reward: from step["reward"]
    
    No retokenization, no prefix reconstruction, no brittle manipulation.
    """
    examples = []
        if state.get("trajectory"):
            for step in state["trajectory"]:
                if step["tokens"] is None:
                    continue  # Skip steps without token data
                # Extract token_ids and logprobs directly from trajectory step tokens
                prompt_ids = step["tokens"]["prompt_ids"]
                completion_ids = step["tokens"]["completion_ids"]
                completion_logprobs = step["tokens"]["completion_logprobs"]
                
                # Combine prompt and completion
                token_ids = prompt_ids + completion_ids
                logprobs = [0.0] * len(prompt_ids) + completion_logprobs  # No prompt logprobs
                
                # Use masks from trajectory step
                if mask_prompts:
                    mask = step["tokens"]["prompt_mask"] + step["tokens"]["completion_mask"]
                else:
                    mask = [1] * len(token_ids)
                
                examples.append(TrainingExample(
                    token_ids=token_ids,

```python
async def add_model_response(
    self,
    state: State,
    prompt_messages: Messages,
    response: ModelResponse,
):
    """Add a model response as a trajectory step."""
    if response is not None and response.id == "overlong-prompt":
        state["prompt_too_long"] = True
        return
    completion_messages = await parse_response_messages(response, self.message_type)
    tokens = await parse_response_tokens(response, self.message_type)
    trajectory_step = TrajectoryStep(
        prompt=prompt_messages,
        completion=completion_messages,
        response=response,
        tokens=tokens,
        reward=None,
        advantage=None,
) -> State:
    """Generate a multi-turn rollout with the environment."""
    state = await self.init_state(input)
    state = await self.setup_state(state)
    while not await self.is_completed(state):
        prompt_messages = await self.get_prompt_messages(state)
        response = await self.get_model_response(
            client,
            model,
            prompt_messages,
            oai_tools=state["oai_tools"],
            sampling_args=sampling_args,
            message_type=self.message_type,
        )
        await self.add_model_response(state, prompt_messages, response)
    return state
```

**Key point**: Each step is a completely independent LLM request. No prefix requirement, no retokenization.


### TrajectoryStep Structure

```python
TrajectoryStepTokens = TypedDict("TrajectoryStepTokens", {
    "prompt_ids": list[int],
    "prompt_mask": list[int],
    "completion_ids": list[int],
    "completion_mask": list[int],
    "completion_logprobs": list[float],
})

TrajectoryStep = TypedDict("TrajectoryStep", {
    "prompt": Messages,                    # Exact prompt sent to LLM for this request
    "completion": Messages,                 # Exact completion returned from this request
    "response": ModelResponse,             # Raw response (ChatCompletion or Completion) with token_ids/logprobs
    "tokens": TrajectoryStepTokens | None, # Extracted token IDs, masks, and logprobs (None if unavailable)
    "reward": float | None,                # Reward for this step (None during rollout, filled by scoring)
    "advantage": float | None,             # Advantage for this step (for RL training)
```

**Note**: 
- `tokens` field contains extracted token IDs and logprobs from the response, enabling direct use for training without retokenization.
- `tokens` can be `None` if token data is unavailable (e.g., when logprobs not requested).
- Only `completion_logprobs` are stored (no prompt logprobs).
- `extras` allows environments to add custom metadata per step.

### Token Extraction

Token extraction is handled by `parse_response_tokens()` in `verifiers/utils/response_utils.py`:
    response: ModelResponse, message_type: MessageType
) -> TrajectoryStepTokens | None:
    """Extract token IDs, masks, and logprobs from vLLM response."""
    if message_type == "chat":
        assert isinstance(response, ChatCompletion)
        # Extract from response.prompt_token_ids and response.choices[0].token_ids
        # Extract logprobs from response.choices[0].logprobs.content
    elif message_type == "completion":
        assert isinstance(response, Completion)
        # Extract from response.choices[0].prompt_token_ids and token_ids
        # Extract logprobs from response.choices[0].logprobs.token_logprobs
    return None  # If token data unavailable
```

**vLLM Configuration**: To ensure token_ids are included in responses, vLLM must be configured with the following flags in `sampling_args`:
sampling_args = {
    "logprobs": 1,  # Enable logprobs
    "extra_body": {
        "return_tokens_as_token_ids": True,  # Return tokens as token IDs
        "return_token_ids": True,             # Include token_ids in response
        "prompt_logprobs": 1,                 # Optional: include prompt logprobs
    },
}
```

**Key insight**: Token IDs and logprobs come directly from vLLM response. No retokenization needed. If these flags are not set, `step["tokens"]` will be `None` and that trajectory step will be skipped during training data extraction.

# Access trajectory from results
for state in results.state:
    print(f"Trajectory steps: {len(state['trajectory'])}")
    for step in state["trajectory"]:
        print(f"  Prompt: {step['prompt']}")
        print(f"  Completion: {step['completion']}")
```

### Example 2: Multi-Turn with Trajectories

env = vf.load_environment("wordle")

# Create input from dataset row
row = env.dataset[0]
input = {
    "prompt": row["prompt"],
    "answer": row.get("answer", ""),
    "task": row.get("task", "default"),
    "info": row.get("info", {}),
    "example_id": row["example_id"],
}
state = await env.rollout(input, client, model="gpt-4o-mini")

# Access trajectory steps
for i, step in enumerate(state["trajectory"]):
    print(f"Step {i}:")
    print(f"  Prompt length: {len(step['prompt'])}")
    print(f"  Completion: {step['completion']}")
    print(f"  Reward: {step['reward']}")
    if step["tokens"]:
        print(f"  Token IDs: {step['tokens']['completion_ids']}")
```
)

# Process for training (horizontal processing)
training_examples = process_trajectories_for_training(
    states=results.state,
    mask_prompts=True,
)

# Each example has token_ids, logprobs, mask, reward
for example in training_examples[:5]:
    print(f"Sequence length: {len(example.token_ids)}")
```

## Design Decisions

### 1. Trajectories = LLM Request/Response Pairs ✅
**Decision**: Each trajectory step stores the complete prompt/completion/response from one LLM API call.
**Rationale**: Eliminates retokenization brittleness, keeps vLLM's native token_ids/logprobs pristine, enables horizontal training.

### 2. init_state() vs setup_state() Separation ✅
**Decision**: 
- `init_state()`: Environment-agnostic, takes `input: RolloutInput`, creates state with trajectory list
### 5. Horizontal Training Processing ✅
**Decision**: Each trajectory step becomes an independent training example.
**Rationale**: Simpler than vertical interleaving, avoids retokenization issues, more flexible for trainers.

### 6. Token Extraction ✅
**Decision**: Only `completion_logprobs` stored, not prompt logprobs. `tokens` can be `None` if unavailable.
**Rationale**: Prompt logprobs rarely needed for training. Graceful handling when token data unavailable.

### 7. Decorator-Based Termination ✅
**Decision**: Use `@stop`, `@cleanup`, `@teardown` decorators for lifecycle management instead of manual registration.
**Rationale**: Declarative pattern, automatic discovery via `__post_init__()`, clean separation of concerns, easier to extend.

    model, tokenizer = vf.get_model_and_tokenizer(args.model, use_liger=False)
    dataset = load_dataset(args.dataset, split="train")

    tok_counts = []
    for row in dataset:
        # count tokens in (prompt, completion)
        messages = row["prompt"] + row["completion"]  # type: ignore
        toks = tokenizer.apply_chat_template(messages, tokenize=True)
        tok_counts.append(len(toks))

    # tok count stats
    print(f"Dataset size: {len(tok_counts)}")
- `mock_openai_client` fixture provides a fully mocked AsyncOpenAI client
- Supports both chat completions and regular completions
- No actual API calls are made during testing

### Test Datasets
- `sample_dataset` - Basic question/answer dataset
- `sample_chat_dataset` - Pre-formatted chat messages
- Custom datasets can be created using `Dataset.from_dict()`

### Async Test Examples
```python
class MockAsyncOpenAI:
    """Mock AsyncOpenAI client that maps conversation inputs to outputs."""

    def __init__(self):
        self.chat_completions = {}  # Maps conversation history to responses
        self.text_completions = {}  # Maps prompts to responses
        self.default_chat_response = "This is a test response"
        self.default_text_response = "This is a test completion"
        self.base_url = "http://localhost/v1/"  # For testing URL parsing

        # Create mock structure
            "content": response,
            "finish_reason": finish_reason,
            "tool_calls": tool_calls,
        }

    def add_text_response(self, prompt, response, finish_reason="stop"):
        """Add a mapped response for specific prompt."""
        self.text_completions[prompt] = {
            "text": response,
            "finish_reason": finish_reason,
        }

    def set_default_responses(self, chat_response=None, text_response=None):
        mock_response.model = "test-model"
        mock_response.object = "chat.completion"

        return mock_response

    async def _handle_text_completion(self, prompt, **kwargs):
        """Handle text completion requests."""
        if prompt in self.text_completions:
            response_data = self.text_completions[prompt]
        else:
            response_data = {
                "text": self.default_text_response,
                "finish_reason": "stop",
            }
@pytest.fixture
def sample_dataset():
    """Return a sample dataset for testing."""
    return Dataset.from_dict(
        {
            "question": ["What is 2+2?", "What is the capital of France?"],
            "answer": ["4", "Paris"],
        }
    )


@pytest.fixture
def sample_chat_dataset():
    """Return a sample dataset with chat format."""
    return Dataset.from_dict(
        {
            "prompt": [
                [{"role": "user", "content": "What is 2+2?"}],
                [{"role": "user", "content": "What is the capital of France?"}],
            ],
            "answer": ["4", "Paris"],
            "example_id": [0, 1],
    """Return a SingleTurnEnv with mocked client and dataset."""
    return SingleTurnEnv(
        client=mock_openai_client,
        model="test-model",
        dataset=sample_dataset,
        system_prompt="You are a helpful assistant.",
        parser=Parser(),
        rubric=Rubric(),
    )


@pytest.fixture
def mock_singleturn_env_completion(mock_openai_client):
    """Return a SingleTurnEnv for completion format testing."""
    completion_dataset = Dataset.from_dict(
        {
            "prompt": ["Calculate 2+2:", "Name the capital of France:"],
            "answer": ["4", "Paris"],
        }
    )
    return SingleTurnEnv(
        client=mock_openai_client,

## Key Features

### 1. Input-Output Mapping
- **Chat completions**: Maps conversation history to specific responses
- **Text completions**: Maps prompts to specific responses
- **Order independence**: Responses are consistent regardless of call order
- **Default responses**: Fallback responses for unmapped inputs

### 2. Smart Hashing
- Conversations are converted to hashable keys for consistent lookup
- Message order and content are preserved in the mapping
- System prompts are included in the mapping for realistic testing

## Basic Usage

### Setting Up Chat Response Mappings

### Setting Up Text Completion Mappings

```python
# Add text completion mapping
client.add_text_response(
    prompt="Calculate 2+2:",
    response="4",
    finish_reason="stop"
)

# Use the client
response = client.completions.create(prompt="Calculate 2+2:")
assert response.choices[0].text == "4"
```

### Custom Default Responses

    chat_response="I don't know that",
    text_response="No answer available"
)

# Unmapped requests will use these defaults
response = client.completions.create(prompt="Unknown question")
assert response.choices[0].text == "No answer available"
```

## Advanced Usage

### Testing Order Independence

```python
# Set up multiple mappings
client.add_chat_response(
    messages=[{"role": "user", "content": "Question A"}],
    response="Answer A"
)
client.add_chat_response(
    messages=[{"role": "user", "content": "Question B"}],
    response="Answer B"
)

# Test in different orders
response_a = await client.chat.completions.create(
    messages=[{"role": "user", "content": "Question A"}]
)
response_b = await client.chat.completions.create(
    messages=[{"role": "user", "content": "Question B"}]
)

# Test reverse order
response_b2 = await client.chat.completions.create(
    messages=[{"role": "user", "content": "Question B"}]
)
response_a2 = await client.chat.completions.create(
    messages=[{"role": "user", "content": "Question A"}]
)

# All responses are consistent
assert response_a.choices[0].message.content == response_a2.choices[0].message.content
assert response_b.choices[0].message.content == response_b2.choices[0].message.content
# Create environment
env = SingleTurnEnv(
    client=client,
    model="test-model",
    dataset=your_dataset,
    system_prompt="You are a helpful assistant.",
    parser=Parser(),
    rubric=Rubric()
)

# Run rollouts - responses will be consistent
completion, state = await env.rollout(
    client=client,
    model="test-model",
    prompt=[{"role": "user", "content": "What is 2+2?"}],
    answer="4"
)
```

## Comparison with Alternative Approaches
- Graceful fallbacks for edge cases

## Best Practices

1. **Set up mappings early**: Add all expected input-output pairs before running tests
2. **Use realistic inputs**: Include system prompts and full conversation context
3. **Test edge cases**: Include unmapped inputs to verify default behavior
4. **Keep mappings simple**: One mapping per distinct conversation or prompt
5. **Group related tests**: Use fixtures to share common mappings across test methods


================================================
FILE: tests/mock_openai_client.py
        default_response: str = "Test response",
    ):
        self.responses = responses or {}
        self.default_response = default_response
        self.call_count = 0
        self.last_prompt = None
        self.last_model = None

    def create(self, model: str, prompt: str, **kwargs) -> MockCompletionResponse:
        self.call_count += 1
        self.last_prompt = prompt
        self.last_model = model

        # Handle special error cases
        if kwargs.get("max_tokens", 0) == 1:
            return MockCompletionResponse("", "length")

        # Check for specific response patterns
        for pattern, response in self.responses.items():
            if pattern in prompt:
                return MockCompletionResponse(response)

        return MockCompletionResponse(self.default_response)


            )
        ]

    def test_stop_conditions_sorted_by_priority(self, mock_openai_client):
        """Test that stop conditions are sorted by priority."""
        dataset = Dataset.from_dict({"question": ["test"], "answer": ["test"]})
        env = RankedStopEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=dataset,
            parser=vf.Parser(),
        names = [method.__name__ for method in class_stop_conditions]
        assert names == ["early_stop", "another_default", "default_stop", "late_stop"]

    def test_stop_conditions_tie_breaking_alphabetical(self, mock_openai_client):
        """Test that stop conditions with same priority are sorted alphabetically."""
        dataset = Dataset.from_dict({"question": ["test"], "answer": ["test"]})
        env = RankedStopEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=dataset,
            parser=vf.Parser(),
        assert names == ["another_default", "default_stop"]

    @pytest.mark.asyncio
    async def test_stop_conditions_execution_order(self, mock_openai_client):
        """Test that stop conditions execute in priority order."""
        dataset = Dataset.from_dict({"question": ["test"], "answer": ["test"]})
        env = RankedStopEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=dataset,
            parser=vf.Parser(),

        mock_openai_client.set_default_responses(chat_response="test")

        await env.rollout(
            input=RolloutInput(
                prompt=[{"role": "user", "content": "test"}],
                answer="test",
                example_id=0,
                task="test",
            ),
            client=mock_openai_client,
class TestCleanupPriorityOrdering:
    """Test cleanup handler priority ordering."""

    def test_cleanup_handlers_sorted_by_priority(self, mock_openai_client):
        """Test that cleanup handlers are sorted by priority."""
        dataset = Dataset.from_dict({"question": ["test"], "answer": ["test"]})
        env = RankedCleanupEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=dataset,
            parser=vf.Parser(),
            "late_cleanup",
        ]

    def test_cleanup_handlers_tie_breaking_alphabetical(self, mock_openai_client):
        """Test that cleanup handlers with same priority are sorted alphabetically."""
        dataset = Dataset.from_dict({"question": ["test"], "answer": ["test"]})
        env = RankedCleanupEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=dataset,
            parser=vf.Parser(),
        assert names == ["another_default_cleanup", "default_cleanup"]

    @pytest.mark.asyncio
    async def test_cleanup_handlers_execution_order(self, mock_openai_client):
        """Test that cleanup handlers execute in priority order."""
        dataset = Dataset.from_dict({"question": ["test"], "answer": ["test"]})
        env = RankedCleanupEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=dataset,
            parser=vf.Parser(),
            rubric=vf.Rubric(),
        )

        state = await env.init_state(
            RolloutInput(
                prompt=[{"role": "user", "content": "test"}],
                answer="test",
                example_id=0,
                task="test",
            ),
            client=mock_openai_client,
class TestTeardownPriorityOrdering:
    """Test teardown handler priority ordering."""

    def test_teardown_handlers_sorted_by_priority(self, mock_openai_client):
        """Test that teardown handlers are sorted by priority."""
        dataset = Dataset.from_dict({"question": ["test"], "answer": ["test"]})
        env = RankedTeardownEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=dataset,
            parser=vf.Parser(),
            "late_teardown",
        ]

    def test_teardown_handlers_tie_breaking_alphabetical(self, mock_openai_client):
        """Test that teardown handlers with same priority are sorted alphabetically."""
        dataset = Dataset.from_dict({"question": ["test"], "answer": ["test"]})
        env = RankedTeardownEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=dataset,
            parser=vf.Parser(),
        assert names == ["another_default_teardown", "default_teardown"]

    @pytest.mark.asyncio
    async def test_teardown_handlers_execution_order(self, mock_openai_client):
        """Test that teardown handlers execute in priority order."""
        dataset = Dataset.from_dict({"question": ["test"], "answer": ["test"]})
        env = RankedTeardownEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=dataset,
            parser=vf.Parser(),
                pass

            async def env_response(self, messages, state, **kwargs):
                return []

        dataset = Dataset.from_dict({"question": ["test"], "answer": ["test"]})
        env = DefaultPriorityEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=dataset,
            parser=vf.Parser(),
                return False

            async def env_response(self, messages, state, **kwargs):
                return []

        dataset = Dataset.from_dict({"question": ["test"], "answer": ["test"]})
        env = HighPriorityEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=dataset,
            parser=vf.Parser(),
                return False

            async def env_response(self, messages, state, **kwargs):
                return []

        dataset = Dataset.from_dict({"question": ["test"], "answer": ["test"]})
        env = LowPriorityEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=dataset,
            parser=vf.Parser(),
                return False

            async def env_response(self, messages, state, **kwargs):
                return []

        dataset = Dataset.from_dict({"question": ["test"], "answer": ["test"]})
        env = MixedPriorityEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=dataset,
            parser=vf.Parser(),
            return 0.8

        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            eval_dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(funcs=[func1, func2], weights=[1.0, 0.5]),
        )

        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            eval_dataset=Dataset.from_dict({"question": ["q2"], "answer": ["a2"]}),
            rubric=Rubric(funcs=[func2, func3], weights=[0.7, 1.0]),
        )

        env_map = {"task1": env1, "task2": env2}
        rubric = EnvGroupRubric(env_map)
            return 0.6

        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            eval_dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(funcs=[func1], weights=[1.0]),
        )

        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            eval_dataset=Dataset.from_dict({"question": ["q2"], "answer": ["a2"]}),
            rubric=Rubric(funcs=[func2], weights=[1.0]),
        )

        env_map = {"math": env1, "code": env2}
        rubric = EnvGroupRubric(env_map)

        # Test scoring for "math" task
        state = State(
            input=RolloutInput(
                prompt="Test prompt",
                answer="Test answer",
                task="math",
                example_id=0,
            )
        )
    async def test_env_group_rubric_unknown_task(self, mock_openai_client):
        """Test scoring with unknown task returns zeros."""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            eval_dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(),
        )

        env_map = {"known_task": env1}
        rubric = EnvGroupRubric(env_map)

        state = State(
            input=RolloutInput(
                prompt="Test",
                task="unknown_task",
                example_id=0,
            )
        )
        state["completion"] = "Test"
    def test_env_group_initialization(self, mock_openai_client):
        """Test EnvGroup initialization with multiple environments."""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(),
        )

        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q2"], "answer": ["a2"]}),
            rubric=Rubric(),
        )

        env_group = EnvGroup(envs=[env1, env2])

    def test_env_group_unique_example_ids(self, mock_openai_client):
        """Test EnvGroup initialization with multiple environments."""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(),
        )

        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q2"], "answer": ["a2"]}),
            rubric=Rubric(),
        )

        env_group = EnvGroup(envs=[env1, env2])
        dataset = env_group.get_dataset()
    def test_env_group_with_custom_names(self, mock_openai_client):
        """Test EnvGroup with custom environment names."""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(),
        )

        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q2"], "answer": ["a2"]}),
            rubric=Rubric(),
        )

        env_group = EnvGroup(envs=[env1, env2], env_names=["math", "code"])

    def test_env_group_mismatched_names_fails(self, mock_openai_client):
        """Test that EnvGroup fails when env_names length doesn't match envs."""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            eval_dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(),
        )

        with pytest.raises(
            ValueError, match="Number of env_names must match number of envs"
        """Test that EnvGroup properly concatenates datasets with task labels."""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict(
                {"question": ["q1", "q2"], "answer": ["a1", "a2"]}
            ),
            rubric=Rubric(),
        )

        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q3"], "answer": ["a3"]}),
            rubric=Rubric(),
        )

        env_group = EnvGroup(envs=[env1, env2], env_names=["math", "code"])

    def test_env_group_rubric_type(self, mock_openai_client):
        """Test that EnvGroup creates EnvGroupRubric."""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(),
        )

        env_group = EnvGroup(envs=[env1])

        """Test that rollout is properly routed to the correct sub-environment."""
        # Create environments with different behaviors
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(),
        )

        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q2"], "answer": ["a2"]}),
            rubric=Rubric(),
        )

        # Mock the rollout methods to return different values
        async def env1_rollout(*args, **kwargs):
            state = State(
                input=RolloutInput(prompt="Test prompt", task="math", example_id=0)
            )
            state["env"] = "env1"
            return state

        async def env2_rollout(*args, **kwargs):
            state = State(
                input=RolloutInput(prompt="Test prompt", task="code", example_id=0)
            )
            state["env"] = "env2"
            return state

        # Explicitly mark shadowing as intentional for the type checker, and keep references to mocks

        env_group = EnvGroup(envs=[env1, env2], env_names=["math", "code"])

        # Test routing to math environment
        state1 = await env_group.rollout(
            input=RolloutInput(prompt="Test prompt", task="math", example_id=0),
            client=mock_openai_client,
            model="test-model",
        )

        assert state1["env"] == "env1"
        env1_rollout_mock.reset_mock()
        env2_rollout_mock.reset_mock()

        # Test routing to code environment
        state2 = await env_group.rollout(
            input=RolloutInput(prompt="Test prompt", task="code", example_id=0),
            client=mock_openai_client,
            model="test-model",
        )

        assert state2["env"] == "env2"
    def test_get_env_for_task(self, mock_openai_client):
        """Test getting environment for a specific task."""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(),
        )

        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q2"], "answer": ["a2"]}),
            rubric=Rubric(),
        )

        env_group = EnvGroup(envs=[env1, env2], env_names=["math", "code"])

    async def test_env_group_generate(self, mock_openai_client):
        """Test generate method with EnvGroup."""
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            rubric=Rubric(),
        )

        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q2"], "answer": ["a2"]}),
            rubric=Rubric(),
        )

        env_group = EnvGroup(envs=[env1, env2], env_names=["math", "code"])


        cast(Any, env_group.rubric).score_group = mock_score_group

        inputs = [
            RolloutInput(
                prompt=[{"role": "user", "content": "Math question"}],
                answer="math_answer",
                task="math",
                example_id=0,
            ),
            RolloutInput(
                prompt=[{"role": "user", "content": "Code question"}],
                answer="code_answer",
                task="code",
                example_id=1,
            ),
        ]
        """Test EnvGroup with environments having different dataset configurations."""
        # Environment with both train and eval datasets
        env1 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q1"], "answer": ["a1"]}),
            eval_dataset=Dataset.from_dict({"question": ["eq1"], "answer": ["ea1"]}),
            rubric=Rubric(),
        )

        # Environment with only eval dataset
        env2 = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=Dataset.from_dict({"question": ["q2"], "answer": ["a2"]}),
            eval_dataset=Dataset.from_dict({"question": ["eq2"], "answer": ["ea2"]}),
            rubric=Rubric(),
        )

        env_group = EnvGroup(envs=[env1, env2], env_names=["task1", "task2"])

    ):
        """Simple test rollout implementation."""
        state = await self.init_state(input, client=client, model=model)
        state = await self.setup_state(state)

        prompt_messages = state["prompt"]
        response = await self.get_model_response(state, prompt_messages)

        from verifiers.utils.response_utils import parse_response_messages

        completion_messages = await parse_response_messages(response, self.message_type)
        from verifiers.types import TrajectoryStep
        from verifiers.utils.response_utils import parse_response_tokens

        tokens = await parse_response_tokens(response, self.message_type)
        trajectory_step = TrajectoryStep(
            prompt=prompt_messages,
            completion=completion_messages,
            response=response,
            tokens=tokens,
            reward=None,
            advantage=None,
        state["trajectory"].append(trajectory_step)
        state["is_completed"] = True

        from verifiers.utils.message_utils import concat_messages

        last_prompt = state["trajectory"][-1]["prompt"]
        last_completion = state["trajectory"][-1]["completion"]
        full_conversation = concat_messages([last_prompt, last_completion])
        state["completion"] = full_conversation[len(state["prompt"]) :]

        return state


def _make_metadata(
                model="test-model",
                parser=Parser(),
                rubric=Rubric(),
            )

    def test_completion_mode_with_system_prompt_raises_error(
        self, mock_openai_client, sample_dataset
    ):
        """Test that completion mode with system prompt raises error."""
        with pytest.raises(ValueError, match="not supported for completion tasks"):
            SimpleEnvironment(
                dataset=sample_dataset,
                message_type="completion",
                system_prompt="test prompt",
                parser=Parser(),
                rubric=Rubric(),
            )

    def test_different_parser_rubric_parser_warns(
    async def test_get_model_response_chat(self, mock_openai_client):
        """Test get_model_response with chat format."""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model="test-model",
            eval_dataset=Dataset.from_dict({"question": ["test"], "answer": ["test"]}),
            parser=Parser(),
            rubric=Rubric(),
        )

        prompt: Messages = [{"role": "user", "content": "Hello"}]
        state = await env.init_state(
            input=RolloutInput(example_id=0, task="test", prompt=prompt),
            client=mock_openai_client,
            model="test-model",
        )
        response = await env.get_model_response(
            state,
            prompt,
        )

        # Check response structure
        assert hasattr(response, "choices")
        assert response is not None
    async def test_get_model_response_completion(self, mock_openai_client):
        """Test get_model_response with completion format."""
        env = SimpleEnvironment(
            client=mock_openai_client,
            model="test-model",
            eval_dataset=Dataset.from_dict({"prompt": ["test"], "answer": ["test"]}),
            message_type="completion",
            parser=Parser(),
            rubric=Rubric(),
        )

        prompt = "Complete this:"
        state = await env.init_state(
            input=RolloutInput(example_id=0, task="test", prompt=prompt),
            client=mock_openai_client,
            model="test-model",
        )
        response = await env.get_model_response(
            state,
            prompt,
        )

        # Check response structure
        assert hasattr(response, "choices")
        assert response is not None

        env.rubric.score_group = mock_score_group  # type: ignore[attr-defined]

        inputs = [
            RolloutInput(
                prompt=[{"role": "user", "content": "Hello"}],
                answer="Hi",
                example_id=0,
            )
        ]


        env.rubric.score_group = mock_score_group  # type: ignore[attr-defined]

        inputs = [
            RolloutInput(
                prompt=[{"role": "user", "content": "Hello"}],
                answer="Hi",
                info={},
                example_id=0,
            )
        ]

    def test_make_dataset(self, mock_openai_client, sample_dataset):
        """Test creating a dataset from evaluation results."""

        results = GenerateOutputs(
            prompt=[[{"role": "user", "content": "Hello"}]],
            completion=[[{"role": "assistant", "content": "Hi"}]],
            answer=["Hi"],
            reward=[1.0],
            task=["default"],
            state=[
        )

        dataset = build_dataset(results)

        assert len(dataset) == 1
        assert "prompt" in dataset.column_names
        assert "completion" in dataset.column_names
        assert "answer" in dataset.column_names
        assert "reward" in dataset.column_names
        assert "task" in dataset.column_names
        assert "example_id" in dataset.column_names
    @pytest.mark.asyncio
    async def test_generate_state_preserves_references(self, mock_openai_client):
        """Test that generate creates state with preserved references instead of deep copying"""
        env = SimpleEnvironment(
            eval_dataset=Dataset.from_dict(
                {"question": ["test question"], "answer": ["test answer"]}
            ),
            parser=Parser(),
            rubric=Rubric(),
        )

            return_value=RolloutScores(reward=[1.0], metrics={})
        )

        inputs = [
            RolloutInput(
                prompt=[{"role": "user", "content": "Hello"}],
                answer="Hi",
                info={"key": "value"},
                example_id=0,
            )
        ]
        )

        assert len(results["state"]) == 1
        state = results["state"][0]

        assert state["prompt"] is results["prompt"][0]
        assert state["completion"] is results["completion"][0]
        assert state["answer"] is results["answer"][0]
        assert state["info"] is results["info"][0]
        assert state["example_id"] is results["example_id"][0]

    @pytest.mark.asyncio
    async def test_generate_updates_metadata(self, mock_openai_client):
        """Test that metadata fields are updated after generate() completes."""
        dataset = Dataset.from_dict(
            {
                "question": ["What is 2+2?", "What is 3+3?"],
                "answer": ["4", "6"],
            }
        )

        def reward_a(**kwargs):
    @pytest.mark.asyncio
    async def test_generate_metadata_without_scoring(self, mock_openai_client):
        """Test that metadata handles scoring correctly."""
        dataset = Dataset.from_dict(
            {
                "question": ["What is 2+2?"],
                "answer": ["4"],
            }
        )

        env = SimpleEnvironment(dataset=dataset, rubric=Rubric())
    return MockClientWithKwargsCapture()


@pytest.fixture
def test_environment():
    dummy_dataset = Dataset.from_dict({"prompt": ["test"]})
    return SingleTurnEnv(dataset=dummy_dataset, message_type="chat")


@pytest.mark.asyncio
async def test_sets_modalities_text_when_audio_and_missing(
    mock_client, test_environment
):
    prompt: vf.Messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "input_audio",
            ],
        }
    ]

    state = await test_environment.init_state(
        input=RolloutInput(example_id=0, task="test", prompt=prompt),
        client=mock_client,
        model="gpt-4o-audio-preview",
    )

    await test_environment.get_model_response(state, prompt)

    kwargs = mock_client.get_kwargs()
    assert kwargs is not None
    assert kwargs.get("modalities") == ["text"]
    assert kwargs.get("messages") == prompt


@pytest.mark.asyncio
async def test_does_not_override_existing_modalities(mock_client, test_environment):
    prompt: vf.Messages = [
        {
            "role": "user",
            "content": [
                {
                    "type": "input_audio",
            ],
        }
    ]

    state = await test_environment.init_state(
        input=RolloutInput(example_id=0, task="test", prompt=prompt),
        client=mock_client,
        model="gpt-4o-audio-preview",
        sampling_args={"modalities": ["text", "audio"]},
    )
    await test_environment.get_model_response(state, prompt)

    kwargs = mock_client.get_kwargs()
    assert kwargs is not None
    assert kwargs.get("modalities") == ["text", "audio"]


@pytest.mark.asyncio
async def test_does_not_add_modalities_when_no_audio(mock_client, test_environment):
    prompt: vf.Messages = [{"role": "user", "content": "hello"}]
    state = await test_environment.init_state(
        input=RolloutInput(example_id=0, task="test", prompt=prompt),
        client=mock_client,
        model="gpt-4.1-mini",
    )
    await test_environment.get_model_response(state, prompt)

    kwargs = mock_client.get_kwargs()
    assert kwargs is not None
    assert "modalities" not in kwargs

        state = await self.init_state(
            input, client=client, model=model, sampling_args=sampling_args
        )
        state = await self.setup_state(state)

        prompt_messages = state["prompt"]
        response = await self.get_model_response(state=state, prompt=prompt_messages)
        assert response is not None

        from verifiers.types import TrajectoryStep
        from verifiers.utils.response_utils import (
            parse_response_messages,
        )

        completion_messages = await parse_response_messages(response, self.message_type)
        tokens = await parse_response_tokens(response, self.message_type)
        trajectory_step = TrajectoryStep(
            prompt=prompt_messages,
            completion=completion_messages,
            response=response,
            tokens=tokens,
            reward=None,
            advantage=None,
        state["trajectory"].append(trajectory_step)
        state["is_completed"] = True

        from verifiers.utils.message_utils import concat_messages

        last_prompt = state["trajectory"][-1]["prompt"]
        last_completion = state["trajectory"][-1]["completion"]
        full_conversation = concat_messages([last_prompt, last_completion])
        state["completion"] = full_conversation[len(state["prompt"]) :]

        return state


def _make_metadata(


def _make_env(
    mock_openai_client, dataset: Dataset | None = None, **kwargs
) -> DummyEnvironment:
    ds = dataset or Dataset.from_dict({"question": ["q1"], "answer": ["a1"]})
    return DummyEnvironment(
        client=mock_openai_client,
        model="test-model",
        dataset=ds,
        parser=Parser(),


@pytest.mark.asyncio
async def test_get_model_response_chat_with_tools(mock_openai_client):
    env = _make_env(mock_openai_client)
    prompt: vf.Messages = [{"role": "user", "content": "Hello"}]
    tools = [
        {
            "type": "function",
            "function": {"name": "echo", "description": "echo", "parameters": {}},
        }
    ]
    state = await env.init_state(
        input=RolloutInput(example_id=0, task="test", prompt=prompt),
        client=mock_openai_client,
        model="test-model",
    )
    state["oai_tools"] = tools
    resp = await env.get_model_response(
        state=state,
        prompt=prompt,
    )
    # Ensure the client was invoked and received tools kwarg
    assert hasattr(resp, "choices")
    assert mock_openai_client.chat.completions.create.await_count == 1
    kwargs = mock_openai_client.chat.completions.create.await_args.kwargs
@pytest.mark.asyncio
async def test_get_model_response_completion_rejects_tools(mock_openai_client):
    env = _make_env(mock_openai_client, message_type="completion")
    with pytest.raises(vf.ModelError):
        state = await env.init_state(
            input=RolloutInput(example_id=0, task="test", prompt="Complete this"),
            client=mock_openai_client,
            model="test-model",
        )
        state["oai_tools"] = [{"type": "function", "function": {"name": "noop"}}]
        await env.get_model_response(state=state, prompt="Complete this")


def test_run_rollouts_with_max_concurrent(mock_openai_client):
    env = _make_env(mock_openai_client)
    inputs = [
        RolloutInput(
            prompt=[{"role": "user", "content": "hi"}],
            answer="",
            example_id=i,
        )
        for i in range(3)
    ]

def test_run_rollouts_with_semaphore(mock_openai_client):
    env = _make_env(mock_openai_client)
    inputs = [
        RolloutInput(
            prompt=[{"role": "user", "content": "hi"}],
            answer="",
            example_id=i,
        )
        for i in range(3)
    ]

def test_evaluate_fallback_and_repeat(mock_openai_client):
    # No eval_dataset provided -> falls back to train; ensure >= num_examples
    from datasets import Dataset

    ds = Dataset.from_dict({"question": ["q1", "q2"], "answer": ["a1", "a2"]})
    env = _make_env(mock_openai_client, dataset=ds)
    res = asyncio.run(
        env.evaluate(
            client=mock_openai_client,
            model="test-model",
            num_examples=2,
            rollouts_per_example=2,
        )
    )
    # Expect n * r rollouts in outputs
    assert len(res["prompt"]) == 2 * 2
    assert len(res["completion"]) == 2 * 2


@pytest.mark.asyncio
async def test_generate_inside_running_loop(mock_openai_client):
    env = _make_env(mock_openai_client)
    inputs = [
        RolloutInput(
            prompt=[{"role": "user", "content": "Hi"}],
            answer="",
            example_id=0,
        )
    ]
    # Call the async API directly inside a running event loop to avoid nested sync wrapper issues
    assert isinstance(sanitized[0]["tool_calls"][0], str)


def test_make_dataset_basic_without_tools(mock_openai_client):
    results = GenerateOutputs(
        prompt=[[{"role": "user", "content": "Hi"}]],
        completion=[[{"role": "assistant", "content": "Hello"}]],
        answer=[""],
        state=[
            {
                "timing": {

    async def fake_run_evaluation(config):
        captured["sampling_args"] = dict(config.sampling_args)
        metadata = _make_metadata(config)
        return GenerateOutputs(
            prompt=[[{"role": "user", "content": "p"}]],
            completion=[[{"role": "assistant", "content": "c"}]],
            answer=[""],
            state=[
                {
                    "timing": {

    # Metric follows same pattern
    metric_values = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]

    results = GenerateOutputs(
        prompt=[[{"role": "user", "content": f"q{i}"}] for i in range(6)],
        completion=[[{"role": "assistant", "content": f"a{i}"}] for i in range(6)],
        answer=[""] * 6,
        state=[{"timing": {"generation_ms": 0.0, "scoring_ms": 0.0, "total_ms": 0.0}}]
        * 6,
        task=["default"] * 6,

    rewards = [0.1, 0.2, 0.3]
    example_ids = [0, 1, 2]

    results = GenerateOutputs(
        prompt=[[{"role": "user", "content": f"q{i}"}] for i in range(3)],
        completion=[[{"role": "assistant", "content": f"a{i}"}] for i in range(3)],
        answer=[""] * 3,
        state=[{"timing": {"generation_ms": 0.0, "scoring_ms": 0.0, "total_ms": 0.0}}]
        * 3,
        task=["default"] * 3,
    # Order: [ex0_r0, ex0_r1, ex0_r2, ex1_r0, ex1_r1, ex1_r2]
    rewards = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]
    example_ids = [0, 0, 0, 1, 1, 1]

    results = GenerateOutputs(
        prompt=[[{"role": "user", "content": f"q{i}"}] for i in range(6)],
        completion=[[{"role": "assistant", "content": f"a{i}"}] for i in range(6)],
        answer=[""] * 6,
        state=[{"timing": {"generation_ms": 0.0, "scoring_ms": 0.0, "total_ms": 0.0}}]
        * 6,
        task=["default"] * 6,

        rubric = vf.MathRubric()

        state = vf.State(
            input=vf.RolloutInput(
                prompt="test prompt",
                answer=test_case["answer"],
                task="test_task",
                example_id=0,
            )
        )

        rubric = vf.MathRubric()

        state = vf.State(
            input=vf.RolloutInput(
                prompt="test prompt",
                answer=test_case["answer"],
                task="test_task",
                example_id=0,
            )
        )

        rubric = vf.MathRubric(timeout_seconds=timeout_seconds)

        state = vf.State(
            input=vf.RolloutInput(
                prompt="test prompt",
                answer=answer,
                task="test_task",
                example_id=0,
            )
        )
    Demonstrates that HuggingFace Dataset.map() introduces None values
    when content items have different schemas, and stripping Nones fixes this.
    """
    from datasets import Dataset

    def format_prompt(example):
        return {
            "prompt": [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": example["question"]},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{example['image']}"
                            },
                    ],
                }
            ]
        }

    ds = Dataset.from_dict({"question": ["What is this?"], "image": ["abc123"]})
    ds = ds.map(format_prompt)

    prompt = ds[0]["prompt"]
    content = prompt[0]["content"]

    # Dataset.map() unifies schemas, adding None for missing keys
    assert "image_url" in content[0], (
        "Dataset.map should add image_url key to text item"
    )
    assert content[0]["image_url"] is None, "text item should have image_url=None"
    assert "text" in content[1], "Dataset.map should add text key to image_url item"
    assert content[1]["text"] is None, "image_url item should have text=None"

    # Strip None values (same logic as in get_model_response)
    for msg in prompt:
        msg_content = msg.get("content")
        if isinstance(msg_content, list):
            msg["content"] = [
                {k: v for k, v in c.items() if v is not None}
                if isinstance(c, dict)
                else c
                for c in msg_content
            ]

    cleaned_content = prompt[0]["content"]

    assert "image_url" not in cleaned_content[0], (
        "stripping should remove image_url from text item"
    )
    assert "text" not in cleaned_content[1], (

    @pytest.mark.asyncio
    async def test_basic_multiturn_rollout(self, mock_multiturn_env):
        """Test basic multi-turn conversation that completes normally."""
        # Configure mock to return responses that lead to completion
        prompt = [{"role": "user", "content": "Start conversation"}]

        # Set up responses for the conversation turns
        mock_multiturn_env.client.add_chat_response(
            messages=[{"role": "user", "content": "Start conversation"}],
            response="First response",
            response="Final response DONE",
        )

        state = await mock_multiturn_env.rollout(
            input=RolloutInput(
                prompt=prompt,
                answer="target_answer",
                example_id=0,
            ),
            client=mock_multiturn_env.client,
            model="test-model",
        assert completion[2]["content"] == "Second response"
        assert completion[4]["content"] == "Final response DONE"

        # Check state structure
        assert state["answer"] == "target_answer"
        assert state["prompt"] == prompt

    @pytest.mark.asyncio
    async def test_max_turns_limiting(self, mock_multiturn_env_max_turns):
        """Test that rollout stops at max_turns."""
        # Set up responses that would continue indefinitely
        mock_multiturn_env_max_turns.client.set_default_responses(
            chat_response="Keep going"
        )

        prompt = [{"role": "user", "content": "Start conversation"}]
        state = await mock_multiturn_env_max_turns.rollout(
            input=RolloutInput(
                prompt=prompt,
                answer="target_answer",
                example_id=0,
            ),
            client=mock_multiturn_env_max_turns.client,
            model="test-model",
            rubric=Rubric(),
        )

        mock_openai_client.set_default_responses(chat_response="Still thinking")

        prompt: Messages = [{"role": "user", "content": "Start"}]
        state = await env.rollout(
            input=RolloutInput(
                prompt=prompt,
                answer="target",
                example_id=0,
            ),
            client=mock_openai_client,
            model="test-model",
        """Test that state is properly initialized with all required fields."""
        mock_multiturn_env.client.add_chat_response(
            messages=[{"role": "user", "content": "Test state"}], response="Quick DONE"
        )

        prompt = [{"role": "user", "content": "Test state"}]
        state = await mock_multiturn_env.rollout(
            input=RolloutInput(
                prompt=prompt,
                answer="test_answer",
                task="test_task",
                info={"extra": "data"},
                example_id=0,
            ),
            client=mock_multiturn_env.client,
            model="test-model",
        )

        # Check all state fields are initialized
        assert state["prompt"] == prompt
        assert state["answer"] == "test_answer"
        assert state["task"] == "test_task"
        assert state["info"] == {"extra": "data"}
        assert state["example_id"] == 0
        assert "trajectory" in state

    @pytest.mark.asyncio
    async def test_immediate_completion(self, mock_multiturn_env):
        """Test completion detection on first turn."""
        mock_multiturn_env.client.add_chat_response(
            messages=[{"role": "user", "content": "Quick question"}],
            response="Immediate DONE",
        )

        prompt = [{"role": "user", "content": "Quick question"}]
        state = await mock_multiturn_env.rollout(
            input=RolloutInput(
                prompt=prompt,
                answer="target_answer",
                example_id=0,
            ),
            client=mock_multiturn_env.client,
            model="test-model",
                {"role": "user", "content": "Continue (turn 1)"},
            ],
            response="Final response DONE",
        )

        prompt = [{"role": "user", "content": "Start conversation"}]
        state = await mock_multiturn_env.rollout(
            input=RolloutInput(
                prompt=prompt,
                answer="target_answer",
                example_id=0,
            ),
            client=mock_multiturn_env.client,
            model="test-model",
        user_messages = [msg for msg in completion if msg["role"] == "user"]
        assert len(user_messages) >= 1
        assert "Continue (turn 1)" in user_messages[0]["content"]

    @pytest.mark.asyncio
    async def test_prompt_copying(self, mock_multiturn_env):
        """Test that original prompt is not modified."""
        original_prompt = [{"role": "user", "content": "Original message"}]
        prompt_copy = [{"role": "user", "content": "Original message"}]

        mock_multiturn_env.client.add_chat_response(
            messages=[{"role": "user", "content": "Original message"}],
            response="Response DONE",
        )

        await mock_multiturn_env.rollout(
            input=RolloutInput(
                prompt=original_prompt,
                answer="test_answer",
                example_id=0,
            ),
            client=mock_multiturn_env.client,
            model="test-model",
        )

        # Original prompt should be unchanged
        assert original_prompt == prompt_copy

    @pytest.mark.asyncio
    async def test_sampling_args_passed_through(self, mock_multiturn_env):
        """Test that sampling arguments are passed to model calls."""
        mock_multiturn_env.client.add_chat_response(
            messages=[{"role": "user", "content": "Test sampling"}],
            response="Quick DONE",
        )

        prompt = [{"role": "user", "content": "Test sampling"}]
        sampling_args = {"temperature": 0.8, "max_tokens": 50}

        await mock_multiturn_env.rollout(
            input=RolloutInput(
                prompt=prompt,
                answer="test_answer",
                example_id=0,
            ),
            client=mock_multiturn_env.client,
            model="test-model",
                self, messages: Messages, state: State, **kwargs
            ) -> Messages:
                return " Continue."

        completion_dataset = Dataset.from_dict(
            {"prompt": ["Start:"], "answer": ["Done"]}
        )

        env = CompletionMultiTurnEnv(
            client=mock_openai_client,
            model="test-model",
        mock_openai_client.add_text_response("Start:", "First response")
        mock_openai_client.add_text_response(
            "Start:First response Continue.", "Final DONE"
        )

        prompt = "Start:"
        state = await env.rollout(
            input=RolloutInput(
                prompt=prompt,
                answer="Done",
                example_id=0,
            ),
            client=mock_openai_client,
            model="test-model",
            rubric=Rubric(),
        )

        env.client.set_default_responses(chat_response="Continue")  # type: ignore

        prompt = [{"role": "user", "content": "Start"}]
        state = await env.rollout(
            input=RolloutInput(
                prompt=prompt,
                answer="test",
                example_id=0,
            ),
            client=env.client,  # type: ignore
            model="test-model",

        env.client.add_chat_response(  # type: ignore
            messages=[{"role": "user", "content": "Start"}], response="First response"
        )

        prompt = [{"role": "user", "content": "Start"}]
        state = await env.rollout(
            input=RolloutInput(
                prompt=prompt,
                answer="test",
                example_id=0,
            ),
            client=env.client,  # type: ignore
            model="test-model",
                {"role": "user", "content": "Please finish with DONE"},
            ],
            response="DONE",
        )

        prompt = [{"role": "user", "content": "Start"}]
        state = await mock_multiturn_env.rollout(
            input=RolloutInput(
                prompt=prompt,
                answer="test",
                example_id=0,
            ),
            client=mock_multiturn_env.client,
            model="test-model",

        rubric = Rubric(funcs=[func1, func2], weights=[1.0, 0.5])

        state = State(
            input=RolloutInput(
                prompt="test prompt",
                answer="test",
                task="test_task",
                example_id=0,
            )
        )
            {"role": "assistant", "content": "Hi there!"},
        ]

        state = State(
            input=RolloutInput(
                prompt="test",
                answer="test",
                task="test",
                example_id=0,
            )
        )
        rubric = Rubric(funcs=[accuracy_func, length_func], weights=[1.0, 0.1])

        states = [
            State(
                input=RolloutInput(
                    prompt="prompt1",
                    answer="answer1",
                    task="task1",
                    example_id=0,
                )
            ),
            State(
                input=RolloutInput(
                    prompt="prompt2",
                    answer="answer2",
                    task="task2",
                    example_id=1,
                )
            ),
            State(
                input=RolloutInput(
                    prompt="prompt3",
                    answer="answer3",
                    task="task3",
                    example_id=2,
                )
            ),

        rubric = Rubric(funcs=[func1, func2], weights=[2.0, 3.0])

        state = State(
            input=RolloutInput(
                prompt="test",
                answer="test",
                task="test",
                example_id=0,
            )
        )

        rubric = Rubric(funcs=[simple_func], weights=[1.0])

        state = State(
            input=RolloutInput(
                prompt="test",
                answer="test",
                task="test",
                example_id=0,
            )
        )

        rubric = Rubric(funcs=[scalar_func], weights=[1.0])

        state = State(
            input=RolloutInput(
                prompt="test",
                answer="test",
                task="test",
                example_id=0,
            )
        )

        rubric = Rubric(funcs=[f_no_kwargs, f_with_kwargs], weights=[1.0, 2.0])

        state = State(
            input=RolloutInput(
                prompt=[{"role": "user", "content": "q"}],
                answer="ans",
                task="default",
                example_id=0,
                info={"extra": 123},
            )

        rubric = Rubric(funcs=[g1, g2], weights=[1.0, 1.0])

        state = State(
            input=RolloutInput(
                prompt="q",
                answer="ans",
                task="default",
                example_id=0,
            )
        )
        group = RubricGroup(rubrics=[rubric1, rubric2])

        # Create state
        state = State(
            input=RolloutInput(
                prompt=[{"role": "user", "content": "What is 1+1?"}],
                answer="2",
                task="default",
                example_id=0,
            )
        )
        group = RubricGroup(rubrics=[rubric1, rubric2])

        # Create state
        state = State(
            input=RolloutInput(
                prompt=[{"role": "user", "content": "What is 1+1?"}],
                answer="2",
                task="default",
                example_id=0,
            )
        )
        group = RubricGroup(rubrics=[rubric1, rubric2])

        # Create state
        state = State(
            input=RolloutInput(
                prompt=[{"role": "user", "content": "What is 1+1?"}],
                answer="2",
                task="default",
                example_id=0,
                info={"custom_param": "test"},
            )
        group = RubricGroup(rubrics=[rubric1])

        # Create state
        state = State(
            input=RolloutInput(
                prompt=[{"role": "user", "content": "What is 1+1?"}],
                answer="2",
                task="default",
                example_id=0,
            )
        )

        # Create states
        states = [
            State(
                input=RolloutInput(
                    prompt=[{"role": "user", "content": "What is 1+1?"}],
                    answer="2",
                    task="default",
                    example_id=0,
                )
            ),
            State(
                input=RolloutInput(
                    prompt=[{"role": "user", "content": "What is 2+2?"}],
                    answer="4",
                    task="default",
                    example_id=1,
                )
            ),
        rubric = Rubric(funcs=[reward_func], parser=xml_parser)
        group = RubricGroup(rubrics=[rubric])

        state = State(
            input=RolloutInput(
                prompt=[{"role": "user", "content": "What is 6 * 7?"}],
                answer="42",
                task="default",
                example_id=0,
            )
        )


@pytest.fixture
def sandbox_env():
    """Fixture to create a SandboxEnv instance with mocked dataset."""
    mock_dataset = Dataset.from_dict({"question": ["mock question"], "info": [{}]})

    mock_async_client_patcher = patch("verifiers.envs.sandbox_env.AsyncSandboxClient")
    mock_request_patcher = patch("verifiers.envs.sandbox_env.CreateSandboxRequest")

    mock_async_client = mock_async_client_patcher.start()
        env = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=sample_dataset,
            message_type="chat",
            system_prompt="You are helpful.",
            parser=Parser(),
            rubric=Rubric(),
        )
        assert env.message_type == "chat"
        assert isinstance(env.parser, Parser)

    def test_singleturn_env_initialization_completion(self, mock_openai_client):
        """Test SingleTurnEnv initialization with completion format."""
        completion_dataset = Dataset.from_dict(
            {
                "prompt": ["Calculate 2+2:", "What is the capital?"],
                "answer": ["4", "It depends on the country"],
            }
        )

        env = SingleTurnEnv(
    async def test_is_completed_method(self, mock_singleturn_env):
        """Test the is_completed method logic."""
        # No trajectory steps yet
        state = {
            "trajectory": [],
            "prompt": [{"role": "user", "content": "Hello"}],
            "timing": RolloutTiming(
                generation_ms=0.0,
                scoring_ms=0.0,
                total_ms=0.0,
                start_time=0.0,
        from verifiers.types import TrajectoryStep

        state = {
            "trajectory": [
                TrajectoryStep(
                    prompt=[{"role": "user", "content": "Hello"}],
                    completion=[{"role": "assistant", "content": "Hi"}],
                    response=MagicMock(),
                    tokens=None,
                    reward=None,
                    advantage=None,
                    extras={},
                )
            ],
            "prompt": [{"role": "user", "content": "Hello"}],
            "timing": RolloutTiming(
                generation_ms=0.0,
                scoring_ms=0.0,
                total_ms=0.0,
                start_time=0.0,
            await mock_singleturn_env.env_response(messages, state)

    @pytest.mark.asyncio
    async def test_rollout_chat_format(self, mock_singleturn_env):
        """Test rollout with chat format."""
        prompt = [{"role": "user", "content": "What is 2+2?"}]
        answer = "4"

        state = await mock_singleturn_env.rollout(
            input=RolloutInput(
                prompt=prompt,
                answer=answer,
                example_id=0,
            ),
            client=mock_singleturn_env.client,
            model="test-model",
        mock_singleturn_env.client.chat.completions.create.assert_called_once()

    @pytest.mark.asyncio
    async def test_rollout_completion_format(self, mock_singleturn_env_completion):
        """Test rollout with completion format."""
        prompt = "Calculate 2+2:"
        answer = "4"

        state = await mock_singleturn_env_completion.rollout(
            input=RolloutInput(
                prompt=prompt,
                answer=answer,
                example_id=0,
            ),
            client=mock_singleturn_env_completion.client,
            model="test-model",
        mock_singleturn_env_completion.client.completions.create.assert_called_once()

    @pytest.mark.asyncio
    async def test_rollout_with_sampling_args(self, mock_singleturn_env):
        """Test rollout with custom sampling arguments."""
        prompt = [{"role": "user", "content": "Hello"}]
        answer = "Hi"
        sampling_args = {"temperature": 0.8, "max_tokens": 100}

        state = await mock_singleturn_env.rollout(
            input=RolloutInput(
                prompt=prompt,
                answer=answer,
                example_id=0,
            ),
            client=mock_singleturn_env.client,
            model="test-model",
        assert "max_completion_tokens" in call_args.kwargs

    @pytest.mark.asyncio
    async def test_rollout_with_task_and_info(self, mock_singleturn_env):
        """Test rollout with task and info parameters."""
        prompt = [{"role": "user", "content": "Test question"}]
        answer = "Test answer"
        task = "math"
        info = {"difficulty": "easy"}

        state = await mock_singleturn_env.rollout(
            input=RolloutInput(
                prompt=prompt,
                answer=answer,
                task=task,
                info=info,
                example_id=0,
            ),
        # Mock get_model_response to return an error
        mock_singleturn_env.client.chat.completions.create = AsyncMock(
            side_effect=Exception("API Error")
        )

        prompt = [{"role": "user", "content": "Hello"}]
        answer = "Hi"

        state = await mock_singleturn_env.rollout(
            input=RolloutInput(
                prompt=prompt,
                answer=answer,
                example_id=0,
            ),
            client=mock_singleturn_env.client,
            model="test-model",
        assert isinstance(state["error"], vf.ModelError)

    @pytest.mark.asyncio
    async def test_rollout_state_structure(self, mock_singleturn_env):
        """Test that rollout creates proper state structure."""
        prompt = [{"role": "user", "content": "Hello"}]
        answer = "Hi"
        task = "greeting"
        info = {"context": "test"}

        state = await mock_singleturn_env.rollout(
            input=RolloutInput(
                prompt=prompt,
                answer=answer,
                task=task,
                info=info,
                example_id=0,
            ),
            model="test-model",
        )
        completion = state["completion"]

        # Check all expected state fields
        assert state["prompt"] == prompt
        assert state["completion"] == completion
        assert state["answer"] == answer
        assert state["task"] == task
        assert state["info"] == info
        assert "trajectory" in state
        """Test async generation with basic inputs."""
        from verifiers.types import RolloutInput

        inputs_list = [
            RolloutInput(
                prompt=[{"role": "user", "content": "What is 2+2?"}],
                answer="4",
                example_id=0,
                task="test",
            ),
            RolloutInput(
                prompt=[{"role": "user", "content": "What is 3+3?"}],
                answer="6",
                example_id=1,
                task="test",
            ),
        ]
        """Test async generation without scoring rollouts."""
        from verifiers.types import RolloutInput

        inputs_list = [
            RolloutInput(
                prompt=[{"role": "user", "content": "Hello"}],
                answer="Hi",
                example_id=0,
                task="test",
            ),
        ]
        """Test the synchronous generate wrapper."""
        from verifiers.types import RolloutInput

        inputs = [
            RolloutInput(
                prompt=[{"role": "user", "content": "Hello"}],
                answer="Hi",
                info={},
                example_id=0,
                task="test",
            )
            message_type="chat",
        )

        # Completion environment
        completion_dataset = Dataset.from_dict(
            {"prompt": ["Test prompt"], "answer": ["Test answer"]}
        )
        completion_env = SingleTurnEnv(
            client=mock_openai_client,
            model="test-model",
            dataset=completion_dataset,
        )

        # Test chat rollout
        chat_state = await chat_env.rollout(
            input=RolloutInput(
                prompt=[{"role": "user", "content": "Hello"}],
                answer="Hi",
                example_id=0,
            ),
            client=mock_openai_client,
            model="test-model",
        assert isinstance(chat_completion, list)

        # Test completion rollout
        comp_state = await completion_env.rollout(
            input=RolloutInput(
                prompt="Complete this:",
                answer="Done",
                example_id=0,
            ),
            client=mock_openai_client,
            model="test-model",
        # Before any trajectory steps
        from verifiers.types import RolloutInput, State

        state = State(
            input=RolloutInput(
                prompt=[{"role": "user", "content": "Hello"}],
                example_id=0,
                task="default",
            )
        )
        state["trajectory"] = []
        # After one trajectory step
        from verifiers.types import TrajectoryStep

        state = State(
            input=RolloutInput(
                prompt=[{"role": "user", "content": "Hello"}],
                example_id=0,
                task="default",
            )
        )
        state["trajectory"] = [
            TrajectoryStep(
                prompt=[{"role": "user", "content": "Hello"}],
                completion=[{"role": "assistant", "content": "Hi"}],
                response=MagicMock(),
                tokens=None,
                reward=None,
                advantage=None,
        assert await env.is_completed(state)

        # Even with multiple trajectory steps (shouldn't happen), it's still completed
        state = State(
            input=RolloutInput(
                prompt=[{"role": "user", "content": "Hello"}],
                example_id=0,
                task="default",
            )
        )
        state["trajectory"] = [
            TrajectoryStep(
                prompt=[{"role": "user", "content": "Hello"}],
                completion=[{"role": "assistant", "content": "Hi"}],
                response=MagicMock(),
                tokens=None,
                reward=None,
                advantage=None,
                extras={},
            ),
            TrajectoryStep(
                prompt=[{"role": "user", "content": "Hello"}],
                completion=[{"role": "assistant", "content": "Hi"}],
                response=MagicMock(),
                tokens=None,
                reward=None,
                advantage=None,
            response="Done",
        )

        state = await mock_stateful_tool_env.rollout(
            input=RolloutInput(
                prompt=[user_message],
                task="",
                answer="",
                example_id=0,
            ),
            client=mock_openai_client,
            tool_calls=[tool_call_with_invalid_json_arguments],
        )

        state = await env.rollout(
            input=RolloutInput(
                prompt=[{"role": "user", "content": "Square 4"}],
                answer="",
                task="",
                example_id=0,
            ),
            client=mock_openai_client,
            tool_calls=[tool_call],
        )

        state = await env.rollout(
            input=RolloutInput(
                prompt=[{"role": "user", "content": "Invoke"}],
                answer="",
                task="",
                example_id=0,
            ),
            client=mock_openai_client,

    def test_format_reward_function_no_assistant_messages(self, think_parser):
        """Test format reward function with no assistant messages."""
        reward_func = think_parser.get_format_reward_func()

        completion = [{"role": "user", "content": "Question"}]
        # Should handle gracefully, though the implementation might vary
        # This tests robustness of the reward function
        try:
            reward = reward_func(completion)
            # If it doesn't raise an error, the reward should be reasonable
            response="Done",
        )

        state = await mock_tool_env.rollout(
            input=RolloutInput(
                prompt=[user_message],
                answer="",
                task="",
                example_id=0,
            ),
            client=mock_openai_client,
            response="Hi",
        )

        state = await mock_tool_env.rollout(
            input=RolloutInput(
                prompt=[{"role": "user", "content": "Hello"}],
                answer="",
                task="",
                example_id=0,
            ),
            client=mock_openai_client,
            tool_calls=[tool_call_with_invalid_json_arguments],
        )

        state = await env.rollout(
            input=RolloutInput(
                prompt=[{"role": "user", "content": "Square 4"}],
                answer="",
                task="",
                example_id=0,
            ),
            client=mock_openai_client,
            tool_calls=[tool_call],
        )

        state = await env.rollout(
            input=RolloutInput(
                prompt=[{"role": "user", "content": "Invoke"}],
                answer="",
                task="",
                example_id=0,
            ),
            client=mock_openai_client,
async def test_parse_response_tokens_chat_with_tokens():
    """Test parsing tokens from chat completion response with token data."""
    from verifiers.types import ChatCompletion

    mock_response = MagicMock(spec=ChatCompletion)
    mock_response.prompt_token_ids = [1, 2, 3]
    mock_response.choices = [MagicMock()]
    mock_response.choices[0].token_ids = [4, 5, 6]
    mock_response.choices[0].logprobs = MagicMock()
    mock_response.choices[0].logprobs.content = [
        MagicMock(logprob=-0.1),
    ]

    tokens = await parse_response_tokens(mock_response, "chat")

    assert tokens is not None
    assert tokens["prompt_ids"] == [1, 2, 3]
    assert tokens["completion_ids"] == [4, 5, 6]
    assert tokens["prompt_mask"] == [0, 0, 0]
    assert tokens["completion_mask"] == [1, 1, 1]
    assert tokens["completion_logprobs"] == [-0.1, -0.2, -0.3]


@pytest.mark.asyncio
    """Test parsing tokens from chat completion response without token data."""
    from verifiers.types import ChatCompletion

    mock_response = MagicMock(spec=ChatCompletion)
    mock_response.choices = [MagicMock()]
    del mock_response.prompt_token_ids

    tokens = await parse_response_tokens(mock_response, "chat")

    assert tokens is None

    """Test parsing tokens from completion response with token data."""
    from verifiers.types import Completion

    mock_response = MagicMock(spec=Completion)
    mock_response.choices = [MagicMock()]
    mock_response.choices[0].prompt_token_ids = [10, 20]
    mock_response.choices[0].token_ids = [30, 40, 50]
    mock_response.choices[0].logprobs = MagicMock()
    mock_response.choices[0].logprobs.token_logprobs = [-0.5, -0.6, -0.7]

    tokens = await parse_response_tokens(mock_response, "completion")

    assert tokens is not None
    assert tokens["prompt_ids"] == [10, 20]
    assert tokens["completion_ids"] == [30, 40, 50]
    assert tokens["prompt_mask"] == [0, 0]
    assert tokens["completion_mask"] == [1, 1, 1]
    assert tokens["completion_logprobs"] == [-0.5, -0.6, -0.7]


@pytest.mark.asyncio
    """Test parsing tokens from completion response without token data."""
    from verifiers.types import Completion

    mock_response = MagicMock(spec=Completion)
    mock_response.choices = [MagicMock()]
    del mock_response.choices[0].prompt_token_ids

    tokens = await parse_response_tokens(mock_response, "completion")

    assert tokens is None


def test_process_trajectory_steps_for_training():
    """Test processing trajectory steps into training examples."""
    state1 = State(
        input=RolloutInput(
            prompt=[{"role": "user", "content": "Hello"}],
            example_id=0,
            task="test",
        )
    )
    state1["trajectory"] = [
        TrajectoryStep(
            prompt=[{"role": "user", "content": "Hello"}],
            completion=[{"role": "assistant", "content": "Hi"}],
            response=MagicMock(),
            tokens=TrajectoryStepTokens(
                prompt_ids=[1, 2],
                prompt_mask=[0, 0],
                completion_ids=[3, 4],
                completion_mask=[1, 1],
                completion_logprobs=[-0.1, -0.2],
            ),
            reward=1.0,
        )
    ]

    state2 = State(
        input=RolloutInput(
            prompt=[{"role": "user", "content": "Bye"}],
            example_id=1,
            task="test",
        )
    )
    state2["trajectory"] = [
        TrajectoryStep(
            prompt=[{"role": "user", "content": "Bye"}],
            completion=[{"role": "assistant", "content": "Goodbye"}],
            response=MagicMock(),
            tokens=TrajectoryStepTokens(
                prompt_ids=[5],
                prompt_mask=[0],
                completion_ids=[6, 7, 8],
                completion_mask=[1, 1, 1],
                completion_logprobs=[-0.3, -0.4, -0.5],
            ),
            reward=0.5,
    ]

    states = [state1, state2]

    # Process trajectories horizontally - each step becomes a separate training example
    prompt_ids_list = []
    completion_ids_list = []
    completion_logprobs_list = []
    prompt_mask_list = []
    completion_mask_list = []
    rewards_list = []

    for state in states:
        trajectory = state["trajectory"]
        for step in trajectory:
            if step["tokens"] is None:
                continue
            tokens = step["tokens"]
            prompt_ids_list.append(tokens["prompt_ids"])
            completion_ids_list.append(tokens["completion_ids"])
            completion_logprobs_list.append(tokens["completion_logprobs"])
            prompt_mask_list.append(tokens["prompt_mask"])
            completion_mask_list.append(tokens["completion_mask"])
            rewards_list.append(step.get("reward", 0.0))

    assert len(prompt_ids_list) == 2
    assert prompt_ids_list[0] == [1, 2]
    assert prompt_ids_list[1] == [5]
    assert completion_ids_list[0] == [3, 4]
    assert completion_ids_list[1] == [6, 7, 8]
    assert completion_logprobs_list[0] == [-0.1, -0.2]
    assert completion_logprobs_list[1] == [-0.3, -0.4, -0.5]
    assert rewards_list == [1.0, 0.5]

def test_process_trajectory_steps_skip_missing_tokens():
    """Test that trajectory steps without tokens are skipped."""
    state = State(
        input=RolloutInput(
            prompt=[{"role": "user", "content": "Hello"}],
            example_id=0,
            task="test",
        )
    )
    state["trajectory"] = [
        TrajectoryStep(
            prompt=[{"role": "user", "content": "Hello"}],
            completion=[{"role": "assistant", "content": "Hi"}],
            response=MagicMock(),
            tokens=None,
            reward=1.0,
            advantage=None,
            extras={},
        ),
        TrajectoryStep(
            prompt=[{"role": "user", "content": "Hello"}],
            completion=[{"role": "assistant", "content": "Hi again"}],
            response=MagicMock(),
            tokens=TrajectoryStepTokens(
                prompt_ids=[1],
                prompt_mask=[0],
                completion_ids=[2, 3],
                completion_mask=[1, 1],
                completion_logprobs=[-0.1, -0.2],
            ),
            reward=0.5,
    assert processed_steps[0]["tokens"] is not None
    assert processed_steps[0]["reward"] == 0.5


def test_trajectory_step_mask_combining():
    """Test combining prompt and completion masks for training."""
    tokens = TrajectoryStepTokens(
        prompt_ids=[1, 2, 3],
        prompt_mask=[0, 0, 0],
        completion_ids=[4, 5],
        completion_mask=[1, 1],
        completion_logprobs=[-0.1, -0.2],
    )

    # Combine for training
    token_ids = tokens["prompt_ids"] + tokens["completion_ids"]
    mask = tokens["prompt_mask"] + tokens["completion_mask"]
    logprobs = [0.0] * len(tokens["prompt_ids"]) + tokens["completion_logprobs"]

    assert token_ids == [1, 2, 3, 4, 5]
    assert mask == [0, 0, 0, 1, 1]
    assert logprobs == [0.0, 0.0, 0.0, -0.1, -0.2]

    extract_boxed_answer,
    extract_hash_answer,
    load_example_dataset,
)
from .utils.env_utils import load_environment
from .utils.logging_utils import print_prompt_completions_sample


# Setup default logging configuration
def setup_logging(
    level: str = "INFO",
    "extract_boxed_answer",
    "extract_hash_answer",
    "load_example_dataset",
    "setup_logging",
    "load_environment",
    "print_prompt_completions_sample",
    "get_model",
    "get_model_and_tokenizer",
    "RLTrainer",
    "RLConfig",
    "GRPOTrainer",
    """Used to catch errors while interacting with the model."""

    pass


class OverlongPromptError(Error):
    """Used to catch overlong prompt errors (e.g. prompt + requested number of tokens exceeds model context length)"""

    pass


class ToolError(Error):
GroupRewardFunc = Callable[..., list[float] | Awaitable[list[float]]]
RewardFunc = IndividualRewardFunc | GroupRewardFunc


class TrajectoryStepTokens(TypedDict):
    prompt_ids: list[int]
    prompt_mask: list[int]
    completion_ids: list[int]
    completion_mask: list[int]
    completion_logprobs: list[float]
    overlong_prompt: bool
    is_truncated: bool


class TrajectoryStep(TypedDict):
    prompt: Messages
    completion: Messages
    response: ModelResponse
    tokens: TrajectoryStepTokens | None
    reward: float | None
    advantage: float | None
    extras: dict[str, Any]


class BaseRolloutInput(TypedDict):
    prompt: Messages
    example_id: int
    task: str


class RolloutInput(BaseRolloutInput, total=False):
    # required: prompt, example_id, task
    # optional: answer, info
    answer: str
    info: Info


    scoring_ms: float
    total_ms: float


class State(dict):
    INPUT_FIELDS = ["prompt", "answer", "task", "info", "example_id"]
    # rollout inputs
    input: RolloutInput
    client: AsyncOpenAI | None
    model: str | None
    sampling_args: SamplingArgs | None


class GenerateOutputs(TypedDict):
    """TypedDict for generation outputs."""

    prompt: list[Messages]
    completion: list[Messages]
    answer: list[str]
    state: list[State]
    task: list[str]
    info: list[Info]


class ProcessedOutputs(TypedDict):
    """TypedDict for processed outputs."""

    prompt_ids: list[list[int]]
    prompt_mask: list[list[int]]
    completion_ids: list[list[int]]
    completion_mask: list[list[int]]
    completion_logprobs: list[list[float]]
    rewards: list[float]
    is_truncated: list[bool]
        )

    def format_dataset(
        self,
        dataset: Dataset,
        system_prompt: str | None = None,
        few_shot: vf.ChatMessages | None = None,
        question_key: str = "question",
        answer_key: str = "answer",
        map_kwargs: dict = {},
    ) -> Dataset:
        """
        Ensure unique example_ids and mapped tasks across concatenated datasets.
        """
        # use parent's prompt handling
        dataset = self._ensure_prompt(
            dataset, system_prompt, few_shot, question_key, answer_key, map_kwargs
        )
        # task is already set during concatenation, so skip _ensure_task

        # ensure unique example_ids across concatenated datasets
        if "example_id" in dataset.column_names:
            return example

        dataset = dataset.map(add_example_id, with_indices=True, **map_kwargs)

        assert "example_id" in dataset.column_names
        assert "prompt" in dataset.column_names
        assert "task" in dataset.column_names, (
            "Task column should be set during concatenation in __init__"
        )
        return dataset


    def __init__(
        self,
        dataset: Dataset | None = None,
        eval_dataset: Dataset | None = None,
        system_prompt: str | None = None,
        few_shot: list[ChatMessage] | None = None,
        parser: Parser | None = None,
        rubric: Rubric | None = None,
        sampling_args: SamplingArgs | None = None,
        message_type: MessageType = "chat",
        **kwargs,
    ):
        self.logger = logging.getLogger(f"verifiers.envs.{self.__class__.__name__}")
        self.message_type: Literal["chat", "completion"] = message_type
        self.oai_tools: list[ChatCompletionToolParam] | None = oai_tools
        self.system_prompt = system_prompt
        self.few_shot = few_shot
        self.parser = parser or Parser()
        self.rubric = rubric or Rubric()
        if self.parser.__class__ != self.rubric.parser.__class__:
            self.logger.warning(
        self.env_args = env_args or {}
        self.max_seq_len = max_seq_len
        if self.message_type == "chat":
            if dataset is not None:
                self.dataset = self.format_dataset(
                    dataset, self.system_prompt, self.few_shot, map_kwargs=map_kwargs
                )
            else:
                self.dataset = None
            if eval_dataset is not None:
                self.eval_dataset = self.format_dataset(
                    eval_dataset,
                    self.system_prompt,
                    self.few_shot,
                    map_kwargs=map_kwargs,
                )
            else:
                self.eval_dataset = None
        else:
            if self.system_prompt or self.few_shot:
                raise ValueError(
                    'The fields "system_prompt" and "few_shot" are not supported for completion tasks.'
                    'Please use message_type="chat" instead, or pre-format your dataset '
                    'to contain a "prompt" column.'
                )
            if dataset is not None:
                self.dataset = self.format_completion_dataset(
                    dataset, map_kwargs=map_kwargs
                )
            dataset = dataset.rename_column("example_id", "src_id")
        if "example_id" not in dataset.column_names:
            dataset = dataset.add_column("example_id", range(len(dataset)))  # type: ignore (weird datasets thing)
        return dataset

    def _ensure_prompt(
        self,
        dataset: Dataset,
        system_prompt: str | None = None,
        few_shot: list[ChatMessage] | None = None,
        question_key: str = "question",
        answer_key: str = "answer",
        map_kwargs: dict = {},
    ) -> Dataset:
        """Ensure prompt column exists."""
        if "prompt" not in dataset.column_names:

            def format_prompt_fn(prompt_str: str) -> list[ChatMessage]:
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                if few_shot:
                    messages.extend(few_shot)
                messages.append({"role": "user", "content": prompt_str})
                return messages

            if answer_key == "answer":
                dataset = dataset.map(
                    lambda x: {
                        "prompt": format_prompt_fn(x[question_key]),
                    },
                    **map_kwargs,
                )
            else:
                dataset = dataset.map(
                    lambda x: {
                        "prompt": format_prompt_fn(x[question_key]),
                        "answer": x[answer_key],
                    },
                    **map_kwargs,
                )
        return dataset
        return dataset

    def format_dataset(
        self,
        dataset: Dataset,
        system_prompt: str | None = None,
        few_shot: list[ChatMessage] | None = None,
        question_key: str = "question",
        answer_key: str = "answer",
        map_kwargs: dict = {},
    ) -> Dataset:
        """
        Format dataset by creating example_id and prompt columns, and setting task column.
        """
        dataset = self._ensure_example_id(dataset)
        dataset = self._ensure_prompt(
            dataset, system_prompt, few_shot, question_key, answer_key, map_kwargs
        )
        dataset = self._ensure_task(dataset, map_kwargs)
        return dataset

    def format_completion_dataset(
        self, dataset: Dataset, map_kwargs: dict = {}
    ) -> Dataset:
        """
        Format dataset by creating example_id and prompt columns, and setting task column.
        """
        dataset = self._ensure_example_id(dataset)
        dataset = self._ensure_task(dataset, map_kwargs)
        return dataset

        return self.eval_dataset

    async def get_model_response(
        self,
        state: State,
        prompt: Messages,
        client: AsyncOpenAI | None = None,
        model: str | None = None,
        oai_tools: list[ChatCompletionToolParam] | None = None,
        sampling_args: SamplingArgs | None = None,
        message_type: MessageType | None = None,
    ) -> ModelResponse:
        """
        Get model response for a given prompt (chat or completion).

        Convenience function for wrapping (chat, completion) API calls.
        Returns special error messages for context length issues.
        """
        # resolve optional argument, fallback to state or class defaults
        ):
            sampling_args.pop("max_completion_tokens")
        clean_sampling_args = {k: v for k, v in sampling_args.items() if v is not None}
        try:
            if message_type == "chat":
                assert isinstance(prompt, list)
                prompt = strip_nones_from_content(prompt)
                # --- detect audio parts and force text-only modality if caller didn't set one ---
                has_audio = False
                try:
                    for m in prompt:
                        c = m.get("content")
                        if isinstance(c, list):
                            for p in c:
                                if isinstance(p, dict) and str(
                                    p.get("type", "")
                    }

                if oai_tools:
                    response = await client.chat.completions.create(
                        model=model,
                        messages=prompt,
                        tools=oai_tools,
                        **clean_sampling_args,
                    )
                else:
                    response = await client.chat.completions.create(
                        model=model,
                        messages=prompt,
                        **clean_sampling_args,
                    )
                return response
            elif message_type == "completion":
                if oai_tools:
                    raise ValueError(
                        "oai_tools are not supported for completion tasks."
                    )
                assert isinstance(prompt, str)
                response = await client.completions.create(
                    model=model, prompt=prompt, **clean_sampling_args
                )
                return response
        except Exception as e:
            # in case of making a request with an overlong prompt, e.g from a too-long
            # environment response, we return a dummy response to with finish_reason "length"
            if isinstance(e, BadRequestError):
                error_text = e.response.text.lower()
                context_length_phrases = [
                    "this model's maximum context length is",
                    "exceed the configured limit",
                    "exceeds the configured limit",
                    "exceeded model",
                ]
                if any(phrase in error_text for phrase in context_length_phrases):
                    self.logger.debug("Caught overlong prompt.")
                    raise vf.OverlongPromptError(e)
            raise vf.ModelError(e)

    async def init_state(
        self,
        input: RolloutInput,

    async def _render_completion(self, state: State):
        if len(state["trajectory"]) == 0:
            state["completion"] = []
            return
        last_prompt = state["trajectory"][-1]["prompt"]
        last_completion = state["trajectory"][-1]["completion"]
        full_conversation = concat_messages([last_prompt, last_completion])
        state["completion"] = full_conversation[len(state["prompt"]) :]

    async def is_completed(self, state: State, **kwargs) -> bool:
        """Check all stop conditions. Sets state.is_completed=True if any condition is met."""
        for condition in self._stop_conditions:
            if await self._render_stop(state, condition):
        # Determine path_to_save
        if results_path is None:
            path_to_save = get_results_path(self.env_id, model)
        else:
            path_to_save = results_path
        prompts = [state["prompt"] for state in all_states]
        completions = [state.get("completion") for state in all_states]
        answers = [state.get("answer", "") for state in all_states]
        tasks = [state.get("task", "default") for state in all_states]
        infos = [state.get("info", {}) for state in all_states]
        example_ids = [state.get("example_id", 0) for state in all_states]
            state_columns=state_columns or [],
            path_to_save=path_to_save,
        )

        return GenerateOutputs(
            prompt=prompts,
            completion=completions,
            answer=answers,
            state=all_states,
            task=tasks,
            info=infos,
    async def has_error(self, state: State, **kwargs) -> bool:
        """Abrupts rollout early if an error has occurred."""
        return state.get("error") is not None

    @vf.stop
    async def prompt_too_long(self, state: State) -> bool:
        return state.get("prompt_too_long", False)

    @vf.stop
    async def max_turns_reached(self, state: State) -> bool:
        """Check if the maximum number of turns has been reached."""
        return len(state["trajectory"]) >= self.max_turns and self.max_turns > 0
        """
        Generate a response from the environment.
        """
        pass

    async def get_prompt_messages(self, state: State) -> Messages:
        if len(state["trajectory"]) == 0:
            return state["prompt"]
        else:
            prev_turn_prompt = state["trajectory"][-1]["prompt"]
            prev_turn_completion = state["trajectory"][-1]["completion"]
            messages = concat_messages([prev_turn_prompt, prev_turn_completion])
            env_response = await self.env_response(messages, state)
            return concat_messages([messages, env_response])

    async def add_model_response(
        self,
        state: State,
        prompt_messages: Messages,
        response: ModelResponse,
    ):
        if response is not None and response.id == "overlong-prompt":
            state["prompt_too_long"] = True
        completion_messages = await parse_response_messages(response, self.message_type)
        tokens = await parse_response_tokens(
            response, self.message_type, self.max_seq_len
        )
        trajectory_step = TrajectoryStep(
            prompt=prompt_messages,
            completion=completion_messages,
            response=response,
            tokens=tokens,
            reward=None,
            advantage=None,
            state = await self.setup_state(state)
        except vf.Error as e:
            state["error"] = e
        while not await self.is_completed(state):
            try:
                prompt_messages = await self.get_prompt_messages(state)
                response = await self.get_model_response(state, prompt_messages)
                await self.add_model_response(state, prompt_messages, response)
            except vf.Error as e:
                state["error"] = e
        return state



try:
    import reasoning_gym as rg  # type: ignore
    from reasoning_gym.composite import DatasetSpec
    from reasoning_gym.dataset import ProceduralDataset
    from reasoning_gym.utils import SYSTEM_PROMPTS

    DEFAULT_SYSTEM_PROMPT = SYSTEM_PROMPTS["default"]
except ImportError:
    print(
        "reasoning-gym is not installed. Please install it with `uv pip install reasoning-gym`."
    )
    exit(1)
    def __init__(
        self,
        gym: str | List[str | dict],
        num_train_examples: int = 1000,
        num_eval_examples: int = 100,
        system_prompt: str = DEFAULT_SYSTEM_PROMPT,
        parser: vf.Parser | None = None,
        seed: int = 0,
    ):
        self.gym = gym
        self.num_train_examples = num_train_examples
        rubric.add_reward_func(check_answer_reward_func)
        rubric.add_reward_func(parser.get_format_reward_func(), weight=0.0)
        super().__init__(
            dataset=dataset,
            eval_dataset=eval_dataset,
            system_prompt=system_prompt,
            parser=parser,
            rubric=rubric,
            message_type="chat",
        )
        self.parser = parser
    def rg_to_hf(self, rg_dataset: ProceduralDataset) -> Tuple[Dataset, Dataset]:
        train_dataset_rows = []
        eval_dataset_rows = []
        for i, x in enumerate(rg_dataset):
            row = {
                "question": x["question"],
                "answer": str(i),  # in verifiers, an answer must be a string
                "task": x["metadata"]["source_dataset"],
            }
            if i < self.num_train_examples:
                train_dataset_rows.append(row)
    def __init__(
        self,
        game: str = "Wordle-v0",
        num_train_examples: int = 1000,
        num_eval_examples: int = 0,
        system_prompt: str | None = None,
        parser: vf.XMLParser | None = None,
        rubric: vf.Rubric | None = None,
        feedback_fn: Callable[[str], str] = lambda x: x,
        seed: int = 0,
        **kwargs,
        dataset, eval_dataset = self.ta_to_hf()

        super().__init__(
            dataset=dataset,
            eval_dataset=eval_dataset,
            system_prompt=system_prompt,
            parser=parser,
            rubric=rubric,
            message_type="chat",
            **kwargs,
        )
    def ta_to_hf(self) -> tuple[Dataset, Dataset | None]:
        dataset_rows = []
        eval_dataset_rows = []
        ta_env = ta.make(env_id=self.game)
        ta_env.reset(num_players=1)
        _, user_prompt = ta_env.get_observation()
        words = ta_env.word_list
        # set seed
        random.seed(self.seed)
        for i in range(self.num_train_examples + self.num_eval_examples):
            question = user_prompt
            answer = random.choice(words)
            if i < self.num_train_examples:
                dataset_rows.append({"question": question, "answer": answer})
            else:
                eval_dataset_rows.append({"question": question, "answer": answer})
        dataset = Dataset.from_list(dataset_rows)
        if self.num_eval_examples > 0:
            eval_dataset = Dataset.from_list(eval_dataset_rows)
        else:
            eval_dataset = None
    - `lora_use_rslora`: whether to use RSLoRA (default is `False`)
- Training configuration arguments:
  - `learning_rate`: the learning rate for the training (default is `1e-5`)
  - `micro_batch_size`: rollouts per GPU per gradient accumulation step (default is `8`)
  - `batch_size`: rollouts per global batch (default is `512`)
  - `rollouts_per_example`: rollouts per example/prompt (default is `16`)
  - `max_seq_len`: the maximum sequence length for the training (default is `2048`)
  - `max_steps`: the maximum number of steps for the training (default is `500`)
- Sampling configuration arguments:
  - `max_tokens`: the maximum number of tokens per request (default is `None`)
  - `temperature`: the temperature for the sampling (default is `0.7`)
    )
    max_seq_len: int = field(
        default=2048,
        metadata={"help": "Maximum length for training sequences."},
    )
    max_prompt_len: Optional[int] = field(
        default=512,
        metadata={
            "help": "Maximum length of the prompt. If the prompt is longer than this value, it will be truncated left."
        },
    )
    max_steps: int = field(
        default=500,
        metadata={"help": "Total number of training steps to perform."},
        },
    )
    repetition_penalty: float = field(
        default=1.0,
        metadata={
            "help": "Float that penalizes new tokens based on whether they appear in the prompt and the generated "
            "text so far. Values > 1.0 encourage the model to use new tokens, while values < 1.0 encourage the model "
            "to repeat tokens."
        },
    )
    presence_penalty: float = field(
        metadata={"help": "Integration to report results and logs to (e.g., 'wandb')."},
    )
    remove_unused_columns: bool = field(
        default=False,
        metadata={
            "help": "Whether to only keep the column 'prompt' in the dataset. If you use a custom reward function "
            "that requires any column other than 'prompts' and 'completions', you should keep this to `False`."
        },
    )
    shuffle_dataset: bool = field(
        default=True,
        metadata={"help": "Whether to shuffle the training dataset."},
                "skip_special_tokens": False,
                "spaces_between_special_tokens": False,
                "include_stop_str_in_output": False,
                "return_tokens_as_token_ids": True,
                "return_token_ids": True,
                "prompt_logprobs": True,
            },
        }
        self.gradient_accumulation_steps = 1
        super().__post_init__()

    microbatches: list[list[Microbatch]]
    items_per_process: list[int]
    global_item_count: int
    # logging
    generation_time: float = 0.0
    prompts: list[Any] = Field(default_factory=list)
    completions: list[Any] = Field(default_factory=list)
    errors: list[Any] = Field(default_factory=list)
    metrics_dict: dict[str, float] = Field(default_factory=dict)
    rewards_dict: dict[str, list[float]] = Field(default_factory=dict)

        num_processes: int,
        generation_timeout: float,
        processing_class: PreTrainedTokenizerBase,
        mask_env_responses: bool,
        max_seq_len: int,
        max_prompt_len: int,
        mask_truncated_completions: bool,
        zero_truncated_completions: bool,
        max_concurrent: int,
    ):
        self.env = env
        self.client_timeout = client_timeout
        self.client = None  # created in worker thread
        self.model_name = model_name
        self.sampling_args = sampling_args
        self.rollouts_per_example = rollouts_per_example
        self.prompts_per_batch = batch_size // rollouts_per_example
        self.micro_batch_size = micro_batch_size
        self.num_processes = num_processes
        self.generation_timeout = generation_timeout
        self.processing_class = processing_class
        self.mask_env_responses = mask_env_responses
        self.max_seq_len = max_seq_len
        self.max_prompt_len = max_prompt_len
        self.mask_truncated_completions = mask_truncated_completions
        self.zero_truncated_completions = zero_truncated_completions
        self.max_concurrent = max_concurrent

        # queues for communication
        self.stop_event = threading.Event()
        self.logger = logging.getLogger(__name__)
        self.is_generating = False
        self.worker_loop = None

        max_length = self.max_prompt_len
        assert env.dataset is not None

        def filter_by_prompt_length(example, processing_class):
            prompt = example["prompt"]
            if isinstance(prompt, list):
                prompt_text = processing_class.apply_chat_template(
                    prompt, tokenize=False, add_generation_prompt=True
                )
            else:
                prompt_text = prompt
            prompt_ids = processing_class.encode(prompt_text)
            return len(prompt_ids) <= max_length

        env.dataset = env.dataset.filter(
            filter_by_prompt_length,
            fn_kwargs={"processing_class": processing_class},
        )

    def get_dataset_slice(self, batch_id: int) -> Dataset:
        """Get dataset slice for a given batch id"""
        num_rows = self.prompts_per_batch
        dataset = self.env.get_dataset()
        total_rows = len(dataset)
        if total_rows == 0:
            raise ValueError("Environment dataset is empty")
        offset = (batch_id * num_rows) % total_rows
        )
        self.is_generating = False
        wall_clock_s = time.time() - start_time

        # process trajectories horizontally - each step becomes a separate training example
        prompt_ids: list[list[int]] = []
        prompt_mask: list[list[int]] = []
        completion_ids: list[list[int]] = []
        completion_mask: list[list[int]] = []
        completion_logprobs: list[list[float]] = []
        advantages: list[float] = []

            trajectory = state["trajectory"]
            for step in trajectory:
                tokens = step["tokens"]
                if tokens is None:
                    continue
                prompt_ids.append(tokens["prompt_ids"])
                prompt_mask.append(tokens["prompt_mask"])
                completion_ids.append(tokens["completion_ids"])
                completion_mask.append(tokens["completion_mask"])
                completion_logprobs.append(tokens["completion_logprobs"])
                advantages.append(step["advantage"])

            pe = ps + per_proc
            proc_mbs: list[Microbatch] = []
            proc_item_total = 0
            for s in range(ps, pe, self.micro_batch_size):
                e = min(s + self.micro_batch_size, pe)
                ids_chunk = [prompt_ids[i] + completion_ids[i] for i in range(s, e)]
                mask_chunk = [prompt_mask[i] + completion_mask[i] for i in range(s, e)]
                logprobs_chunk = [
                    [0.0] * len(prompt_mask[i]) + completion_logprobs[i]
                    for i in range(s, e)
                ]
                lengths = [len(mask) for mask in mask_chunk]
                adv_chunk = [
                    [advantages[i]] * lengths[idx]
            items_per_process=items_per_process,
            global_item_count=global_item_count,
            generation_time=wall_clock_s,
            rewards_dict=rewards_dict,
            completions=env_results["completion"],
            prompts=env_results["prompt"],
            errors=errors,
            metrics_dict=metrics_dict,
        )


    selective_log_softmax,
    summarize_values,
    update_stat_tracker,
)
from verifiers.types import Messages
from verifiers.utils.logging_utils import print_prompt_completions_sample
from verifiers.utils.message_utils import messages_to_printable, sanitize_tool_calls


class RLTrainer(Trainer):
    def __init__(
                num_processes=self.accelerator.num_processes,
                generation_timeout=args.generation_timeout,
                processing_class=self.processing_class,
                mask_env_responses=args.mask_env_responses,
                max_seq_len=self.max_seq_len,
                max_prompt_len=args.max_prompt_len or self.max_seq_len,
                mask_truncated_completions=args.mask_truncated_completions,
                zero_truncated_completions=args.zero_truncated_completions,
                max_concurrent=args.max_concurrent,
            )
            self.orchestrator.start()

        # metrics
        self._metrics = {"train": defaultdict(list), "eval": defaultdict(list)}
        self._total_train_tokens = 0
        self._textual_logs = {
            "prompt": deque(),
            "completion": deque(),
            "error": deque(),
            "rewards": defaultdict(lambda: deque()),
        }

            self.log_metrics(
                mode="train",
                batch_metrics=metrics_to_log,
            )
            self.log_rollouts(
                prompts=batch.prompts,
                completions=batch.completions,
                errors=batch.errors,
                rewards_dict=batch.rewards_dict,
            )

        logs = {**logs, **metrics}
        super().log(logs, start_time)
        self._metrics[mode].clear()

        if self.accelerator.is_main_process:
            print_prompt_completions_sample(
                list(self._textual_logs["prompt"]),  # type: ignore[arg-type]
                list(self._textual_logs["completion"]),  # type: ignore[arg-type]
                list(self._textual_logs["error"]),  # type: ignore[arg-type]
                list(self._textual_logs["rewards"]["reward"]),  # type: ignore[arg-type]
                self.state.global_step,
            )
                            "content": m.get("content", ""),
                        }
                        for m in messages
                    ]

                prompts_clean = [
                    role_content_only(sanitize_tool_calls(messages_to_printable(p)))
                    for p in self._textual_logs["prompt"]
                ]
                completions_clean = [
                    role_content_only(sanitize_tool_calls(messages_to_printable(c)))
                    for c in self._textual_logs["completion"]
                ]
                table = {
                    "step": [str(self.state.global_step)]
                    * len(self._textual_logs["prompt"]),
                    "prompt": prompts_clean,
                    "completion": completions_clean,
                    **{k: list(v) for k, v in self._textual_logs["rewards"].items()},  # type: ignore[union-attr]
                }
                df = pd.DataFrame(table)
                wandb.log({"completions": wandb.Table(dataframe=df)})

            # clear after logging
            self._textual_logs["prompt"].clear()
            self._textual_logs["completion"].clear()
            self._textual_logs["error"].clear()
            for key in self._textual_logs["rewards"]:
                self._textual_logs["rewards"][key].clear()

    def log_rollouts(
        self,
        prompts: List[Messages],
        completions: List[Messages],
        errors: List[Error | None],
        rewards_dict: Dict[str, Any],
    ) -> None:
        self._textual_logs["prompt"].extend(prompts)  # type: ignore[union-attr]
        self._textual_logs["completion"].extend(completions)  # type: ignore[union-attr]
        self._textual_logs["error"].extend(errors)  # type: ignore[union-attr]
        for reward_key in rewards_dict:
            reward_values = rewards_dict[reward_key]
            self._textual_logs["rewards"][reward_key].extend(reward_values)  # type: ignore[union-attr]
from verifiers.parsers.parser import Parser
from verifiers.rubrics.rubric import Rubric
from verifiers.types import Messages, State
from verifiers.utils.async_utils import maybe_await

DEFAULT_JUDGE_PROMPT = """Given a ground truth answer \
and a response, determine if the response is correct.

Question:
```
{question}
```

Ground truth answer:
```
{answer}
        parser: Parser | None = None,
        parallelize_scoring: bool = False,
        judge_client: AsyncOpenAI | None = None,
        judge_model: str = "gpt-4.1-nano",
        judge_sampling_args: dict[str, Any] | None = None,
        judge_prompt: str = DEFAULT_JUDGE_PROMPT,
    ):
        super().__init__(parser=parser)
        self.judge_client = judge_client if judge_client is not None else AsyncOpenAI()
        self.judge_model = judge_model
        self.judge_prompt = judge_prompt
        self.judge_sampling_args = judge_sampling_args or {}
        self.class_objects = {
            "parser": self.parser,
            "judge": self.judge,
            "judge_client": self.judge_client,
            "judge_model": self.judge_model,
            "judge_prompt": self.judge_prompt,
            "judge_sampling_args": self.judge_sampling_args,
        }

    async def judge(
        self,
        prompt: Messages,
        completion: Messages,
        answer: str,
        state: State,
    ) -> str:
        if isinstance(prompt, list):
            last_msg = prompt[-1]
            if isinstance(last_msg, dict) and "content" in last_msg:
                question = str(last_msg["content"])
            else:
                question = ""
        else:
            question = str(prompt)
        response = self.parser.parse_answer(completion)
        judge_prompt = self.judge_prompt.format(
            question=question, answer=answer, response=response
        )
        cached = state.get("judge_response")
        if isinstance(cached, dict) and judge_prompt in cached:
            return cached[judge_prompt]
        # Normalize judge sampling args for chat API
        judge_args = dict(self.judge_sampling_args or {})
        if "max_tokens" in judge_args:
            if judge_args["max_tokens"] is None:
                judge_args.pop("max_tokens")

        try:
            judge_response = await maybe_await(
                self.judge_client.chat.completions.create,
                model=self.judge_model,
                messages=[{"role": "user", "content": judge_prompt}],
                **judge_args,
            )
            judge_response = str(judge_response.choices[0].message.content)
        except RateLimitError as e:
            self.logger.warning(
                f"Error: {str(e)}"
            ) from e

        if not isinstance(cached, dict):
            cached = {}
        cached[judge_prompt] = judge_response
        state["judge_response"] = cached
        return judge_response



class Rubric:
    """
    Rubric class for reward functions.

    Each reward function takes:
    - prompt: list[dict[str, str]] | str
    - completion: list[dict[str, str]] | str
    - answer: Any (metadata for scoring)
    - task (optional): str (type of task)
    - **kwargs: additional kwargs

        return self.weights

    def _is_group_func(self, func: RewardFunc) -> bool:
        """Check if a function is a GroupRewardFunc by inspecting its signature."""
        sig = inspect.signature(func)
        # GroupRewardFunc has plural parameters: states, prompts, completions, etc.
        param_names = set(sig.parameters.keys())
        group_indicators = {
            "states",
            "prompts",
            "completions",
            "answers",
            "tasks",
            "infos",
        }

        async def _call():
            sig = inspect.signature(func)

            merged = dict(
                prompt=state["prompt"],
                completion=state["completion"],
                answer=state.get("answer", ""),
                state=state,
                task=state["task"],
                info=state.get("info", {}),
        """

        async def _call():
            sig = inspect.signature(func)
            merged = dict(
                prompts=[state["prompt"] for state in states],
                completions=[state["completion"] for state in states],
                answers=[state.get("answer", "") for state in states],
                states=states,
                tasks=[state["task"] for state in states],
                infos=[state.get("info", {}) for state in states],
# ----------------------------
# Formatting helpers
# ----------------------------


def format_prompt_or_completion(prompt_or_completion) -> Text:
    """Format completion for display."""
    out = Text()
    if isinstance(prompt_or_completion, list):
        for msg in prompt_or_completion:
            if not isinstance(msg, dict):
                out.append(str(msg))
                out.append("\n\n")
                continue
            role = msg.get("role", "")
                    else:
                        out.append(str(tc))
                    out.append("\n")
            out.append("\n")
        return out
    out.append(str(prompt_or_completion))
    return out


# ----------------------------
# Custom Panel Widget
            )

            # Rollout section with two columns
            with Horizontal(classes="rollout-container"):
                with Panel(classes="column-panel"):
                    yield Label(Text("Prompt", style="bold"), classes="column-header")
                    yield VerticalScroll(
                        Static("", id="prompt-content", markup=False),
                        id="prompt-scroll",
                    )

                with Panel(classes="column-panel"):
                    yield Label(
                        Text("Completion", style="bold"), classes="column-header"
        if not self.records:
            return

        record = self.records[self.current_record_idx]

        # Update prompt
        prompt = record.get("prompt", "")
        prompt_widget = self.query_one("#prompt-content", Static)
        prompt_widget.update(format_prompt_or_completion(prompt))

        # Update completion
        completion = record.get("completion", "")
        completion_widget = self.query_one("#completion-content", Static)
        completion_text = format_prompt_or_completion(completion)
        error = record.get("error")
        if error is not None:
            completion_text.append("\n\n")
            completion_text.append("error: ", style="bold red")
            completion_text.append(str(error), style="red")
    def action_prev_record(self) -> None:
        if self.records:
            self.current_record_idx = (self.current_record_idx - 1) % len(self.records)
            self.update_display()
            # Reset scroll positions
            self.query_one("#prompt-scroll").scroll_y = 0
            self.query_one("#completion-scroll").scroll_y = 0

    def action_next_record(self) -> None:
        if self.records:
            self.current_record_idx = (self.current_record_idx + 1) % len(self.records)
            self.update_display()
            # Reset scroll positions
            self.query_one("#prompt-scroll").scroll_y = 0
            self.query_one("#completion-scroll").scroll_y = 0


# ----------------------------
# Main App
        margin-bottom: 1;
        text-align: center;
        text-style: bold;
    }
    
    #prompt-scroll, #completion-scroll {
        height: 1fr;
        background: $surface;
        padding: 0 1;
        scrollbar-color: $secondary;
        scrollbar-background: $panel;

from datasets import Dataset, concatenate_datasets, load_dataset

from verifiers.types import ChatMessage

### PROMPTS ###

THINK_BOXED_SYSTEM_PROMPT = "Think step-by-step inside <think>...</think> tags. \
    Then, give your final answer inside \\boxed{}."

### https://github.com/huggingface/lighteval/blob/ecef2c662b9418866b6447d33b5e7d5dedd74af8/src/lighteval/tasks/default_prompts.py#L1474
BOXED_SYSTEM_PROMPT = (
    "Please reason step by step, and put your final answer within \\boxed{}."
)
###############


def format_dataset(
    dataset: Dataset,
    system_prompt: str | None = None,
    few_shot: list[ChatMessage] | None = None,
    question_key: str = "question",
    answer_key: str = "answer",
    map_kwargs: dict = {},
) -> Dataset:
    """
    Create `example_id` and `prompt` columns if not present.
    """
    # if "id" column is present and not int, rename it to "src_id"
    if "example_id" in dataset.column_names and not isinstance(
        dataset["example_id"][0], int
    ):
        dataset = dataset.rename_column("example_id", "src_id")
    if "example_id" not in dataset.column_names:
        dataset = dataset.add_column("example_id", range(len(dataset)))  # type: ignore

    # extract format_prompt as a standalone function to avoid capturing self
    def format_prompt_fn(prompt_str: str) -> list[ChatMessage]:
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        if few_shot:
            messages.extend(few_shot)
        messages.append({"role": "user", "content": prompt_str})
        return messages

    if "prompt" not in dataset.column_names:
        if answer_key == "answer":
            dataset = dataset.map(
                lambda x: {
                    "prompt": format_prompt_fn(x[question_key]),
                },
                **map_kwargs,
            )
        else:
            dataset = dataset.map(
                lambda x: {
                    "prompt": format_prompt_fn(x[question_key]),
                    "answer": x[answer_key],
                },
                **map_kwargs,
            )
    assert "example_id" in dataset.column_names
    assert "prompt" in dataset.column_names
    return dataset


def extract_boxed_answer(text: str) -> str:
    def find_matching_brace(s: str, start: int) -> int:
def get_preprocess_fn(name: str) -> Callable[[dict], dict]:
    if name == "aime2024":

        def preprocess_aime2024(x: dict[str, Any]) -> dict[str, Any]:
            return {
                "question": x["problem"],
                "answer": str(int(x["answer"])),
            }

        return preprocess_aime2024
    elif name == "aime2025":

        def preprocess_aime2025(x: dict[str, Any]) -> dict[str, Any]:
            return {
                "question": x["question"],
                "answer": strip_non_numeric(x["answer"]),
            }

        return preprocess_aime2025
    elif name == "amc2023":

        def preprocess_amc2023(x: dict[str, Any]) -> dict[str, Any]:
            return {
                "question": x["problem"],
                "answer": x["answer"],
            }

        return preprocess_amc2023
    elif name in ["gpqa_diamond", "gpqa_main"]:

        def preprocess_gpqa(x: dict[str, Any]) -> dict[str, Any]:
            q = x["Question"]
            letters = ["A", "B", "C", "D"]
            random.shuffle(letters)
            itos = {k: v for k, v in enumerate(letters)}
            ans = {
                itos[0]: x["Correct Answer"],
                itos[1]: x["Incorrect Answer 1"],
                itos[2]: x["Incorrect Answer 2"],
                itos[3]: x["Incorrect Answer 3"],
            }
            question = f"Question: {q}\n\n"
            question += f"A: {ans['A']}\n"
            question += f"B: {ans['B']}\n"
            question += f"C: {ans['C']}\n"
            question += f"D: {ans['D']}"

            return {
                "question": question,
                "answer": itos[0],
            }

        return preprocess_gpqa
    elif name == "gsm8k":

        def preprocess_gsm8k(x: dict[str, Any]) -> dict[str, Any]:
            return {
                "question": x["question"],
                "answer": extract_hash_answer(x["answer"]),
            }

        return preprocess_gsm8k
    elif name == "math":

        def preprocess_math(x: dict[str, Any]) -> dict[str, Any]:
            return {
                "question": x["problem"],
                "answer": extract_boxed_answer(x["solution"]),
            }

        return preprocess_math
    elif name == "math500":

        def preprocess_math500(x: dict[str, Any]) -> dict[str, Any]:
            return {
                "question": x["problem"],
                "answer": x["answer"],
            }

        return preprocess_math500
    elif name == "mmlu":
        mmlu_map = ["A", "B", "C", "D"]

        def preprocess_mmlu(x: dict[str, Any]) -> dict[str, Any]:
            options = x["choices"]
            answer = x["answer"]
            question = f"Question: {x['question']}\n"
            for i, option in enumerate(options):
                question += f"\n{mmlu_map[i]}: {option}"
            return {
                "question": question,
                "temp_answer": mmlu_map[answer],
            }

        return preprocess_mmlu
    elif name == "mmlu_pro":
        mmlu_map = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J"]

        def preprocess_mmlu(x: dict[str, Any]) -> dict[str, Any]:
            options = x["options"]
            answer = x["answer"]
            question = f"Question: {x['question']}\n"
            for i, option in enumerate(options):
                question += f"\n{mmlu_map[i]}: {option}"
            return {
                "question": question,
                "answer": answer,
            }

        return preprocess_mmlu
    elif name == "openbookqa":

            formatted_choices = []
            for i in range(len(choices_labels)):
                formatted_choices.append(f"{choices_labels[i]}. {choices_texts[i]}")

            question = f"Question: {x['question_stem']}\n\nChoices:\n" + "\n".join(
                formatted_choices
            )
            return {
                "question": question,
                "answer": x["answerKey"],
            }

        return preprocess_openbookqa
    elif name in ["openrs", "openrs_easy", "openrs_hard"]:

        def preprocess_openrs(x: dict[str, Any]) -> dict[str, Any]:
            return {
                "question": x["problem"],
                "answer": x["answer"],
            }

        return preprocess_openrs
    elif name == "prime_code":

        def preprocess_prime_code(x: dict[str, Any]) -> dict[str, Any]:
            return {
                "question": x["prompt"],
                "answer": x["verification_info"],
            }

        return preprocess_prime_code
    else:
            split = "train"
        dataset = cast(
            Dataset, load_dataset("PrimeIntellect/verifiable-coding-problems")[split]
        )
        dataset = dataset.filter(
            lambda x: x["prompt"].startswith(
                "Solve the following coding problem using the programming language python:"
            )
        )
    else:
        raise ValueError(
            f"Dataset {name} not supported for preprocess_dataset. \nPlease ensure that the dataset is formatted with 'prompt' (str) and 'answer' (str) keys."
        )

    preprocess_fn = get_preprocess_fn(name)
    dataset = cast(Dataset, dataset)
    if n is not None and n > 0:
from datasets.utils import logging as ds_logging

import verifiers as vf
from verifiers.types import Endpoints, EvalConfig, GenerateMetadata, GenerateOutputs
from verifiers.utils.client_utils import setup_client
from verifiers.utils.logging_utils import print_prompt_completions_sample
from verifiers.utils.message_utils import messages_to_printable, sanitize_tool_calls
from verifiers.utils.path_utils import get_eval_results_path

logger = logging.getLogger(__name__)

    print(f"Provider: {results['metadata']['base_url']}")
    print(f"Examples: {results['metadata']['num_examples']}")
    print(f"Rollouts per example: {results['metadata']['rollouts_per_example']}")
    print("--- Example ---")

    printable_prompts = [messages_to_printable(p) for p in results["prompt"]]
    printable_completions = [messages_to_printable(c) for c in results["completion"]]
    errors = [s.get("error") for s in results["state"]]
    print_prompt_completions_sample(
        printable_prompts,
        printable_completions,
        errors,
        results["reward"],
        step=0,
        num_samples=num_samples,
    )
    return dataset_name


def make_dataset(results: GenerateOutputs, **kwargs) -> Dataset:
    clean_prompts = [messages_to_printable(p) for p in results["prompt"]]
    clean_prompts = [sanitize_tool_calls(p) for p in clean_prompts]
    clean_completions = [messages_to_printable(c) for c in results["completion"]]
    clean_completions = [sanitize_tool_calls(c) for c in clean_completions]
    save_info = any(info != {} for info in results["info"])
    save_answer = any(answer != "" for answer in results["answer"])
    errors = [s.get("error") for s in results["state"]]
    results_dict = {
        "example_id": results["example_id"],
        "prompt": clean_prompts,
        "completion": clean_completions,
        "task": results["task"],
        "reward": results["reward"],
        "error": [repr(e) if e is not None else None for e in errors],
        "generation_ms": [s["timing"]["generation_ms"] for s in results["state"]],

    # Prevent the logger from propagating messages to the root logger
    logger.propagate = False


def print_prompt_completions_sample(
    prompts: list[Messages],
    completions: list[Messages],
    errors: list[Error | None],
    rewards: list[float],
    step: int,
    num_samples: int = 1,
        return out

    console = Console()
    table = Table(show_header=True, header_style="bold white", expand=True)

    table.add_column("Prompt", style="bright_yellow")
    table.add_column("Completion", style="bright_green")
    table.add_column("Reward", style="bold cyan", justify="right")

    reward_values = rewards
    if len(reward_values) < len(prompts):
        reward_values = reward_values + [0.0] * (len(prompts) - len(reward_values))

    samples_to_show = min(num_samples, len(prompts))
    for i in range(samples_to_show):
        prompt = list(prompts)[i]
        completion = list(completions)[i]
        error = errors[i]
        reward = reward_values[i]

        formatted_prompt = _format_messages(prompt)
        formatted_completion = _format_messages(completion)
        if error is not None:
            formatted_completion += Text("\n\n") + _format_error(error)

        table.add_row(formatted_prompt, formatted_completion, Text(f"{reward:.2f}"))
        if i < samples_to_show - 1:
            table.add_section()

    panel = Panel(table, expand=False, title=f"Step {step}", border_style="bold white")
    console.print(panel)
    if message_type == "chat":
        assert isinstance(response, ChatCompletion)
        assert len(response.choices) == 1, "Response should always have one choice"
        if not hasattr(response.choices[0], "token_ids"):
            return None
        if not hasattr(response, "prompt_token_ids"):
            return None
        if not hasattr(response.choices[0], "logprobs"):
            return None
        if response.choices[0].logprobs is None:
            return None
            and "content" in response.choices[0].logprobs.keys()
            and response.choices[0].logprobs["content"] is not None
        )
        if not (has_logprobs_obj or has_logprobs_dict):
            return None
        prompt_ids = getattr(response, "prompt_token_ids")
        prompt_mask = [0] * len(prompt_ids)
        completion_ids = getattr(response.choices[0], "token_ids")
        completion_mask = [1] * len(completion_ids)
        if has_logprobs_obj:
            assert response.choices[0].logprobs.content is not None
            logprobs_content = response.choices[0].logprobs.content
            assert isinstance(response.choices[0].logprobs, dict)
            logprobs_content = response.choices[0].logprobs["content"]
            completion_logprobs = [token["logprob"] for token in logprobs_content]
    elif message_type == "completion":
        assert isinstance(response, Completion)
        if not hasattr(response.choices[0], "prompt_token_ids"):
            return None
        if not hasattr(response.choices[0], "token_ids"):
            return None
        if not hasattr(response.choices[0], "logprobs"):
            return None
        if response.choices[0].logprobs is None:
            return None
        if not hasattr(response.choices[0].logprobs, "token_logprobs"):
            return None
        prompt_ids = getattr(response.choices[0], "prompt_token_ids")
        prompt_mask = [0] * len(prompt_ids)
        completion_ids = getattr(response.choices[0], "token_ids")
        completion_mask = [1] * len(completion_ids)
        completion_logprobs = getattr(response.choices[0].logprobs, "token_logprobs")
    if max_seq_len is not None:
        prompt_len = len(prompt_ids)
        completion_len = len(completion_ids)
        overlong_prompt = prompt_len > max_seq_len
        if overlong_prompt:
            is_truncated = True
            prompt_ids = prompt_ids[:max_seq_len]
            prompt_mask = prompt_mask[:max_seq_len]
            completion_ids = []
            completion_mask = []
            completion_logprobs = []
        elif prompt_len + completion_len > max_seq_len:
            is_truncated = True
            completion_ids = completion_ids[: max_seq_len - prompt_len]
            completion_mask = completion_mask[: max_seq_len - prompt_len]
            completion_logprobs = completion_logprobs[: max_seq_len - prompt_len]
        else:
            is_truncated = False
    else:
        overlong_prompt = False
        is_truncated = False
    return TrajectoryStepTokens(
        prompt_ids=prompt_ids,
        prompt_mask=prompt_mask,
        completion_ids=completion_ids,
        completion_mask=completion_mask,
        completion_logprobs=completion_logprobs,
        overlong_prompt=overlong_prompt,
        is_truncated=is_truncated,
    )


async def parse_response_messages(
        return md(response.text)
    except Exception as e:
        return f"Error: {str(e)}"


def ask(question: str, url: str) -> str:
    """Ask a question about a web page returned from search results.

    Args:
        question: The question to be answered (by an LLM who will be given the web page contents)
        url: The URL of the web page to query

    Returns:
        A LLM-generated answer to the question based on the web page contents.

    Examples:
        {"question": "What is the capital of France?", "url": "https://en.wikipedia.org/wiki/France"} -> "The capital of France is Paris."
        {"question": "How many people live in the United States?", "url": "https://en.wikipedia.org/wiki/United_States"} -> "The population of the United States is approximately 340 million people."
    """
    BASE_URL = "https://api.deepinfra.com/v1/openai"
    API_KEY = os.getenv("DEEPINFRA_API_KEY")
    MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"


    from openai import OpenAI

    client = OpenAI(base_url=BASE_URL, api_key=API_KEY)

    prompt = f"""Answer the following question based on the provided web page contents:

    Question: {question}

    Page: {url}

    Page contents:
    {contents}
    """

    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=4000,
        )
        return response.choices[0].message.content or "Error: No response from model."
    except Exception as e:
        return f"Error: {str(e)}"
