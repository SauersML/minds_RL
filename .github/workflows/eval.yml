name: Eval Science Pipeline

on:
  workflow_dispatch:
    inputs:
      checkpoint_path:
        description: 'Tinker path to the RL checkpoint (e.g., tinker://.../sampler_weights/final). Leave empty to use latest from repo state.'
        required: false
      base_model:
        description: 'Base model name (e.g., Qwen/Qwen3-30B-A3B)'
        default: 'Qwen/Qwen3-30B-A3B'
        required: true

jobs:
  # 1. Resolve Configuration (Runs Once)
  resolve-config:
    runs-on: ubuntu-latest
    outputs:
      checkpoint_path: ${{ steps.resolve.outputs.path }}
      base_model: ${{ inputs.base_model }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Resolve Checkpoint Path
        id: resolve
        run: |
          if [ -n "${{ inputs.checkpoint_path }}" ]; then
            echo "Using manual input checkpoint."
            echo "path=${{ inputs.checkpoint_path }}" >> $GITHUB_OUTPUT
          else
            echo "Resolving latest checkpoint from repository state..."

            if [ ! -f run_state.json ]; then
              echo "::error::No run_state.json found and no manual checkpoint provided."
              exit 1
            fi

            RUN_ID=$(python3 -c "import json; print(json.load(open('run_state.json'))['run_id'])")
            echo "Found Run ID: $RUN_ID"

            # Find the checkpoints file anywhere inside the run folder
            CKPT_FILE=$(find "outputs/$RUN_ID" -name "checkpoints.jsonl" | head -n 1)

            if [ -z "$CKPT_FILE" ]; then
               echo "::error::Checkpoints file not found in outputs/$RUN_ID/"
               exit 1
            fi

            echo "Reading from: $CKPT_FILE"

            # Robust parsing to find the last valid sampler_path
            SAMPLER_PATH=$(python3 -c "import sys, json, textwrap; \
            exec(textwrap.dedent('''
              try:
                with open(sys.argv[1]) as f:
                    lines = f.readlines()
              except Exception as e:
                print(f'Error reading file: {e}', file=sys.stderr)
                sys.exit(1)
              
              found = None
              for l in reversed(lines):
                l = l.strip()
                if not l: continue
                try:
                  d = json.loads(l)
                  if 'sampler_path' in d:
                    found = d['sampler_path']
                    break
                except Exception:
                  continue
              
              if found: print(found)
              else: 
                print('sampler_path not found in any record', file=sys.stderr)
                sys.exit(1)
            '''))" "$CKPT_FILE")

            if [ -z "$SAMPLER_PATH" ]; then
               echo "::error::Could not resolve sampler_path from $CKPT_FILE"
               exit 1
            fi

            echo "Resolved latest checkpoint: $SAMPLER_PATH"
            echo "path=$SAMPLER_PATH" >> $GITHUB_OUTPUT
          fi

  # 2. Parallel Evaluation (Runs 5x Simultaneous Jobs)
  evaluate:
    needs: resolve-config
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        include:
          - id: math
            name: "Static Math"
            module: "evals.eval_math_static"
            num: 50
          - id: calibration
            name: "Calibration"
            module: "evals.eval_calibration_static"
            num: 50
          - id: ghost
            name: "Frozen Ghost"
            module: "evals.eval_ghost_static"
            num: 20
          - id: sanity
            name: "Sanity Check"
            module: "evals.eval_sanity_check"
            num: 20
          - id: scheming
            name: "Scheming Commitment"
            module: "evals.deception"
            num: 10

    env:
      TINKER_API_KEY: ${{ secrets.TINKER_API_KEY }}
      TOKENIZERS_PARALLELISM: "false"
      CHECKPOINT_PATH: ${{ needs.resolve-config.outputs.checkpoint_path }}
      BASE_MODEL: ${{ needs.resolve-config.outputs.base_model }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
    
      - name: Install System Dependencies
        run: |
          pip install uv
          uv pip install --system torch --index-url https://download.pytorch.org/whl/cpu
          uv pip install --system tinker tinker-cookbook verifiers

      - name: Install Local Environment Packages
        run: |
          python - <<'PY'
          import subprocess
          import sys
          from pathlib import Path

          envs_dir = Path("environments")
          if not envs_dir.exists():
              print("No environments directory found.")
              sys.exit(0)

          for env_dir in sorted(envs_dir.iterdir()):
              if (env_dir / "pyproject.toml").exists():
                  print(f"Installing env package: {env_dir.name}")
                  subprocess.check_call([
                      sys.executable,
                      "-m",
                      "pip",
                      "install",
                      "-e",
                      str(env_dir),
                  ])
          PY

      - name: Create Results Directory
        run: mkdir -p results

      - name: Create Evaluation Wrapper Script
        run: |
          cat <<EOF > run_eval_wrapper.py
          import argparse
          import asyncio
          import json
          import os
          import sys
          
          # Import the evaluate function directly from the installed package
          from tinker_cookbook.recipes.verifiers_rl.evaluate import evaluate

          async def main():
              parser = argparse.ArgumentParser()
              parser.add_argument("--vf_env_id", required=True)
              parser.add_argument("--model_name", required=False)
              parser.add_argument("--model_path", required=False)
              parser.add_argument("--num_examples", type=int, default=10)
              parser.add_argument("--output_file", required=True)
              args = parser.parse_args()

              # Use default constants matching the CLI Config in evaluate.py
              kwargs = {
                  "vf_env_id": args.vf_env_id,
                  "vf_env_args": {},
                  "model_name": args.model_name,
                  "num_examples": args.num_examples,
                  "rollouts_per_example": 1,
                  "max_concurrent": 32,
                  "max_tokens": 1024,
                  "temperature": 1.0,
                  "model_path": args.model_path or None
              }
              
              print(f"Starting Eval: {args.vf_env_id} -> {args.output_file}")
              
              # Run evaluation
              # The evaluate() function prints logs to stdout internally (via log_results)
              results = await evaluate(**kwargs)
              
              # Convert Pydantic models to dict if necessary
              if hasattr(results, "model_dump"):
                  data = results.model_dump()
              elif hasattr(results, "dict"):
                  data = results.dict()
              else:
                  data = results 
                  
              # Save full structured data
              with open(args.output_file, "w") as f:
                  json.dump(data, f)
              
              print(f"Successfully saved structured results to {args.output_file}")

          if __name__ == "__main__":
              asyncio.run(main())
          EOF

      - name: Eval Base Model (${{ matrix.name }})
        run: |
          echo "Running ${{ matrix.name }} on Base Model..."
          set +e
          python run_eval_wrapper.py \
            --vf_env_id="${{ matrix.module }}" \
            --model_name="${{ env.BASE_MODEL }}" \
            --num_examples=${{ matrix.num }} \
            --output_file="results/base_${{ matrix.id }}.json" \
            > results/base_${{ matrix.id }}.log 2>&1
          
          EXIT_CODE=$?
          cat results/base_${{ matrix.id }}.log
          if [ $EXIT_CODE -ne 0 ]; then
            echo "::error::Base model eval failed for ${{ matrix.name }}"
            exit $EXIT_CODE
          fi

      - name: Eval Checkpoint (${{ matrix.name }})
        run: |
          echo "Running ${{ matrix.name }} on Checkpoint..."
          set +e
          python run_eval_wrapper.py \
            --vf_env_id="${{ matrix.module }}" \
            --model_path="${{ env.CHECKPOINT_PATH }}" \
            --num_examples=${{ matrix.num }} \
            --output_file="results/ckpt_${{ matrix.id }}.json" \
            > results/ckpt_${{ matrix.id }}.log 2>&1
            
          EXIT_CODE=$?
          cat results/ckpt_${{ matrix.id }}.log
          if [ $EXIT_CODE -ne 0 ]; then
            echo "::error::Checkpoint eval failed for ${{ matrix.name }}"
            exit $EXIT_CODE
          fi

      - name: Upload Artifacts (Logs & JSON)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.id }}
          path: results/

  # 3. Generate Report (Aggregates All Logs & JSONs)
  generate-report:
    needs: [resolve-config, evaluate]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Download All Artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: results-*
          merge-multiple: true
          path: results

      - name: Create Science Aggregator
        run: |
          cat <<EOF > aggregate_science.py
          import glob
          import json
          import csv
          import os
          import math

          print("ðŸ”Ž Starting Science Aggregation...")

          rows = []
          summary_stats = {} # (task, metric) -> {base: [], ckpt: []}

          files = glob.glob("results/*.json")
          print(f"Found {len(files)} result files.")

          for fname in sorted(files):
              basename = os.path.basename(fname)
              # Filename format: {type}_{taskid}.json (e.g. base_math.json)
              parts = basename.replace(".json", "").split("_", 1)
              
              if len(parts) != 2:
                  print(f"Skipping malformed filename: {fname}")
                  continue
                  
              version = parts[0] # 'base' or 'ckpt'
              task_id = parts[1] # 'math', 'scheming', etc.
              
              try:
                  with open(fname) as f:
                      data = json.load(f)
              except Exception as e:
                  print(f"Failed to load {fname}: {e}")
                  continue
                  
              # Extract rewards
              rewards = data.get("reward", [])
              
              # Extract other metrics
              metrics = data.get("metrics", {})
              
              # Flatten Rewards
              # rollouts_per_example was 1 for this pipeline, so index is example_id
              for i, r in enumerate(rewards):
                  rows.append([task_id, version, i, 0, "reward", r])
                  
                  # Collect stats for summary
                  key = (task_id, "reward")
                  if key not in summary_stats: summary_stats[key] = {"base": [], "ckpt": []}
                  summary_stats[key][version].append(r)

              # Flatten Metrics
              for metric_name, values in metrics.items():
                  for i, v in enumerate(values):
                      rows.append([task_id, version, i, 0, metric_name, v])
                      
                      # Collect stats
                      key = (task_id, metric_name)
                      if key not in summary_stats: summary_stats[key] = {"base": [], "ckpt": []}
                      summary_stats[key][version].append(v)

          # Write Master TSV
          tsv_path = "evals.tsv"
          with open(tsv_path, "w", newline="") as f:
              writer = csv.writer(f, delimiter="\t")
              writer.writerow(["Task", "Model_Version", "Example_ID", "Rollout_ID", "Metric", "Value"])
              writer.writerows(rows)
          
          print(f"âœ… Saved {len(rows)} rows to {tsv_path}")

          # Generate Markdown Summary
          md_path = "summary.md"
          with open(md_path, "w") as f:
              f.write("## ðŸ§ª Eval Science Report\n\n")
              f.write("| Task | Metric | Base Mean | Ckpt Mean | Delta |\n")
              f.write("|---|---|---|---|---|\n")
              
              # Sort by task for cleaner output
              sorted_keys = sorted(summary_stats.keys())
              
              for (task, metric) in sorted_keys:
                  stats = summary_stats[(task, metric)]
                  base_vals = stats["base"]
                  ckpt_vals = stats["ckpt"]
                  
                  if not base_vals or not ckpt_vals:
                      continue
                      
                  avg_base = sum(base_vals) / len(base_vals)
                  avg_ckpt = sum(ckpt_vals) / len(ckpt_vals)
                  delta = avg_ckpt - avg_base
                  
                  # Formatting highlight
                  delta_str = f"{delta:+.4f}"
                  if delta > 0: delta_str = f"**{delta_str}** ðŸŸ¢"
                  elif delta < 0: delta_str = f"{delta_str} ðŸ”´"
                  
                  f.write(f"| {task} | {metric} | {avg_base:.4f} | {avg_ckpt:.4f} | {delta_str} |\n")

          print("âœ… Generated summary.md")
          EOF

      - name: Run Aggregation
        run: python3 aggregate_science.py

      - name: Publish Summary
        run: |
          cat summary.md >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Configuration" >> $GITHUB_STEP_SUMMARY
          echo "- **Base Model**: ${{ needs.resolve-config.outputs.base_model }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Checkpoint**: ${{ needs.resolve-config.outputs.checkpoint_path }}" >> $GITHUB_STEP_SUMMARY

      - name: Save Aggregated Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: eval-science-pack
          path: |
            evals.tsv
            summary.md
            results/*.log
            results/*.json
