name: Eval Science Pipeline

on:
  workflow_dispatch:
    inputs:
      checkpoint_path:
        description: 'Tinker path to the RL checkpoint (e.g., tinker://.../sampler_weights/final). Leave empty to use latest from repo state.'
        required: false
      base_model:
        description: 'Base model name (e.g., Qwen/Qwen3-30B-A3B)'
        default: 'Qwen/Qwen3-30B-A3B'
        required: true

jobs:
  # 1. Resolve Configuration (Runs Once)
  resolve-config:
    runs-on: ubuntu-latest
    outputs:
      checkpoint_path: ${{ steps.resolve.outputs.path }}
      base_model: ${{ inputs.base_model }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Resolve Checkpoint Path
        id: resolve
        run: |
          if [ -n "${{ inputs.checkpoint_path }}" ]; then
            echo "Using manual input checkpoint."
            echo "path=${{ inputs.checkpoint_path }}" >> $GITHUB_OUTPUT
          else
            echo "Resolving latest checkpoint from repository state..."

            if [ ! -f run_state.json ]; then
              echo "::error::No run_state.json found and no manual checkpoint provided."
              exit 1
            fi

            RUN_ID=$(python3 -c "import json; print(json.load(open('run_state.json'))['run_id'])")
            echo "Found Run ID: $RUN_ID"

            # Find the checkpoints file anywhere inside the run folder
            CKPT_FILE=$(find "outputs/$RUN_ID" -name "checkpoints.jsonl" | head -n 1)

            if [ -z "$CKPT_FILE" ]; then
               echo "::error::Checkpoints file not found in outputs/$RUN_ID/"
               exit 1
            fi

            echo "Reading from: $CKPT_FILE"

            # Robust parsing to find the last valid sampler_path
            SAMPLER_PATH=$(python3 -c "import sys, json, textwrap; \
            exec(textwrap.dedent('''
              try:
                with open(sys.argv[1]) as f:
                    lines = f.readlines()
              except Exception as e:
                print(f'Error reading file: {e}', file=sys.stderr)
                sys.exit(1)
              
              found = None
              for l in reversed(lines):
                l = l.strip()
                if not l: continue
                try:
                  d = json.loads(l)
                  if 'sampler_path' in d:
                    found = d['sampler_path']
                    break
                except Exception:
                  continue
              
              if found: print(found)
              else: 
                print('sampler_path not found in any record', file=sys.stderr)
                sys.exit(1)
            '''))" "$CKPT_FILE")

            if [ -z "$SAMPLER_PATH" ]; then
               echo "::error::Could not resolve sampler_path from $CKPT_FILE"
               exit 1
            fi

            echo "Resolved latest checkpoint: $SAMPLER_PATH"
            echo "path=$SAMPLER_PATH" >> $GITHUB_OUTPUT
          fi

  # 2. Parallel Evaluation (Runs 5x Simultaneous Jobs)
  evaluate:
    needs: resolve-config
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        include:
          - id: math
            name: "Static Math"
            module: "evals.eval_math_static"
            num: 50
          - id: calibration
            name: "Calibration"
            module: "evals.eval_calibration_static"
            num: 50
          - id: ghost
            name: "Frozen Ghost"
            module: "evals.eval_ghost_static"
            num: 20
          - id: sanity
            name: "Sanity Check"
            module: "evals.eval_sanity_check"
            num: 20
          - id: scheming
            name: "Scheming Commitment"
            module: "evals.eval_scheming_commitment"
            num: 10

    env:
      TINKER_API_KEY: ${{ secrets.TINKER_API_KEY }}
      TOKENIZERS_PARALLELISM: "false"
      CHECKPOINT_PATH: ${{ needs.resolve-config.outputs.checkpoint_path }}
      BASE_MODEL: ${{ needs.resolve-config.outputs.base_model }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
    
      - name: Install System Dependencies
        run: |
          pip install uv
          uv pip install --system torch --index-url https://download.pytorch.org/whl/cpu
          uv pip install --system tinker tinker-cookbook verifiers

      - name: Install Local Environment Packages
        run: |
          python - <<'PY'
          import subprocess
          import sys
          from pathlib import Path

          envs_dir = Path("environments")
          if not envs_dir.exists():
              print("No environments directory found.")
              sys.exit(0)

          for env_dir in sorted(envs_dir.iterdir()):
              if (env_dir / "pyproject.toml").exists():
                  print(f"Installing env package: {env_dir.name}")
                  subprocess.check_call([
                      sys.executable,
                      "-m",
                      "pip",
                      "install",
                      "-e",
                      str(env_dir),
                  ])
          PY

      - name: Create Results Directory
        run: mkdir -p results

      - name: Eval Base Model (${{ matrix.name }})
        run: |
          echo "Running ${{ matrix.name }} on Base Model..."
          set +e
          python -m tinker_cookbook.recipes.verifiers_rl.evaluate \
            vf_env_id="${{ matrix.module }}" \
            model_name="${{ env.BASE_MODEL }}" \
            num_examples=${{ matrix.num }} \
            rollouts_per_example=1 \
            > results/base_${{ matrix.id }}.log 2>&1
          EXIT_CODE=$?
          cat results/base_${{ matrix.id }}.log
          if [ $EXIT_CODE -ne 0 ]; then
            echo "::error::Base model eval failed for ${{ matrix.name }}"
            exit $EXIT_CODE
          fi

      - name: Eval Checkpoint (${{ matrix.name }})
        run: |
          echo "Running ${{ matrix.name }} on Checkpoint..."
          set +e
          python -m tinker_cookbook.recipes.verifiers_rl.evaluate \
            vf_env_id="${{ matrix.module }}" \
            model_path="${{ env.CHECKPOINT_PATH }}" \
            num_examples=${{ matrix.num }} \
            rollouts_per_example=1 \
            > results/ckpt_${{ matrix.id }}.log 2>&1
          EXIT_CODE=$?
          cat results/ckpt_${{ matrix.id }}.log
          if [ $EXIT_CODE -ne 0 ]; then
            echo "::error::Checkpoint eval failed for ${{ matrix.name }}"
            exit $EXIT_CODE
          fi

      - name: Upload Logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.id }}
          path: results/*.log

  # 3. Generate Report (Aggregates All Logs)
  generate-report:
    needs: [resolve-config, evaluate]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Download All Artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: results-*
          merge-multiple: true
          path: results

      - name: List Results
        run: ls -R results

      - name: Generate Science Summary
        run: |
          echo " ðŸ§ª Eval Science Report" >> $GITHUB_STEP_SUMMARY
          echo "Evaluated Checkpoint: \`${{ needs.resolve-config.outputs.checkpoint_path }}\`" >> $GITHUB_STEP_SUMMARY
          
          # Helper function to safely grep score
          get_score() {
            local file=$1
            if [ -f "$file" ]; then
              grep 'reward: avg' "$file" || echo 'N/A'
            else
              echo 'Missing Log'
            fi
          }

          echo " 1. Accuracy (Static Math)" >> $GITHUB_STEP_SUMMARY
          echo "Base: $(get_score results/base_math.log)" >> $GITHUB_STEP_SUMMARY
          echo "Ckpt: $(get_score results/ckpt_math.log)" >> $GITHUB_STEP_SUMMARY

          echo " 2. Confidence (Calibration Static)" >> $GITHUB_STEP_SUMMARY
          echo "Base: $(get_score results/base_calibration.log)" >> $GITHUB_STEP_SUMMARY
          echo "Ckpt: $(get_score results/ckpt_calibration.log)" >> $GITHUB_STEP_SUMMARY
          
          echo " 3. Grounding (Frozen Ghost)" >> $GITHUB_STEP_SUMMARY
          echo "Base: $(get_score results/base_ghost.log)" >> $GITHUB_STEP_SUMMARY
          echo "Ckpt: $(get_score results/ckpt_ghost.log)" >> $GITHUB_STEP_SUMMARY
          
          echo " 4. Safety (Sanity Check)" >> $GITHUB_STEP_SUMMARY
          echo "Base: $(get_score results/base_sanity.log)" >> $GITHUB_STEP_SUMMARY
          echo "Ckpt: $(get_score results/ckpt_sanity.log)" >> $GITHUB_STEP_SUMMARY

          echo " 5. Deception (Scheming Commitment)" >> $GITHUB_STEP_SUMMARY
          echo "Base: $(get_score results/base_scheming.log)" >> $GITHUB_STEP_SUMMARY
          echo "Ckpt: $(get_score results/ckpt_scheming.log)" >> $GITHUB_STEP_SUMMARY

      - name: Save Aggregated Logs
        uses: actions/upload-artifact@v4
        with:
          name: full-eval-logs
          path: results/*.log
