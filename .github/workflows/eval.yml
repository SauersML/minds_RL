name: Eval Science Pipeline

on:
  workflow_dispatch:
    inputs:
      checkpoint_path:
        description: 'Tinker path to the RL checkpoint (e.g., tinker://.../sampler_weights/final). Leave empty to use latest from repo state.'
        required: false
      base_model:
        description: 'Base model name (e.g., Qwen/Qwen3-30B-A3B)'
        default: 'Qwen/Qwen3-30B-A3B'
        required: true

jobs:
  run-science-evals:
    runs-on: ubuntu-latest
    env:
      TINKER_API_KEY: ${{ secrets.TINKER_API_KEY }}
      TOKENIZERS_PARALLELISM: "false"

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
    
      - name: Install System Dependencies
        run: |
          pip install uv
          uv pip install --system torch --index-url https://download.pytorch.org/whl/cpu
          uv pip install --system tinker tinker-cookbook verifiers

      - name: Install Local Environment Packages
        run: |
          # Evaluation scripts in evals/ depend on modules in environments/
          # We must install these local directories as editable packages.
          python - <<'PY'
          import subprocess
          import sys
          from pathlib import Path

          envs_dir = Path("environments")
          if not envs_dir.exists():
              print("No environments directory found.")
              sys.exit(0)

          for env_dir in sorted(envs_dir.iterdir()):
              if (env_dir / "pyproject.toml").exists():
                  print(f"Installing env package: {env_dir.name}")
                  subprocess.check_call([
                      sys.executable,
                      "-m",
                      "pip",
                      "install",
                      "-e",
                      str(env_dir),
                  ])
          PY

      - name: Resolve Checkpoint Path
        id: resolve
        run: |
          if [ -n "${{ inputs.checkpoint_path }}" ]; then
            echo "Using manual input checkpoint."
            echo "CHECKPOINT_PATH=${{ inputs.checkpoint_path }}" >> $GITHUB_ENV
          else
            echo "Resolving latest checkpoint from repository state..."

            if [ ! -f run_state.json ]; then
              echo "::error::No run_state.json found and no manual checkpoint provided."
              exit 1
            fi

            RUN_ID=$(python -c "import json; print(json.load(open('run_state.json'))['run_id'])")
            echo "Found Run ID: $RUN_ID"

            # Find the file anywhere inside the run folder
            CKPT_FILE=$(find "outputs/$RUN_ID" -name "checkpoints.jsonl" | head -n 1)

            if [ -z "$CKPT_FILE" ]; then
               echo "::error::Checkpoints file not found in outputs/$RUN_ID/"
               exit 1
            fi

            echo "Reading from: $CKPT_FILE"

            # Robust parsing using textwrap to handle records cleanly
            SAMPLER_PATH=$(python -c "import sys, json, textwrap; \
            exec(textwrap.dedent('''
              try:
                with open(sys.argv[1]) as f:
                    lines = f.readlines()
              except Exception as e:
                print(f'Error reading file: {e}', file=sys.stderr)
                sys.exit(1)
              
              found = None
              for l in reversed(lines):
                l = l.strip()
                if not l: continue
                try:
                  d = json.loads(l)
                  if 'sampler_path' in d:
                    found = d['sampler_path']
                    break
                except Exception:
                  continue
              
              if found: print(found)
              else: 
                print('sampler_path not found in any record', file=sys.stderr)
                sys.exit(1)
            '''))" "$CKPT_FILE")

            if [ -z "$SAMPLER_PATH" ]; then
               echo "::error::Could not resolve sampler_path from $CKPT_FILE"
               exit 1
            fi

            echo "Resolved latest checkpoint: $SAMPLER_PATH"
            echo "CHECKPOINT_PATH=$SAMPLER_PATH" >> $GITHUB_ENV
          fi

      - name: Create Results Directory
        run: mkdir -p results

      # ---------------------------------------------------------
      # 1. BASELINE EVALUATION
      # ---------------------------------------------------------
      
      - name: Base Model - Static Math (Calibration)
        run: |
          echo "Running Static Math on Base Model..."
          set +e
          python -m tinker_cookbook.recipes.verifiers_rl.evaluate \
            vf_env_id="evals.eval_math_static" \
            model_name="${{ inputs.base_model }}" \
            num_examples=50 \
            rollouts_per_example=1 \
            > results/base_math.log 2>&1
          EXIT_CODE=$?
          cat results/base_math.log
          [ $EXIT_CODE -eq 0 ] || exit $EXIT_CODE

      - name: Base Model - Frozen Ghost (Grounding)
        run: |
          echo "Running Frozen Ghost on Base Model..."
          set +e
          python -m tinker_cookbook.recipes.verifiers_rl.evaluate \
            vf_env_id="evals.eval_ghost_static" \
            model_name="${{ inputs.base_model }}" \
            num_examples=20 \
            rollouts_per_example=1 \
            > results/base_ghost.log 2>&1
          EXIT_CODE=$?
          cat results/base_ghost.log
          [ $EXIT_CODE -eq 0 ] || exit $EXIT_CODE

      - name: Base Model - Sanity Check (Regression)
        run: |
          echo "Running Sanity Check on Base Model..."
          set +e
          python -m tinker_cookbook.recipes.verifiers_rl.evaluate \
            vf_env_id="evals.eval_sanity_check" \
            model_name="${{ inputs.base_model }}" \
            num_examples=20 \
            rollouts_per_example=1 \
            > results/base_sanity.log 2>&1
          EXIT_CODE=$?
          cat results/base_sanity.log
          [ $EXIT_CODE -eq 0 ] || exit $EXIT_CODE

      # ---------------------------------------------------------
      # 2. CHECKPOINT EVALUATION
      # ---------------------------------------------------------

      - name: Checkpoint - Static Math (Calibration)
        run: |
          echo "Running Static Math on Checkpoint..."
          set +e
          python -m tinker_cookbook.recipes.verifiers_rl.evaluate \
            vf_env_id="evals.eval_math_static" \
            model_path="${{ env.CHECKPOINT_PATH }}" \
            num_examples=50 \
            rollouts_per_example=1 \
            > results/ckpt_math.log 2>&1
          EXIT_CODE=$?
          cat results/ckpt_math.log
          [ $EXIT_CODE -eq 0 ] || exit $EXIT_CODE

      - name: Checkpoint - Frozen Ghost (Grounding)
        run: |
          echo "Running Frozen Ghost on Checkpoint..."
          set +e
          python -m tinker_cookbook.recipes.verifiers_rl.evaluate \
            vf_env_id="evals.eval_ghost_static" \
            model_path="${{ env.CHECKPOINT_PATH }}" \
            num_examples=20 \
            rollouts_per_example=1 \
            > results/ckpt_ghost.log 2>&1
          EXIT_CODE=$?
          cat results/ckpt_ghost.log
          [ $EXIT_CODE -eq 0 ] || exit $EXIT_CODE

      - name: Checkpoint - Sanity Check (Regression)
        run: |
          echo "Running Sanity Check on Checkpoint..."
          set +e
          python -m tinker_cookbook.recipes.verifiers_rl.evaluate \
            vf_env_id="evals.eval_sanity_check" \
            model_path="${{ env.CHECKPOINT_PATH }}" \
            num_examples=20 \
            rollouts_per_example=1 \
            > results/ckpt_sanity.log 2>&1
          EXIT_CODE=$?
          cat results/ckpt_sanity.log
          [ $EXIT_CODE -eq 0 ] || exit $EXIT_CODE

      # ---------------------------------------------------------
      # 3. REPORTING
      # ---------------------------------------------------------

      - name: Generate Science Summary
        if: always()
        run: |
          echo " ðŸ§ª Eval Science Report" >> $GITHUB_STEP_SUMMARY
          echo " 1. Calibration (Static Math)" >> $GITHUB_STEP_SUMMARY
          echo "Base: $(grep 'reward: avg' results/base_math.log || echo 'N/A')" >> $GITHUB_STEP_SUMMARY
          echo "Ckpt: $(grep 'reward: avg' results/ckpt_math.log || echo 'N/A')" >> $GITHUB_STEP_SUMMARY
          
          echo " 2. Grounding (Frozen Ghost)" >> $GITHUB_STEP_SUMMARY
          echo "Base: $(grep 'reward: avg' results/base_ghost.log || echo 'N/A')" >> $GITHUB_STEP_SUMMARY
          echo "Ckpt: $(grep 'reward: avg' results/ckpt_ghost.log || echo 'N/A')" >> $GITHUB_STEP_SUMMARY
          
          echo " 3. Safety (Sanity Check)" >> $GITHUB_STEP_SUMMARY
          echo "Base: $(grep 'reward: avg' results/base_sanity.log || echo 'N/A')" >> $GITHUB_STEP_SUMMARY
          echo "Ckpt: $(grep 'reward: avg' results/ckpt_sanity.log || echo 'N/A')" >> $GITHUB_STEP_SUMMARY

      - name: Save Full Logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: full-eval-logs
          path: results/*.log
