name: Eval Science Pipeline

on:
  workflow_dispatch:
    inputs:
      checkpoint_path:
        description: 'Tinker path to the RL checkpoint (e.g., tinker://.../sampler_weights/final). Leave empty to use latest from repo state.'
        required: false
      base_model:
        description: 'Base model name (e.g., Qwen/Qwen3-30B-A3B)'
        default: 'Qwen/Qwen3-30B-A3B'
        required: true

jobs:
  run-science-evals:
    runs-on: ubuntu-latest
    env:
      TINKER_API_KEY: ${{ secrets.TINKER_API_KEY }}
      TOKENIZERS_PARALLELISM: "false"

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
    
      - name: Install Dependencies
        run: |
          pip install uv
          uv pip install --system tinker tinker-cookbook verifiers

      - name: Resolve Checkpoint Path
        id: resolve
        run: |
          if [ -n "${{ inputs.checkpoint_path }}" ]; then
            echo "Using manual input checkpoint."
            echo "CHECKPOINT_PATH=${{ inputs.checkpoint_path }}" >> $GITHUB_ENV
          else
            echo "Resolving latest checkpoint from repository state..."

            if [ ! -f run_state.json ]; then
              echo "::error::No run_state.json found and no manual checkpoint provided."
              exit 1
            fi

            RUN_ID=$(python -c "import json; print(json.load(open('run_state.json'))['run_id'])")
            echo "Found Run ID: $RUN_ID"

            # Find the file anywhere inside the run folder (handles the 'multi_task' subfolder)
            CKPT_FILE=$(find "outputs/$RUN_ID" -name "checkpoints.jsonl" | head -n 1)

            if [ -z "$CKPT_FILE" ]; then
               echo "::error::Checkpoints file not found in outputs/$RUN_ID/"
               exit 1
            fi

            echo "Reading from: $CKPT_FILE"

            # Robust parsing using textwrap to handle YAML indentation safely
            SAMPLER_PATH=$(python -c "import sys, json, textwrap; \
            exec(textwrap.dedent('''
              try:
                lines = open(sys.argv[1]).readlines()
              except: sys.exit(1)
              
              found = None
              for l in reversed(lines):
                try:
                  d = json.loads(l)
                  if 'sampler_path' in d:
                    found = d['sampler_path']
                    break
                except: continue
              
              if found: print(found)
              else: sys.exit(1)
            '''))" "$CKPT_FILE")

            if [ -z "$SAMPLER_PATH" ]; then
               echo "::error::Could not find sampler_path in $CKPT_FILE"
               exit 1
            fi

            echo "Resolved latest checkpoint: $SAMPLER_PATH"
            echo "CHECKPOINT_PATH=$SAMPLER_PATH" >> $GITHUB_ENV
          fi

      - name: Create Results Directory
        run: mkdir -p results

      # ---------------------------------------------------------
      # 1. BASELINE EVALUATION
      # ---------------------------------------------------------
      
      - name: Base Model - Static Math (Calibration)
        run: |
          echo "Running Static Math on Base Model..."
          python -m tinker_cookbook.recipes.verifiers_rl.evaluate \
            vf_env_id="./evals/eval_math_static" \
            model_name="${{ inputs.base_model }}" \
            num_examples=50 \
            rollouts_per_example=1 \
            > results/base_math.log 2>&1
          cat results/base_math.log

      - name: Base Model - Frozen Ghost (Grounding)
        run: |
          echo "Running Frozen Ghost on Base Model..."
          python -m tinker_cookbook.recipes.verifiers_rl.evaluate \
            vf_env_id="./evals/eval_ghost_static" \
            model_name="${{ inputs.base_model }}" \
            num_examples=20 \
            rollouts_per_example=1 \
            > results/base_ghost.log 2>&1
          cat results/base_ghost.log

      - name: Base Model - Sanity Check (Regression)
        run: |
          echo "Running Sanity Check on Base Model..."
          python -m tinker_cookbook.recipes.verifiers_rl.evaluate \
            vf_env_id="./evals/eval_sanity_check" \
            model_name="${{ inputs.base_model }}" \
            num_examples=20 \
            rollouts_per_example=1 \
            > results/base_sanity.log 2>&1
          cat results/base_sanity.log

      # ---------------------------------------------------------
      # 2. CHECKPOINT EVALUATION
      # ---------------------------------------------------------

      - name: Checkpoint - Static Math (Calibration)
        run: |
          echo "Running Static Math on Checkpoint..."
          python -m tinker_cookbook.recipes.verifiers_rl.evaluate \
            vf_env_id="./evals/eval_math_static" \
            model_path="${{ env.CHECKPOINT_PATH }}" \
            num_examples=50 \
            rollouts_per_example=1 \
            > results/ckpt_math.log 2>&1
          cat results/ckpt_math.log

      - name: Checkpoint - Frozen Ghost (Grounding)
        run: |
          echo "Running Frozen Ghost on Checkpoint..."
          python -m tinker_cookbook.recipes.verifiers_rl.evaluate \
            vf_env_id="./evals/eval_ghost_static" \
            model_path="${{ env.CHECKPOINT_PATH }}" \
            num_examples=20 \
            rollouts_per_example=1 \
            > results/ckpt_ghost.log 2>&1
          cat results/ckpt_ghost.log

      - name: Checkpoint - Sanity Check (Regression)
        run: |
          echo "Running Sanity Check on Checkpoint..."
          python -m tinker_cookbook.recipes.verifiers_rl.evaluate \
            vf_env_id="./evals/eval_sanity_check" \
            model_path="${{ env.CHECKPOINT_PATH }}" \
            num_examples=20 \
            rollouts_per_example=1 \
            > results/ckpt_sanity.log 2>&1
          cat results/ckpt_sanity.log

      # ---------------------------------------------------------
      # 3. REPORTING
      # ---------------------------------------------------------

      - name: Generate Science Summary
        run: |
          echo " ðŸ§ª Eval Science Report" >> $GITHUB_STEP_SUMMARY
          echo " 1. Calibration (Static Math)" >> $GITHUB_STEP_SUMMARY
          echo "Base: $(grep 'reward: avg' results/base_math.log || echo 'N/A')" >> $GITHUB_STEP_SUMMARY
          echo "Ckpt: $(grep 'reward: avg' results/ckpt_math.log || echo 'N/A')" >> $GITHUB_STEP_SUMMARY
          
          echo " 2. Grounding (Frozen Ghost)" >> $GITHUB_STEP_SUMMARY
          echo "Base: $(grep 'reward: avg' results/base_ghost.log || echo 'N/A')" >> $GITHUB_STEP_SUMMARY
          echo "Ckpt: $(grep 'reward: avg' results/ckpt_ghost.log || echo 'N/A')" >> $GITHUB_STEP_SUMMARY
          
          echo " 3. Safety (Sanity Check)" >> $GITHUB_STEP_SUMMARY
          echo "Base: $(grep 'reward: avg' results/base_sanity.log || echo 'N/A')" >> $GITHUB_STEP_SUMMARY
          echo "Ckpt: $(grep 'reward: avg' results/ckpt_sanity.log || echo 'N/A')" >> $GITHUB_STEP_SUMMARY

      - name: Save Full Logs
        uses: actions/upload-artifact@v4
        with:
          name: full-eval-logs
          path: results/*.log
